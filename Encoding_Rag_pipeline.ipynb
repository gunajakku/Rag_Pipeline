{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9e13d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain\n",
    "#!pip install lanchain_core\n",
    "#!pip install langchain_community\n",
    "#!pip install pypdf\n",
    "#!pip install pymupdf\n",
    "#!pip install sentence_transformers\n",
    "#!pip install faiss_cpu\n",
    "#!pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63925094",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Document structure\n",
    "from langchain_core.documents import Document\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bd02fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = Document(\n",
    "    page_content = \"This is the main text content i have as reference in creating a rag\",\n",
    "    metadata = {\n",
    "        \"source\":\"example.txt\",\n",
    "        \"pages\" :1,\n",
    "        \"author\": \"Krish Naik\",\n",
    "        \"date_created\":\"2025-11-10\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed0f70bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a simple text file\n",
    "os.makedirs(\"..\\\\python_projects\\\\Projects_git\",exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72485d18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b98af757",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_texts = {\"C:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\Text files\\\\rag_ref.txt\":\n",
    "                \"\"\"Ringg AI is a platform providing AI-powered voice assistants designed primarily for business communication automation. It offers no-code tools and APIs to create, deploy, and manage intelligent voice agents that handle high-volume communication tasks such as lead generation, loan collection, last-mile delivery coordination, appointment scheduling, recruitment calls, and customer support. The platform supports multilingual interactions in 20+ languages, provides human-like conversation experiences, and integrates with business tools like Slack, Microsoft Teams, and Discord for seamless workflows.\n",
    "\n",
    "Key services include:\n",
    "\n",
    "Automated outbound and inbound calls for tasks like lead qualification, feedback collection, order confirmations, loan follow-ups, and candidate screening.\n",
    "\n",
    "Embedded voice agents on business websites for real-time customer interaction.\n",
    "\n",
    "Advanced analytics on call performance and customer engagement.\n",
    "\n",
    "Scalable infrastructure with high uptime and low latency.\n",
    "\n",
    "Smart call transfers to human agents maintaining call context.\n",
    "\n",
    "Ringg AI is used across industries including banking, logistics, healthcare, education, and direct-to-consumer businesses to improve efficiency, increase conversion rates, and enhance customer experience at scale.​\n",
    "\n",
    "\"\"\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de781fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text file created!\n"
     ]
    }
   ],
   "source": [
    "for filepath, content in sample_texts.items():\n",
    "    with open (filepath, 'w',encoding = \"utf-8\") as f:\n",
    "        f.write(content)\n",
    "print(\"Sample text file created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2016a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\gunas\\anaconda3\\envs\\New_world\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "[Document(metadata={'source': 'C:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\Text files\\\\rag_ref.txt'}, page_content='Ringg AI is a platform providing AI-powered voice assistants designed primarily for business communication automation. It offers no-code tools and APIs to create, deploy, and manage intelligent voice agents that handle high-volume communication tasks such as lead generation, loan collection, last-mile delivery coordination, appointment scheduling, recruitment calls, and customer support. The platform supports multilingual interactions in 20+ languages, provides human-like conversation experiences, and integrates with business tools like Slack, Microsoft Teams, and Discord for seamless workflows.\\n\\nKey services include:\\n\\nAutomated outbound and inbound calls for tasks like lead qualification, feedback collection, order confirmations, loan follow-ups, and candidate screening.\\n\\nEmbedded voice agents on business websites for real-time customer interaction.\\n\\nAdvanced analytics on call performance and customer engagement.\\n\\nScalable infrastructure with high uptime and low latency.\\n\\nSmart call transfers to human agents maintaining call context.\\n\\nRingg AI is used across industries including banking, logistics, healthcare, education, and direct-to-consumer businesses to improve efficiency, increase conversion rates, and enhance customer experience at scale.\\u200b\\n\\n')]\n"
     ]
    }
   ],
   "source": [
    "## Text loader\n",
    "from langchain_community.document_loaders  import TextLoader\n",
    "# from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"C:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\Text files\\\\rag_ref.txt\", encoding = \"utf-8\")\n",
    "document = loader.load()\n",
    "print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11768121",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 503.28it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'C:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\Text files\\\\rag_ref.txt'}, page_content='Ringg AI is a platform providing AI-powered voice assistants designed primarily for business communication automation. It offers no-code tools and APIs to create, deploy, and manage intelligent voice agents that handle high-volume communication tasks such as lead generation, loan collection, last-mile delivery coordination, appointment scheduling, recruitment calls, and customer support. The platform supports multilingual interactions in 20+ languages, provides human-like conversation experiences, and integrates with business tools like Slack, Microsoft Teams, and Discord for seamless workflows.\\n\\nKey services include:\\n\\nAutomated outbound and inbound calls for tasks like lead qualification, feedback collection, order confirmations, loan follow-ups, and candidate screening.\\n\\nEmbedded voice agents on business websites for real-time customer interaction.\\n\\nAdvanced analytics on call performance and customer engagement.\\n\\nScalable infrastructure with high uptime and low latency.\\n\\nSmart call transfers to human agents maintaining call context.\\n\\nRingg AI is used across industries including banking, logistics, healthcare, education, and direct-to-consumer businesses to improve efficiency, increase conversion rates, and enhance customer experience at scale.\\u200b\\n\\n')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Directory loader\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "## loading all the text files from the directory\n",
    "dir_loader = DirectoryLoader(\"C:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\Text files\",\n",
    "                             glob = \"**\\\\*.txt\", # pattern to match text files\n",
    "                             loader_cls = TextLoader,#loader class to use\n",
    "                             loader_kwargs = {'encoding':'utf-8'},\n",
    "                             show_progress = True\n",
    "                            )\n",
    "\n",
    "documents = dir_loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d381aa66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  2.03it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T20:31:04+05:30', 'trapped': '', 'modDate': \"D:20251109203104+05'30'\", 'creationDate': \"D:20250317134132-06'00'\", 'page': 0}, page_content='Embeddings  \\n& Vector Stores\\nAuthors: Anant Nawalgaria, \\nXiaoqi Ren, and Charles Sugnet'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T20:31:04+05:30', 'trapped': '', 'modDate': \"D:20251109203104+05'30'\", 'creationDate': \"D:20250317134132-06'00'\", 'page': 1}, page_content='Embeddings & Vector Stores\\n2\\nFebrurary 2025\\nContent contributors\\nAntonio Gulli\\nGrace Mollison\\nRuiqi Guo\\nIftekhar Naim\\nJinhyuk Lee\\nAlan Li\\nPatricia Florissi\\nAndrew Brook\\nOmid Fatemieh\\nZhuyun Dai\\nLee Boonstra\\nPer Jacobsson\\nSiddhartha Reddy Jonnalagadda\\nXi Cheng\\nRaphael Hoffmann\\nCurators and Editors\\nAntonio Gulli\\nAnant Nawalgaria\\nGrace Mollison \\nTechnical Writer\\nJoey Haymaker\\nDesigner\\nMichael Lanning \\nAcknowledgements'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T20:31:04+05:30', 'trapped': '', 'modDate': \"D:20251109203104+05'30'\", 'creationDate': \"D:20250317134132-06'00'\", 'page': 2}, page_content='Introduction\\x08\\n5\\nWhy embeddings are important\\x08\\n6\\nEvaluating Embedding Quality\\x08\\n9\\nSearch Example\\x08\\n11\\nTypes of embeddings\\x08\\n16\\nText embeddings\\t\\n16\\nWord embeddings\\t\\n19\\nDocument embeddings\\t\\n23\\nShallow BoW models\\t\\n24\\nDeeper pretrained large language models\\t\\n26\\nImage & multimodal embeddings\\t\\n30\\nStructured data embeddings\\t\\n32\\nGeneral structured data\\t\\n32\\nUser/item structured data\\t\\n33\\nTable of contents'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T20:31:04+05:30', 'trapped': '', 'modDate': \"D:20251109203104+05'30'\", 'creationDate': \"D:20250317134132-06'00'\", 'page': 3}, page_content='Graph embeddings\\t\\n33\\nTraining Embeddings\\x08\\n34\\nVector search\\x08\\n36\\nImportant vector search algorithms\\x08\\n37\\nLocality sensitive hashing & trees\\t\\n38\\nHierarchical navigable small worlds \\t\\n41\\nScaNN\\t\\n44\\nVector databases \\x08\\n47\\nOperational considerations\\x08\\n49\\nApplications\\x08\\n51\\nQ & A with sources (retrieval augmented generation)\\x08\\n52\\nSummary\\x08\\n57\\nEndnotes\\x08\\n59'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T20:31:04+05:30', 'trapped': '', 'modDate': \"D:20251109203104+05'30'\", 'creationDate': \"D:20250317134132-06'00'\", 'page': 4}, page_content=\"Embeddings & Vector Stores\\n5\\nFebrurary 2025\\nIntroduction\\nModern machine learning thrives on diverse data—images, text, audio, and more. This \\nwhitepaper explores the power of embeddings, which transform this heterogeneous data into \\na unified vector representation for seamless use in various applications.\\nWe'll guide you through:\\n•\\t Understanding Embeddings: Why they are essential for handling multimodal data and \\ntheir diverse applications.\\n•\\t Embedding Techniques: Methods for mapping different data types into a common \\nvector space.\\nThese low-dimensional numerical \\nrepresentations of real-world data \\nsignificantly helps efficient large-\\nscale data processing and storage \\nby acting as means of lossy \\ncompression of the original data.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T20:31:04+05:30', 'trapped': '', 'modDate': \"D:20251109203104+05'30'\", 'creationDate': \"D:20250317134132-06'00'\", 'page': 5}, page_content='Embeddings & Vector Stores\\n6\\nFebrurary 2025\\n•\\t Efficient Management: Techniques for storing, retrieving, and searching vast collections \\nof embeddings.\\n•\\t Vector Databases: Specialized systems for managing and querying embeddings, \\nincluding practical considerations for production deployment.\\n•\\t Real-World Applications: Concrete examples of how embeddings and vector databases \\nare combined with large language models (LLMs) to solve real-world problems.\\nThroughout the whitepaper, code snippets provide hands-on illustrations of key concepts.\\nWhy embeddings are important\\nIn essence, embeddings are numerical representations of real-world data such as text, \\nspeech, image, or videos. The name embeddings refers to a similar concept in mathematics \\nwhere one space can be mapped, or embedded, into another space. For example, the \\noriginal BERT Model [ref] embeds text into a vector of 768 numbers, thus mapping from the \\nvery high dimensional space of all sentences to a much smaller 768 dimensions. Embeddings \\nare expressed as low-dimensional vectors where the geometric distance between two \\nvectors in the vector space is a projection of the relationship and semantic similarity between \\nthe two real-world objects that the vectors represent. In other words, they help you with \\nproviding compact representations of data of different types, while simultaneously also \\nallowing you to compare two different data objects and tell how similar or different they \\nare on a numerical scale. For example: the word ‘computer’ has a similar meaning to the \\npicture of a computer, as well as to the word ’laptop’ but not to the word ‘car’. These low-\\ndimensional numerical representations of real-world data significantly help efficient large-\\nscale data processing and storage by acting as means of lossy compression of the original \\ndata while retaining its important semantic properties.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T20:31:04+05:30', 'trapped': '', 'modDate': \"D:20251109203104+05'30'\", 'creationDate': \"D:20250317134132-06'00'\", 'page': 6}, page_content='Embeddings & Vector Stores\\n7\\nFebrurary 2025\\nFor some intuition about embeddings consider the familiar latitude and longitude which \\nare used to map locations on earth to a pair of numbers, or a vector of length two. Latitude \\nand longitude can be thought of as an embedding of a particular location. While seemingly \\nobvious now, this simple mapping of a location to a pair of numbers transformed human \\nnavigation and is still critical to this day. Given the latitude and longitude of two addresses \\nit is relatively easy to see how distant they are from each other, or look up other nearby \\nlocations. As with latitutde and longitude if two text embeddings are close to each other \\nin the embeddings space they will be semantically similar in their text meaning. Also, it \\nis possible to find new semantically similar text phrases by looking nearby in that vector \\nspace. This ability to find similar items in very large data sets with very low latency using \\nvector databases is critical for many production use cases today including search, \\nrecommendations, advertising, fraud detection and many more. Note that while the latitude \\nand longitude embedding model was designed based on the spherical shape of the earth, \\nthe embedding space for text is learned by the neural network model. Importantly, the \\nembeddings learned by different models will not be comparable to each other and it is \\ncritical to make sure in practice that compatible and consistent versions of embeddings are \\nbeing used.\\nKey applications for embeddings are retrieval and recommendations, where the results are \\nusually selected from a massive search space. For example, Google Search is a retrieval task \\nover the search space of the entire internet. Today’s retrieval and recommendation systems’ \\nsuccess depends on the following steps:\\n1.\\t Precomputing the embeddings for billions of items in the search space.\\n2.\\t Mapping query embeddings into the same embedding space.\\n3.\\t Efficient computing and retrieving of the items whose embeddings are the nearest \\nneighbors of the query embeddings in the search space.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T20:31:04+05:30', 'trapped': '', 'modDate': \"D:20251109203104+05'30'\", 'creationDate': \"D:20250317134132-06'00'\", 'page': 7}, page_content='Embeddings & Vector Stores\\n8\\nFebrurary 2025\\nEmbeddings also shine in the world of multimodality. Many applications work with large \\namounts of data of various modalities: text, speech, image, and videos to name a few. \\nJoint embeddings are when multiple types of objects are being mapped into the same \\nembeddings space, for example retrieving videos based on text queries. These embedding \\nrepresentations are designed to capture as much of the original object’s characteristics \\nas possible.\\nFigure 1. Projecting objects/content into a joint vector space with semantic meaning\\nEmbeddings are designed so they place objects with similar semantic properties closer in \\nthe embedding space (a low-dimensional vector space where items can be projected). The \\nembeddings can then be used as a condensed, meaningful input in downstream applications. \\nFor example, you can use them as features for ML models, recommender systems, search'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T20:31:04+05:30', 'trapped': '', 'modDate': \"D:20251109203104+05'30'\", 'creationDate': \"D:20250317134132-06'00'\", 'page': 8}, page_content='Embeddings & Vector Stores\\n9\\nFebrurary 2025\\nengines, and many more. So your data not only gets a compact numerical representation, \\nbut this representation also preserves the semantic meanings for a specific task or across \\na variety of tasks. The fact that these representations are task-specific means you can \\ngenerate different embeddings for the same object, optimized for the task at hand. \\nEvaluating Embedding Quality\\nEmbedding models are evaluated differently depending on the task. Many of the common \\nmetrics for evaluating quality focus on the ability to retrieve similar items, while excluding \\nitems that are not similar. This type of evaluation requires a labeled datasets for which the \\nrelevant, or correct, documents are already known as seen in Snippet 0 where the NFCorpus \\ndataset is used to illustrate different metrics.  For the search use case described above two \\nimportant metrics for evaluating quality are: 1) precision - all documents retrieved should \\nbe relevant and 2) recall - all of the relevant documents should be retrieved. Intuitively, the \\noptimal embedding model would retrieve all of the relevant documents and no documents \\nthat weren’t relevant, however it is often the case that some relevant documents are \\nexcluded and some irrelevant ones get retrieved so more quantitative definitions are required \\nfor evaluating quality over large sets of documents and embedding models. Precision is \\nquantified by dividing the number of relevant documents by the total number of retrieved \\ndocuments. It is often quoted for a particular number of retrieved documents. For example \\nif ten documents were retrieved for an embedding and seven of them were relevant and \\nother three were not,  the precsion@10 would be 7/10 = 0.7. Recall looks at how many of \\nthe relevant documents were retrieved and is calculated by dividing the number of relevant \\ndocuments retrieved by the total number of relevant documents in the corpus. Recall is also \\noften quoted for a particular number of documents retrieved. For example, if 20 documents \\nwere retrieved and three of them were relevant, but there were six total relevant documents \\nin the corpus the recall@20 would be 3/6 = 0.5.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T20:31:04+05:30', 'trapped': '', 'modDate': \"D:20251109203104+05'30'\", 'creationDate': \"D:20250317134132-06'00'\", 'page': 9}, page_content='Embeddings & Vector Stores\\n10\\nFebrurary 2025\\nPrecision and recall are very useful when relevancy scores are binary, but don’t capture \\nthe case when some documents are more relevant than others. For example, when using a \\nsearch engine it is highly desirable that the most relevant result is at the top of the results \\nlist as end users are sensitive to the ordering of those results, even if they are all relevant. \\nWhen the detailed ordering of document relevancy is known for a data set, metrics like the \\nNormalized Discounted Cumulative Gain (nDCG) can measure the quality of the ranking \\nproduced by the embedding model compared to the desired ranking. The formula at\\nposition p for DCG =                           where reli is a relevancy score. The denominator \\npenalizes documents for being lower on the list, and DCG maximizes the score when most \\nrelevant documents are at the top of the list. The normalized version is calculated by dividing \\nthe DCG score by the ideal ordering score and ranges from 0.0 to 1.0 for comparisons across \\ndifferent queries. \\nPublic benchmarks like BEIR42 are widely used for evaluating performance on retrieval tasks \\nand additional tasks are covered by benchmarks like the Massive Text Embedding Benchmark \\n(MTEB)43. Practitioners are encouraged to use a standard library like those originated by \\nText Retrieval Conference (TREC) for consistent benchmarking with other methods, such as \\ntrec_eval44 or python wrappers like pytrec_eval45 when calculating precision, recall, nDCG \\nand others. The optimal way to evaluate embedding models for a particular application \\nmay be application specific, but the intuition that more similar objects should be closer \\nin the embeddings space is often a good start. Additional metrics such as model size, \\nembedding dimension size, latency, and overall cost are also important considerations for \\nproduction applications.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T20:31:04+05:30', 'trapped': '', 'modDate': \"D:20251109203104+05'30'\", 'creationDate': \"D:20250317134132-06'00'\", 'page': 10}, page_content='Embeddings & Vector Stores\\n11\\nFebrurary 2025\\nSearch Example\\nBefore diving into details about the different types of embeddings and the history of \\nembedding model development let’s explore the search example previously described above \\nin more detail. The goal is to find relevant documents in a large corpus given a query from \\nthe user. One approach then is to construct a joint embedding model where the question \\nand answer are mapped to similar locations in the embedding space. As the question and \\nanswer are semantically different, even if complementary, it is often helpful to use two neural \\nnets that have been trained together with one for the question and one for the documents. A \\nvisual representation of this can be seen in Figure 9(b) as an asymmetric dual encoder with \\na separate network for the query and document in contrast to 9(a) displaying a single neural \\nnetwork used for both query and document, also called a siamese network.\\nFigure 2 is a diagram of a search question and answer application using a retrieval \\naugmented generation (RAG) approach where embeddings are used to identify the relevant \\ndocuments before inserting them into the prompt of an LLM for summarization for the \\nend user. The application is split into two main processes. First, the index creation where \\ndocuments are divided into chunks which are used to generate embeddings and stored in a \\nvector database for low latency searches. Specifically, the document embedding portion of \\nthe model of the dual encoder neural network is used for these chunks. The second phase \\nwhen the user asks a question to the system that is embedded using the query portion of the \\nmodel and which will map to relevant documents when using a similarity search in the vector \\ndatabase. This second phase is very latency sensitive as the end user is actively waiting for \\na response so the ability to identify relevant documents from a large corpus in milliseconds \\nusing a vector database of documents is a critical piece of infrastructure.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T20:31:04+05:30', 'trapped': '', 'modDate': \"D:20251109203104+05'30'\", 'creationDate': \"D:20250317134132-06'00'\", 'page': 11}, page_content='Embeddings & Vector Stores\\n12\\nFebrurary 2025\\nFigure 2. Example flow for RAG Search Application highlighting embeddings. Document embeddings are \\ngenerated in the background and stored in a vector database. When the user enters a query, an embedding \\nis generated using the query embedding portion of the dual encoder and used to look up relevant \\ndocuments. Those documents can be inserted into the prompt for LLM to generate a relevant summary \\nresponse for the user.\\nThe quality of embedding models has been improving rapidly since the introduction of \\nBERT and shows no signs of slowing any time soon. While LLMs have captured a lot of the \\nattention in the AI space recently, the improvements in information retrieval and embedding \\nmodels has also been transformative. The original BERT models were a leap forward at the \\ntime, and had an average score of 10.6 on the BEIR benchmark, current 2025 embeddings'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T20:31:04+05:30', 'trapped': '', 'modDate': \"D:20251109203104+05'30'\", 'creationDate': \"D:20250317134132-06'00'\", 'page': 12}, page_content='Embeddings & Vector Stores\\n13\\nFebrurary 2025\\nfrom Google with a simple API call, and no AI knowledge required, now have an average BEIR \\nscore of 55.7. Models continue to improve rapidly, so when putting embedding models into \\nproduction be sure to design with model upgrades in mind. Good evaluation suites designed \\nfor the particular application are critical for ensuring smooth upgrades. Choosing embedding \\nmodels on platforms that have upgrade paths in place can help save developer time and \\nreduce operational overhead for teams without deep AI expertise, for example Snippet 0 \\nbelow uses a simple API call via Google Vertex.\\nSnippet 1 contains basic embedding code sample to illustrate some of the important \\nconcepts covered above for embeddings using the NFCorpus dataset46 that contains health \\nrelated questions and documents:\\n•\\t The text documents with information relevant to the queries are embedded using \\nthe Google Vertex APIs for both high quality and operational ease. The RETRIEVAL_\\nDOCUMENT task type is used as questions and answers are often phrased differently \\nand use a single model with semantic similarity would result in a reduced performance \\ncompared to joint document and query embeddings.\\n•\\t Embeddings are stored using the faiss47 library for efficient similarity search. \\n•\\t For a particular query the text embeddings is generated using the RETRIEVAL_QUERY \\ntask type.\\n•\\t The query embedding is used by the faiss library to look up the ids for documents whose \\nembeddings are close using the default eucldidean distance metric.\\n•\\t Embeddings for all of the queries are generated and most similar documents retrieved. \\nRetrieval quality is evaluated against the “gold” values using the pytrec library to measure \\nprecision@1, recall@10, ndcg@10 metrics.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T20:31:04+05:30', 'trapped': '', 'modDate': \"D:20251109203104+05'30'\", 'creationDate': \"D:20250317134132-06'00'\", 'page': 13}, page_content='Embeddings & Vector Stores\\n14\\nFebrurary 2025\\nfrom beir import util\\nfrom beir.datasets.data_loader import GenericDataLoader\\nimport faiss\\nimport vertexai\\nfrom vertexai.language_models import TextEmbeddingInput, TextEmbeddingModel\\nimport numpy as np\\nimport pandas as pd\\nimport pytrec_eval\\ndef embed_text(texts, model, task, batch_size=5) :\\n embed_mat = np.zeros((len(texts),768))\\n for batch_start in range(0,len(texts),batch_size):\\n   size = min(len(texts) - batch_start, batch_size)\\n   inputs = [TextEmbeddingInput(texts[batch_start+i], task_type=task) for i in range(size)]\\n   embeddings = model.get_embeddings(inputs)\\n   for i in range(size) :\\n     embed_mat[batch_start + i, :] = embeddings[i].values\\n return embed_mat\\n# Download smallish NFCorpus dataset of questions and document text\\nurl = \"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/nfcorpus.zip\"\\ndata_path = util.download_and_unzip(url, \"datasets\")\\t\\n# Corpus of text chunks, text queries and “gold” set of query to relevant documents dict\\ncorpus, queries, qrels = GenericDataLoader(\"datasets/nfcorpus\").load(split=\"test\")\\n# Note need to setup Google Cloud project and fill in id & location below\\nvertexai.init(project=\"PROJECT_ID\", location=\"LOCATION\")\\nmodel = TextEmbeddingModel.from_pretrained(\"text-embedding-005\")\\ndoc_ids,docs = zip(*[(doc_id, doc[\\'text\\']) for doc_id,doc in corpus.items()])\\nq_ids,questions = zip(*[(q_id, q) for q_id,q in queries.items()])\\nContinues next page...'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T20:31:04+05:30', 'trapped': '', 'modDate': \"D:20251109203104+05'30'\", 'creationDate': \"D:20250317134132-06'00'\", 'page': 14}, page_content='Embeddings & Vector Stores\\n15\\nFebrurary 2025\\nSnippet 1. Example semantic search using text embeddings and evaluation for quality of \\nretrieved documents.\\nBoth training and evaluating neural networks requires datasets that contains pairs of \\nquestions and relevant documents such as the NFCorpus used in Snippet 0. The dataset that \\nis best suited to train or evaluate for a particular application will depend on the nature of \\n# Embed the documents and queries jointly using different models\\ndoc_embeddings = embed_text(docs, model, \"RETRIEVAL_DOCUMENT\")\\nindex = faiss.IndexFlatL2(doc_embeddings.shape[1])\\nindex.add(doc_embeddings)\\n# Example look up example query to find relevant doc - note using \\'RETRIEVAL_QUERY\\'\\nexample_embed = embed_text([\\'Is Caffeinated Tea Really Dehydrating?\\'], \\nmodel, \\'RETRIEVAL_QUERY\\')\\ns,q = index.search(example_embed,1)\\nprint(f\\'Score: {s[0][0]:.2f}, Text: \"{docs[q[0][0]]}\"\\')\\n# Score: 0.49, Text: \"There is a belief that caffeinated drinks, such as tea, \\n# may adversely affect hydration. This was investigated in a randomised \\n# controlled trial ... revealed no significant differences\\n# between tea and water for any of the mean blood or urine measurements…”\\n# Embed all queries to evaluate quality compared to \"gold\" answers\\nquery_embeddings = embed_text(questions, model, \"RETRIEVAL_QUERY\")\\nq_scores, q_doc_ids = index.search(query_embeddings, 10)\\n# Create a dict of query to document scores dict for pytrec evaluation \\t\\n# Multiply scores by -1 for sorting as smaller distance is better score for pytrec eval\\nsearch_qrels = { q_ids[i] : { doc_ids[_id] : -1*s.item() for _id, s in zip(q_doc_ids[i], q_\\nscores[i])} for i in range(len(q_ids))}\\nevaluator = pytrec_eval.RelevanceEvaluator(qrels, {\\'ndcg_cut.10\\',\\'P_1\\',\\'recall_10\\'})\\neval_results = evaluator.evaluate(search_qrels)\\ndf = pd.DataFrame.from_dict(eval_results, orient=\\'index\\')\\ndf.mean()\\n#P_1            0.517028 // precision@1\\n#recall_10      0.203507 // recall@10\\n#ndcg_cut_10    0.402624 // nDCG@10'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T20:31:04+05:30', 'trapped': '', 'modDate': \"D:20251109203104+05'30'\", 'creationDate': \"D:20250317134132-06'00'\", 'page': 15}, page_content='Embeddings & Vector Stores\\n16\\nFebrurary 2025\\nthat application. For example, a medical application will use different jargon and conventions \\nthan an application focusing on legal use cases. These labeled datasets can be expensive \\nand time consuming to generate using human experts. The Gecko embedding model paper \\nfrom Google DeepMind48 discusses in detail how an LLM was used to generate a large set \\nof synthetic question and document pairs for training, leading to an improved model and \\nperformance on many benchmarks. Using LLMs to assist experts in generating training data \\nand also for the evaluation of answers can be an effective way to scale training, tuning, and \\nevaluation datasets cost effectively.\\nTypes of embeddings\\nEmbeddings aim to obtain a low dimensional representation of the original data while \\npreserving most of the ‘essential information’. The types of data an embedding represents \\ncan be of various different forms.  Below you’ll see some standard techniques used for \\ndifferent types of data, including text and image.\\nText embeddings\\nText embeddings are used extensively as part of natural language processing (NLP). They \\nare often used to embed the meaning of natural language in machine learning for processing \\nin various downstream applications such as text generation, classification, sentiment \\nanalysis, and more. These embeddings broadly fall into two categories: token/word and \\ndocument embeddings.\\nBefore diving deeper into these categories, it’s important to understand the entire lifecycle \\nof text: from its input by the user to its conversion to embeddings.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T20:31:04+05:30', 'trapped': '', 'modDate': \"D:20251109203104+05'30'\", 'creationDate': \"D:20250317134132-06'00'\", 'page': 16}, page_content='Embeddings & Vector Stores\\n17\\nFebrurary 2025\\nFigure 3. The process of turning text into embeddings\\nIt all starts with the input string which is split into smaller meaningful pieces called tokens.  \\nThis process is called tokenization. Commonly, these tokens are wordpieces, characters, \\nwords, numbers, and punctuations using one of the many existing tokenization techniques.1 \\nAfter the string is tokenized, each of these tokens is then assigned a unique integer value \\nusually in the range: [0, cardinality of the total number of tokens in the corpus]. For example, \\nfor a 16 word vocabulary the IDs would range between 0-15. This value is also referred to as \\ntoken ID. These tokens can be used to represent each string as a sparse numerical vector \\nrepresentation of documents used for downstream tasks directly, or after one-hot encoding. \\nOne-hot encoding is a binary representation of categorical values where the presence \\nof a word is represented by 1, and its absence by 0. This ensures that the token IDs are \\ntreated as categorical values as they are, but often results in a dense vector the size of the \\nvocabulary of the corpus. Snippet 2 and Figure 4 show an example of how this can be done \\nusing Tensorflow.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T20:31:04+05:30', 'trapped': '', 'modDate': \"D:20251109203104+05'30'\", 'creationDate': \"D:20250317134132-06'00'\", 'page': 17}, page_content='Embeddings & Vector Stores\\n18\\nFebrurary 2025\\n# Tokenize the input string data\\nfrom tensorflow.keras.preprocessing.text import Tokenizer\\ndata = [\\n  \"The earth is spherical.\",\\n  \"The earth is a planet.\",\\n  \"I like to eat at a restaurant.\"]\\n# Filter the punctiations, tokenize the words and index them to integers  \\ntokenizer = Tokenizer(num_words=15, filters=\\'!\"#$%&()*+,-./:;<=>?[\\\\\\\\]^_\\'{|}~\\\\t\\\\n\\', lower=True, \\nsplit=\\' \\')\\ntokenizer.fit_on_texts(data)\\n# Translate each sentence into its word-level IDs, and then one-hot encode those IDs \\nID_sequences = tokenizer.texts_to_sequences(data)\\nbinary_sequences = tokenizer.sequences_to_matrix(ID_sequences)\\nprint(\"ID dictionary:\\\\n\", tokenizer.word_index)\\nprint(\"\\\\nID sequences:\\\\n\", ID_sequences)\\nprint(\"\\\\n One-hot encoded sequences:\\\\n\", binary_sequences)\\nSnippet 2. Tokenizing, indexing and one-hot encoding strings\\nFigure 4. Output of Snippet 2\\nHowever, since these Integer IDs (or their corresponding one-hot encoded vectors) are \\nassigned randomly to words, they lack any inherent semantic meaning. This is where \\nembeddings are much more useful. Although it’s possible to embed character and sub-word \\nlevel tokens as well, let us look at word and document embeddings to understand some of \\nthe methods behind them.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T20:31:04+05:30', 'trapped': '', 'modDate': \"D:20251109203104+05'30'\", 'creationDate': \"D:20250317134132-06'00'\", 'page': 18}, page_content='Embeddings & Vector Stores\\n19\\nFebrurary 2025\\nWord embeddings\\nIn this section, you’ll see a few word embedding techniques and algorithms to both train \\nand use word embeddings which were precusors to the modern text embedding currently \\nbeing used. While there are many ML driven algorithms developed over time optimized for \\ndifferent objectives, the most common ones were GloVe,2 SWIVEL,3 and Word2Vec.4 Word \\nembeddings or sub-word embeddings can also be directly obtained from hidden layers of \\nlanguage models. However, the embeddings will be different for the same word in different \\ncontexts of the text. This section focuses on lightweight, context-free word embedding and \\nleaves the context-aware document embeddings for the document embeddings section. \\nWord embedding can be directly applied to downstream tasks like named entity extraction \\nand topic modeling.\\nWord2Vec is a family of model architectures that operates on the principle of “the semantic \\nmeaning of a word is defined by its neighbors”, or words that frequently appear close to each \\nother in the training corpus. This method can be both used to train your own embeddings \\nfrom large datasets or be quickly integrated through one of the readily available pre-trained \\nembeddings available online.5 The embeddings for each word - which are essentially fixed \\nlength vectors - are randomly initialized to kick off the process, resulting in a matrix of shape \\n(size_of_vocabulary, size_of_each_embedding). This matrix can be used as a lookup table \\nafter the training process is completed using one of the following methods (see Figure 4). \\n•\\t The Continuous bag of words (CBOW) approach: Tries to predict the middle word, using \\nthe embeddings of the surrounding words as input. This method is agnostic to the order \\nof the surrounding words in the context. This approach is fast to train and is slightly more \\naccurate for frequent words.\\n•\\t The skip-gram approach: The setup is inverse of that of CBOW, with the middle word \\nbeing used to predict the surrounding words within a certain range. This approach is \\nslower to train but works well with small data and is more accurate for rare words.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T20:31:04+05:30', 'trapped': '', 'modDate': \"D:20251109203104+05'30'\", 'creationDate': \"D:20250317134132-06'00'\", 'page': 19}, page_content='Embeddings & Vector Stores\\n20\\nFebrurary 2025\\nFigure 5. Diagram explaining how CBOW and Skip-Gram methods work\\nThe Word2Vec algorithms can also be extended to the sub-word level, which has been the \\ninspiration for algorithms such as FastText.6 However, one of the major caveats of Word2Vec \\nis that although it accounts well for local statistics of words within a certain sliding window, it \\ndoes not capture the global statistics (words in the whole corpus). This shortcoming is what \\nmethods like the GloVe algorithm address.\\nGloVe is a word embedding technique that leverages both global and local statistics of words. \\nIt does this by first creating a co-occurrence matrix, which represents the relationships \\nbetween words. GloVe then uses a factorization technique to learn word representations \\nfrom the co-occurrence matrix. The resulting word representations are able to capture both \\nglobal and local information about words, and they are useful for a variety of NLP tasks.\\nIn addition to GloVE, SWIVEL is another approach which leverages the co-occurrence \\nmatrix to learn word embeddings. SWIVEL stands for Skip-Window Vectors with Negative \\nSampling. Unlike GloVE, it uses local windows to learn the word vectors by taking into'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T20:31:04+05:30', 'trapped': '', 'modDate': \"D:20251109203104+05'30'\", 'creationDate': \"D:20250317134132-06'00'\", 'page': 20}, page_content='Embeddings & Vector Stores\\n21\\nFebrurary 2025\\naccount the co-occurrence of words within a fixed window of its neighboring words. \\nFurthermore, SWIVEL also considers unobserved co-occurrences and handles it using a \\nspecial piecewise loss, boosting its performance with rare words. It is generally considered \\nonly slightly less accurate than GloVe on average, but is considerably faster to train. This is \\nbecause it leverages distributed training by subdividing the Embedding vectors into smaller \\nsub-matrices and executing matrix factorization in parallel on multiple machines. Snippet 2 \\nbelow demonstrates loading pre-trained word embeddings for both Word2Vec and GloVe and \\nvisualizing them in a 2D space, and computing nearest neighbors.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T20:31:04+05:30', 'trapped': '', 'modDate': \"D:20251109203104+05'30'\", 'creationDate': \"D:20250317134132-06'00'\", 'page': 21}, page_content='Embeddings & Vector Stores\\n22\\nFebrurary 2025\\nfrom gensim.models import Word2Vec \\nimport gensim.downloader as api\\nimport pprint\\nimport matplotlib.pyplot as plt\\nfrom sklearn.manifold import TSNE\\nimport numpy as np\\ndef tsne_plot(models, words, seed=23):\\n  \"Creates a TSNE models & plots for multiple word models for the given words\"\\n  plt.figure(figsize=(len(models)*30, len(models)*30))\\n  model_ix = 0\\n  for model in models:\\n    labels = []\\n    tokens = []\\n    for word in words:\\n      tokens.append(model[word])\\n      labels.append(word)\\n    tsne_model = TSNE(perplexity=40, n_components=2, init=\\'pca\\', n_iter=2500, random_state=seed) \\n    new_values = tsne_model.fit_transform(np.array(tokens))\\n    x = []\\n    y = []\\n    for value in new_values:\\n      x.append(value[0])\\n      y.append(value[1])\\n    model_ix +=1\\n    plt.subplot(10, 10, model_ix)\\n    for i in range(len(x)):\\n      plt.scatter(x[i],y[i])\\n      plt.annotate(labels[i],\\n            xy=(x[i], y[i]),\\n            xytext=(5, 2),\\n            textcoords=\\'offset points\\',\\n            ha=\\'right\\',\\n             va=\\'bottom\\')\\n  plt.tight_layout()\\n  plt.show()\\nv2w_model = api.load(\\'word2vec-google-news-300\\')\\nglove_model = api.load(\\'glove-twitter-25\\')\\nprint(\"words most similar to \\'computer\\' with word2vec and glove respectively:\")\\npprint.pprint(v2w_model.most_similar(\"computer\")[:3])\\npprint.pprint(glove_model.most_similar(\"computer\")[:3]) \\npprint.pprint(\"2d projection of some common words of both models\")\\nsample_common_words= list(set(v2w_model.index_to_key[100:10000])\\n                        & set(glove_model.index_to_key[100:10000]))[:100]\\ntsne_plot([v2w_model, glove_model], sample_common_words)\\nSnippet 3. Loading and plotting GloVe and Word2Vec embeddings in 2D'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T20:31:04+05:30', 'trapped': '', 'modDate': \"D:20251109203104+05'30'\", 'creationDate': \"D:20250317134132-06'00'\", 'page': 22}, page_content='Embeddings & Vector Stores\\n23\\nFebrurary 2025\\nFigure 6 Shows semantically similar words are clustered differently for the two algorithms.\\nFigure 6. 2D visualization of pre-trained GloVe and Word2Vec word embeddings\\nDocument embeddings\\nEmbedding documents to low dimensional dense embedding has attracted long-lasting \\ninterests since the 1980s. Document embeddings can be used in various applications, \\nincluding semantic search, topic discovery, classification, and clustering to embed \\nthe meaning of a series of words in paragraphs and documents and use it for various \\ndownstream applications. The evolution of the embeddings models can mainly be \\ncategorized into two stages: shallow Bag-of-words (BoW) models and deeper pretrained \\nlarge language models.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T20:31:04+05:30', 'trapped': '', 'modDate': \"D:20251109203104+05'30'\", 'creationDate': \"D:20250317134132-06'00'\", 'page': 23}, page_content='Embeddings & Vector Stores\\n24\\nFebrurary 2025\\nShallow BoW models\\nEarly document embedding works follow the bag-of-words (BoW) paradigm, assuming a \\ndocument is an unordered collection of words. These early works include latent semantic \\nanalysis (LSA)7 and latent dirichlet allocation (LDA).8 Latent semantic analysis (LSA) uses \\na co-occurrence matrix of words in documents and latent dirichlet allocation (LDA) uses a \\nbayesian network to model the document embeddings. Another well known bag-of-words \\nfamily of document embeddings is TF-IDF (term frequency-inverse document frequency) \\nbased models, which are statistical models that use the word frequency to represent the \\ndocument embedding. TF-IDF-based models can either be a sparse embedding, which \\nrepresents the term-level importance, or can be combined with word embeddings as a \\nweighting factor to generate a dense embedding for the documents. For example, BM2549, a \\nTF-IDF-based bag-of-words model, is still a strong baseline in today’s retrieval benchmarks.9\\nHowever,  the bag-of-words paradigm also has two major weaknesses: both the word \\nordering and the semantic meanings are ignored. BoW models fail to capture the sequential \\nrelationships between words, which are crucial for understanding meaning and context. \\nInspired by Word2Vec, Doc2Vec10 was proposed in 2014 for generating document \\nembeddings using (shallow) neural networks. The Doc2Vec model adds an additional \\n‘paragraph’ embedding or, in other words, document embedding in the model of Word2Vec \\nas illustrated in Figure 6. The paragraph embedding is concatenated or averaged with other \\nword embeddings to predict a random word in the paragraph. After training, for existing \\nparagraphs or documents, the learned embeddings can be directly used in downstream \\ntasks. For a new paragraph or document, extra inference steps need to be performed to \\ngenerate the paragraph or document embedding.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T20:31:04+05:30', 'trapped': '', 'modDate': \"D:20251109203104+05'30'\", 'creationDate': \"D:20250317134132-06'00'\", 'page': 24}, page_content='Embeddings & Vector Stores\\n25\\nFebrurary 2025\\nFigure 7. Doc2vec CBOW model\\nSnippet 4 below shows how you can train your own doc2Vec models on a custom corpus:\\nfrom gensim.test.utils import common_texts\\nfrom gensim.models.Doc2Vec import Doc2Vec, TaggedDocument\\nfrom gensim.test.utils import get_tmpfile\\n#train model on a sequence of documents tagged with their IDs\\ndocuments = [TaggedDocument(doc, [i]) for i, doc in enumerate(common_texts)]\\nmodel = Doc2Vec(documents, vector_size=8, window=3, min_count=1, workers=6)\\n# persist model to disk, and load it to infer on new documents\\nmodel_file = get_tmpfile(\"Doc2Vec_v1\")\\nmodel.save(model_file)\\nmodel = Doc2Vec.load(model_file)  \\nmodel.infer_vector([\"human\", \"interface\"])\\nSnippet 4. Self-supervised Training and inference using Doc2Vec on private corpus'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T20:31:04+05:30', 'trapped': '', 'modDate': \"D:20251109203104+05'30'\", 'creationDate': \"D:20250317134132-06'00'\", 'page': 25}, page_content='Embeddings & Vector Stores\\n26\\nFebrurary 2025\\nDeeper pretrained large language models\\nMotivated by the development of deep neural networks, different embedding models and \\ntechniques were proposed, and the state-of-the-art models are progressing rapidly. Main \\nchanges of the models include: \\n1.\\t Using more complex learning models, especially bi-directional deep neural network \\nmodels. \\n2.\\t The use of massive pre-training on unlabeled text. \\n3.\\t The use of a subword tokenizer. \\n4.\\t Using fine-tuning for various downstream NLP tasks. \\nIn 2018, BERT11 - which stands for bidirectional encoder representations from transformers - \\nwas proposed with groundbreaking results on 11 NLP tasks. Transformer, the model paradigm \\nBERT based on, has become the mainstream model paradigm until today. Besides using a \\ntransformer as the model backbone, another key of BERT’s success is from pre-training with \\na massive unlabeled corpus. In pretraining, BERT utilized masked language model (MLM) as \\nthe pre-training objective. It did this by randomly masking some tokens of the input and using \\nthe masked token id as the prediction objective. This allows the model to utilize both the \\nright and left context to pretrain a deep bidirectional transformer. BERT also utilizes the next \\nsentence prediction task in pretraining. BERT outputs a contextualized embedding for every \\ntoken in the input. Typically, the embedding of the first token (a special token named [CLS]) is \\nused as the embedding for the whole input.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T20:31:04+05:30', 'trapped': '', 'modDate': \"D:20251109203104+05'30'\", 'creationDate': \"D:20250317134132-06'00'\", 'page': 26}, page_content='Embeddings & Vector Stores\\n27\\nFebrurary 2025\\nFigure 8. The BERT architecture\\nBERT became the base model for multiple embedding models, including Sentence-BERT,12 \\nSimCSE,13 and E5.14 Meanwhile, the evolution of language models - especially large language \\nmodels - never stops. T550 was proposed in 2019 with up to 11B parameters. PaLM51 was \\nproposed in 2022 to push the large language model to a surprising 540B parameters. Models \\nlike Gemini52 from Google, GPT53 models from OpenAI and Llama54 models from Meta are \\nalso evolving to newer generations at astonishing speed. Please refer to the whitepaper on \\nFoundational models for more information about some common LLMs.\\nNew embedding models based on large language models have been proposed. For example, \\nGTR and Sentence-T5 show better performance on retrieval and sentence similarity \\n(respectively) than BERT family models. Recently, a new embedding model powered by the \\nGemini model backbone has been released on Vertex AI, achieving superior results on all \\npublic benchmarks. Matryoshka Embeddings55,56 allow the downstream user to select how \\nmany dimensions are appropriate for their task to reduce data required for storage and \\nindexing when possible.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T20:31:04+05:30', 'trapped': '', 'modDate': \"D:20251109203104+05'30'\", 'creationDate': \"D:20250317134132-06'00'\", 'page': 27}, page_content='Embeddings & Vector Stores\\n28\\nFebrurary 2025\\nAnother approach to new embeddings models development is generating multi-vector \\nembeddings instead of a single vector to enhance the representational power of the models. \\nEmbedding models in this family include ColBERT15 and XTR.16 ColPali57 is also an approach \\nusing mult-vectors, but extending their application from text only to join embedding text and \\nimages for multi-modal documents. \\nFigure 9. An illustration of the taxonomy diagram of the embedding models\\nAlthough the deep neural network models require a lot more data and compute time to train, \\nthey have much better performance compared to models using bag-of-words paradigms. \\nFor example, for the same word the embeddings would be different with different contexts, \\nbut by definition that is not true for bag-of-words. Snippet 4 demonstrates how pre-trained \\ndocument embedding models from Tensorflow-hub17 (for example,Sentence t5)A and Vertex \\nAIB can be used for training models with Keras and TF datasets. Vertex Generative AI text \\nembeddings can be used with the Vertex AI SDK, Langchain, and Google’s BigQuery (Snippet \\n5) for embedding and advanced workflows.18\\nA. Note: not all models on https://tfhub.dev/ can be commercially used. Please check the licenses of the models \\nand the training datasets and consult the legal team before commercial usage.\\t\\nB. Note: not all models on https://tfhub.dev/ can be commercially used. Please check the licenses of the models \\nand the training datasets and consult the legal team before commercial usage.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T20:31:04+05:30', 'trapped': '', 'modDate': \"D:20251109203104+05'30'\", 'creationDate': \"D:20250317134132-06'00'\", 'page': 28}, page_content='Embeddings & Vector Stores\\n29\\nFebrurary 2025\\nimport vertexai\\nfrom vertexai.language_models import TextEmbeddingInput, TextEmbeddingModel\\n# Set the model name. For multilingual: use \"text-multilingual-embedding-002\"\\nMODEL_NAME = \"text-embedding-004\"\\n# Set the task_type, text and optional title as the model inputs.\\n# Available task_types are \"RETRIEVAL_QUERY\", \"RETRIEVAL_DOCUMENT\", \\n# \"SEMANTIC_SIMILARITY\", # \"CLASSIFICATION\", and \"CLUSTERING\"\\nTASK_TYPE = \"RETRIEVAL_DOCUMENT\" \\nTITLE = \"Google\"\\nTEXT = \"Embed text.\"\\n# Use Vertex LLM text embeddings\\nembeddings_vx = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@004\")\\ndef LLM_embed(text):\\n    def embed_text(text):\\n        text_inp = TextEmbeddingInput(task_type=\"CLASSIFICATION\",   text=text.numpy())\\n        return np.array(embeddings_vx.get_embeddings([text_inp])[0].values)\\n\\t\\noutput = tf.py_function(func=embed_text, inp=[text], Tout=tf.float32)\\n\\t\\noutput.set_shape((768,))\\n\\t\\nreturn output\\n# Embed strings using vertex LLMs\\nLLM_embeddings=train_data.map(lambda x,y: ((LLM_embed(x), y))\\n# Embed strings in the tf.dataset using one of the tf hub models\\nembedding = \"https://tfhub.dev/google/sentence-t5/st5-base/1\"\\nhub_layer = hub.KerasLayer(embedding, input_shape=[],dtype=tf.string, trainable=True)\\n                          \\n# Train model \\nmodel = tf.keras.Sequential()\\nmodel.add(hub_layer) # omit this layer if using Vertex LLM embeddings\\nmodel.add(tf.keras.layers.Dense(16, activation=\\'relu\\'))\\nmodel.add(tf.keras.layers.Dense(1))\\nmodel.compile(optimizer=\\'adam\\',loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\\n  metrics=[\\'accuracy\\'])\\nhistory = model.fit(train_data.shuffle(100).batch(8))\\nSnippet 4. Creating & integrating text embeddings (Vertex, Tfhub) into keras text classification models'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T20:31:04+05:30', 'trapped': '', 'modDate': \"D:20251109203104+05'30'\", 'creationDate': \"D:20250317134132-06'00'\", 'page': 29}, page_content='Embeddings & Vector Stores\\n30\\nFebrurary 2025\\nSELECT * FROM ML.GENERATE_TEXT_EMBEDDING(\\nMODEL my_project.my_company.llm_embedding_model,\\n(\\nSELECT review as content\\nFROM bigquery-public-data.imdb.reviews));\\nSnippet 5. Creating LLM based text embeddings in BigQuery for selected columns in a table\\nImage & multimodal embeddings\\nMuch like text, it’s also possible to create both image and multimodal embeddings. \\nUnimodal image embeddings can be derived in many ways such as by training a CNN or \\nVision Transformer model on a large scale image classification task (for example, Imagenet), \\nand then using the penultimate layer as the image embedding. This layer has learnt some \\nimportant discriminative feature maps for the training task. It contains a set of feature maps \\nthat are discriminative for the task at hand and can be extended to other tasks as well. \\nTo obtain multimodal embeddings19 you take the individual unimodal text and image \\nembeddings and create the joint embedding of their semantic relationships learnt via another \\ntraining process. This gives you a fixed size semantic representation in the same latent \\nspace. Snippet 6 computes image and multimodal embeddings for images and text and can \\nbe used with a keras model directly (much like the text embedding example). Multimodal \\nembedding approaches like ColPali57 use image models to enable retrieval from text queries \\non multimodal documents without complex OCR or layout preprocesing. The model searches \\nthe images as they would be displayed to a user in a web browser or pdf viewer rahter than \\nhaving to convert to a text only form for indexing.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T20:31:04+05:30', 'trapped': '', 'modDate': \"D:20251109203104+05'30'\", 'creationDate': \"D:20250317134132-06'00'\", 'page': 30}, page_content='Embeddings & Vector Stores\\n31\\nFebrurary 2025\\nimport base64\\nimport tensorflow as tf\\nfrom google.cloud import aiplatform\\nfrom google.protobuf import struct_pb2\\n#fine-tunable layer for image embeddings which can be used for downstream keras modelimage_\\nembed=hub.KerasLayer(\"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_s/feature_\\nvector/2\",trainable=False) \\nclass EmbeddingPredictionClient:\\n  \"\"\"Wrapper around Prediction Service Client.\"\"\"\\n  def __init__(self, project : str,\\n    location : str = \"us-central1\",\\n    api_regional_endpoint: str = \"us-central1-aiplatform.googleapis.com\"):\\n    client_options = {\"api_endpoint\": api_regional_endpoint}\\n    self.client = aiplatform.gapic.PredictionServiceClient(client_options=client_options)  \\n    self.location = location\\n    self.project = project\\n  def get_embedding(self, text : str = None, gs_image_path : str = None):\\n   #load the image from a bucket in google cloud storage\\n   with tf.io.gfile.GFile(gs_image_path, \"rb\") as f:\\n     image_bytes = f.read()\\n   if not text and not image_bytes:\\n    raise ValueError(\\'At least one of text or image_bytes must be specified.\\')\\n   #Initialize a protobuf data struct with the text and image inputs \\n   instance = struct_pb2.Struct()\\n    if text:\\n      instance.fields[\\'text\\'].string_value = text\\n      if image_bytes:\\n      encoded_content = base64.b64encode(image_bytes).decode(\"utf-8\")\\n      image_struct = instance.fields[\\'image\\'].struct_value\\n      image_struct.fields[\\'bytesBase64Encoded\\'].string_value = .string_value = encoded_content\\n     #Make predictions using the multimodal embedding model\\n     instances = [instance]\\n     endpoint = (f\"projects/{self.project}/locations/{self.location}\"\\n         \"/publishers/google/models/multimodalembedding@001\")\\n     response = self.client.predict(endpoint=endpoint, instances=instances)\\n     text_embedding = None\\n     if text:    \\n      text_emb_value = response.predictions[0][\\'textEmbedding\\']\\n      text_embedding = [v for v in text_emb_value]\\n     image_embedding = None\\n     if image_bytes:    \\n      image_emb_value = response.predictions[0][\\'textEmbedding\\']\\n      image_embedding = [v for v in image_emb_value]\\nContinues next page...'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T20:31:04+05:30', 'trapped': '', 'modDate': \"D:20251109203104+05'30'\", 'creationDate': \"D:20250317134132-06'00'\", 'page': 31}, page_content='Embeddings & Vector Stores\\n32\\nFebrurary 2025\\n \\n return EmbeddingResponse (text_embedding=text_embedding, image_embedding=image_embedding)\\n#compute multimodal embeddings for text and images\\nclient.get_embedding(text=\"sample_test\", gs_image_path=\"gs://bucket_name../image_filename..\")\\nSnippet 6. Using Vertex API to create Multimodal embeddings Graph embeddings\\nStructured data embeddings\\nStructured data refers to data has a defined schema, like an table in a database \\nwhere individual fields have known types and definitions. Unlike unstructured text and \\nimage data, where a pre-trained embedding model is typically available, we have to \\ncreate the embedding model for the structured data since it would be specific to a \\nparticular application.\\nGeneral structured data\\nGiven a general structured data table, we can create embeddings for each row. This can be \\ndone by the ML models in the dimensionality reduction category, such as the PCA model.\\nOne use case for these embeddings are for anomaly detection. For example, we can create \\nembeddings for anomaly detection using large data sets of labeled sensor information \\nthat identify anomalous occurrences.20 Another case use is to feed these embeddings \\nto downstream ML tasks such as classification. Compared to using the original high-\\ndimensional data, using embeddings to train a supervised model requires less data. This is \\nparticularly important in cases where training data is not sufficient.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T20:31:04+05:30', 'trapped': '', 'modDate': \"D:20251109203104+05'30'\", 'creationDate': \"D:20250317134132-06'00'\", 'page': 32}, page_content='Embeddings & Vector Stores\\n33\\nFebrurary 2025\\nUser/item structured data\\nThe input is no longer a general structured data table as above. Instead, the input includes \\nthe user data, item/product data plus the data describing the interaction between user and \\nitem/product, such as rating score. \\nThis category is for recommendation purposes, as it maps two sets of data (user dataset, \\nitem/product/etc dataset) into the same embedding space. For recommender systems, we \\ncan create embeddings out of structured data that correlate to different entities such as \\nproducts, articles, etc. Again, we have to create our own embedding model. Sometimes this \\ncan be combined with unstructured embedding methods when images or text descriptions \\nare found.\\nGraph embeddings\\nGraph embeddings are another embedding technique that lets you represent not \\nonly information about a specific object but also its neighbors (namely, their graph \\nrepresentation). Take an example of a social network where each person is a node, and the \\nconnections between people are defined as edges. Using graph embedding you can model \\neach node as an embedding, such that the embedding captures not only the semantic \\ninformation about the person itself, but also its relations and associations hence enriching \\nthe embedding. For example, if two nodes are connected by an edge, the vectors for those \\nnodes would be similar. You might then be able to predict who the person is most similar \\nto and recommend new connections. Graph embeddings can also be used for a variety of \\ntasks, including node classification, graph classification, link prediction, clustering, search, \\nrecommendation systems, and more. Popular algorithms21,22 for graph embedding include \\nDeepWalk, Node2vec, LINE, and GraphSAGE.23'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T20:31:04+05:30', 'trapped': '', 'modDate': \"D:20251109203104+05'30'\", 'creationDate': \"D:20250317134132-06'00'\", 'page': 33}, page_content='Embeddings & Vector Stores\\n34\\nFebrurary 2025\\nTraining Embeddings\\nCurrent embedding models usually use dual encoder (two tower) architecture. For example, \\nfor the text embedding model used in question-answering, one tower is used to encode \\nthe queries and the other tower is used to encode the documents. For the image and text \\nembedding model, one tower is used to encode the images and the other tower is used \\nto encode the text. The model can have various sub architectures, depending on how the \\nmodel components are shared between the two towers. The following figure shows some \\narchitectures of the dual encoders.24 \\nFigure 10. Some architectures of dual encoders\\nThe loss used in embedding models training is usually a variation of contrastive loss, which \\ntakes a tuple of <inputs, positive targets, [optional] negative targets> as the inputs. Training \\nwith contrastive loss brings positive examples closer and negative examples far apart.\\nSimilar to foundation model training, training of an embedding model from scratch usually \\nincludes two stages: pretraining (unsupervised learning) and fine tuning (supervised \\nlearning). Nowadays, the embedding models are usually directly initialized from foundation \\nmodels such as BERT, T5, GPT, Gemini, CoCa. You can use these base models to leverage the \\nmassive knowledge that has been learned from the large-scale pretraining of the foundation'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T20:31:04+05:30', 'trapped': '', 'modDate': \"D:20251109203104+05'30'\", 'creationDate': \"D:20250317134132-06'00'\", 'page': 34}, page_content='Embeddings & Vector Stores\\n35\\nFebrurary 2025\\nmodels. The fine-tuning of the embedding models can have one or more phases. The fine-\\ntuning datasets can be created in various methods, including human labeling, synthetic \\ndataset generation, model distillation, and hard negative mining.\\nTo use embeddings for downstream tasks like classification or named entity recognition, \\nextra layers (for example, softmax classification layer) can be added on top of the embedding \\nmodels. The embedding model can either be frozen (especially when the training dataset is \\nsmall), trained from scratch, or fine-tuned together with the downstream tasks. \\nVertex AI provides the ability to customize the Vertex AI text embedding models.25 Users can \\nalso choose to fine-tune the models directly. An example is fine tuning the BERT model using \\ntensorflow model garden26. You can also directly load the embedding models from tfhub and \\nfine-tune on top of the model. Snippet 7 shows an example how to build a classifier based on \\ntfhub models. \\n# Can switch the embedding to different embeddings from different modalities on # \\ntfhub. Here we use the BERT model as an example.\\ntfhub_link = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\"\\nclass Classifier(tf.keras.Model):\\n    def __init__(self, num_classes):\\n      super(Classifier, self).__init__(name=\"prediction\")\\n      \\t self.encoder = hub.KerasLayer(tfhub_link, trainable=True)\\n      self.dropout = tf.keras.layers.Dropout(0.1)\\n      self.dense = tf.keras.layers.Dense(num_classes)\\n    def call(self, preprocessed_text):\\n      encoder_outputs = self.encoder(preprocessed_text)\\n      pooled_output = encoder_outputs[\"pooled_output\"]\\n   x = self.dropout(pooled_output)\\n   x = self.dense(x)\\n   return x\\nSnippet 7. Creating a Keras model using trainable tfhub layer'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T20:31:04+05:30', 'trapped': '', 'modDate': \"D:20251109203104+05'30'\", 'creationDate': \"D:20250317134132-06'00'\", 'page': 35}, page_content='Embeddings & Vector Stores\\n36\\nFebrurary 2025\\nSo far you’ve seen the various types of embeddings, techniques and best practices to train \\nthem for various data modalities, and some of their applications. The next section discusses \\nhow to persist and search the embeddings that have been created in a fast and scalable way \\nfor production workloads.\\nVector search\\nFull-text keyword search has been the lynchpin of modern IT systems for years. Full-text \\nsearch engines and databases (relational and non-relational) often rely on explicit keyword \\nmatching. For example, if you search for ‘cappuccino’ the search engine or database returns \\nall documents that mention the exact query in the tags or text description. However, if the \\nkey word is misspelled or described with a differently worded text, a traditional keyword \\nsearch returns incorrect or no results. There are traditional approaches which are tolerant of \\nmisspellings and other typographical errors. However, they are still unable to find the results \\nhaving the closest underlying semantic meanings to the query. This is where vector search \\nis very powerful: it uses the vector or embedded semantic representation of documents. As \\nvector search works on any sort of embedding it also allows search on images, videos, and \\nother data types in addition to text.\\nVector search lets you to go beyond searching for exact query literals and allows you to \\nsearch for the meaning across various data modalities. This allows you to find relevant \\nresults even when the wording is different. After you have a function that can compute \\nembeddings of various items,  you compute the embedding of the items of interest and store \\nthis embedding in a database. You then embed the incoming query in the same vector space \\nas the items. Next, you have to find the best matches to the query. This process is analogous'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T20:31:04+05:30', 'trapped': '', 'modDate': \"D:20251109203104+05'30'\", 'creationDate': \"D:20250317134132-06'00'\", 'page': 36}, page_content='Embeddings & Vector Stores\\n37\\nFebrurary 2025\\nto finding the most ‘similar’ matches across the entire collection of searchable vectors: \\nsimilarity between vectors can be computed using a metric such as euclidean distance, \\ncosine similarity, or dot product.\\nFigure 11. Visualization of how different metrics compute vector similarity\\nEuclidean distance (i.e., L2 distance) is a geometric measure of the distance between two \\npoints in a vector space. This works well for lower dimensions. Cosine similarity is a measure \\nof the angle between two vectors. And inner/dot product, is the projection of one vector \\nonto another. They are equivalent when the vector norms are 1. This seems to work better \\nfor higher dimensional data. Vector databases store and help manage and operationalize the \\ncomplexity of vector search at scale, while also addressing the common database needs.\\nImportant vector search algorithms\\nThe most straightforward way to find the most similar match is to run a traditional linear \\nsearch by comparing the query vector with each document vector and return the one with \\nthe highest similarity. However, the runtime of this approach scales linearly (O(N)) with the \\namount of documents or items to search. This approach is unacceptably slow for most'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T20:31:04+05:30', 'trapped': '', 'modDate': \"D:20251109203104+05'30'\", 'creationDate': \"D:20250317134132-06'00'\", 'page': 37}, page_content='Embeddings & Vector Stores\\n38\\nFebrurary 2025\\nuse cases involving several millions of documents or more. Using approximate nearest \\nneighbour (ANN) search for that purpose is more practical.  ANN is a technique for finding \\nthe closest points to a given point in a dataset with a small margin of error - but with far less \\ncomputations required as the search space is greatly reduced to O(logN). There are many \\napproaches with varying trade-offs across scale, indexing time, performance, simplicity and \\nmore.26 They use one or more implementations of the following techniques: quantization, \\nhashing, clustering and trees, among others. Some of the most popular approaches are \\ndiscussed below.\\nLocality sensitive hashing & trees\\nLocality sensitive hashing (LSH) 27 is a technique for finding similar items in a large dataset. \\nIt does this by creating one or more hash functions that map similar items to the same hash \\nbucket with high probability. This means that you can quickly find all of the similar items to \\na given item by only looking at the candidate items in the same hash bucket (or adjacent \\nbuckets) and do a linear search amongst those candidate pairs. This allows for significantly \\nfaster lookups within a specific radius. The number of hash functions/tables and buckets \\ndetermine the search recall/speed tradeoff, as well as the false positive / true positive one. \\nHaving too many hash functions might cause similar items to different buckets, while too few \\nmight result in too many items falsely being hashed to the same bucket and the number of \\nlinear searches to increase.\\nAnother intuitive way to think about LSH is grouping residences by their postal code or \\nneighborhood name. Then based on where someone chooses to move you look at the \\nresidences for only that neighborhood and find the closest match.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T20:31:04+05:30', 'trapped': '', 'modDate': \"D:20251109203104+05'30'\", 'creationDate': \"D:20250317134132-06'00'\", 'page': 38}, page_content='Embeddings & Vector Stores\\n39\\nFebrurary 2025\\nFigure 12. Visualization of how LSH uses random hyperplanes to partition the vector space\\nTree-based algorithms work similarly. For example, the Kd-tree approach works by creating \\nthe decision boundaries by computing the median of the values of the first dimension, then \\nthat of the second dimension and so on. This approach is very much like a decision tree. \\nNaturally this can be ineffective if searchable vectors are high dimensional. In that case, the \\nBall-tree algorithm is better suited. It is similar in functionality, except instead of going by \\ndimension-wise medians it creates buckets based on the radial distance of the data points \\nfrom the center. Here is an example of the implementation of these three approaches:'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T20:31:04+05:30', 'trapped': '', 'modDate': \"D:20251109203104+05'30'\", 'creationDate': \"D:20250317134132-06'00'\", 'page': 39}, page_content='Embeddings & Vector Stores\\n40\\nFebrurary 2025\\nfrom sklearn.neighbors import NearestNeighbors\\nfrom vertexai.language_models import TextEmbeddingModel\\nfrom lshashing import LSHRandom\\nimport numpy as np\\nmodel = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@004\")\\ntest_items= [\\n  \"The earth is spherical.\",\\n  \"The earth is a planet.\",\\n  \"I like to eat at a restaurant.\"]\\nquery = \"the shape of earth\"\\nembedded_test_items = np.array([embedding.values for embedding in model.get_embeddings(test_items)])\\nembedded_query = np.array(model.get_embeddings([query])[0].values)\\n#Naive brute force search\\nn_neighbors=2\\nnbrs = NearestNeighbors(n_neighbors=n_neighbors, algorithm=\\'brute\\').fit(embedded_test_items)\\nnaive_distances, naive_indices = nbrs.kneighbors(np.expand_dims(embedded_query, axis = 0))\\n#algorithm- ball_tree due to high dimensional vectors or kd_tree otherwise\\nnbrs = NearestNeighbors(n_neighbors=n_neighbors, algorithm=\\'ball_tree\\').fit(embedded_test_items) \\ndistances, indices = nbrs.kneighbors(np.expand_dims(embedded_query, axis = 0))\\n#LSH\\nlsh_random_parallel = LSHRandom(embedded_test_items, 4, parallel = True)\\nlsh_random_parallel.knn_search(embedded_test_items, embedded_query, n_neighbors, 3, parallel = True)\\n#output for all 3 indices = [0, 1] , distances [0.66840428, 0.71048843] for the first 2 neighbours\\n#ANN retrieved the same ranking of items as brute force in a much scalable manner\\nSnippet 8. Using scikit-learn28 and lshashing29 for ANN with LSH, KD/Ball-tree and linear search\\nHashing and tree-based approaches can also be combined and extended upon to obtain \\nthe optimal tradeoff between recall and latency for search algorithms. FAISS with HNSW and \\nScaNN32,33 are good examples.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T20:31:04+05:30', 'trapped': '', 'modDate': \"D:20251109203104+05'30'\", 'creationDate': \"D:20250317134132-06'00'\", 'page': 40}, page_content='Embeddings & Vector Stores\\n41\\nFebrurary 2025\\nHierarchical navigable small worlds \\nFigure 13. Diagram showing how HNSW ‘zooms in’ to perform ANN\\nOne of the FAISS (Facebook AI similarity search) implementations leverages the concept \\nof hierarchical navigable small world (HNSW)30 to perform vector similarity search in sub-\\nlinear (O(Logn)) runtime with a good degree of accuracy. A HNSW is a proximity graph with a \\nhierarchical structure where the graph links are spread across different layers. The top layer \\nhas the longest links and the bottom layer has the shortest ones. As shown in Figure 13, the \\nsearch starts at the topmost layer where the algorithm greedily traverses the graph to find \\nthe vertex most semantically similar to the query. Once the local minimum for that layer is \\nfound, it then switches to the graph for the closest vertex on the layer below. This process \\ncontinues iteratively until the local minimum for the lowest layer is found, with the algorithm \\nkeeping track of all the vertices traversed to return the K-nearest neighbors. This algorithm \\ncan be optionally augmented with quantization and vector indexing to boost speed and \\nmemory efficiency.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T20:31:04+05:30', 'trapped': '', 'modDate': \"D:20251109203104+05'30'\", 'creationDate': \"D:20250317134132-06'00'\", 'page': 41}, page_content='Embeddings & Vector Stores\\n42\\nFebrurary 2025\\n# Create an endpoint\\nmy_index_endpoint = aiplatform.MatchingEngineIndexEndpoint.create(\\n\\t\\ndisplay_name=f\"{DISPLAY_NAME}-endpoint\", public_endpoint_enabled=True\\n)\\n# NOTE : This operation can take upto 20 minutes\\nmy_index_endpoint = my_index_endpoint.deploy_index(\\n\\t\\nindex=my_index, deployed_index_id=DEPLOYED_INDEX_ID\\n)\\n# retrieve the id of the most recently deployed index or manually look up the index \\ndeployed above\\nindex_id=my_index_endpoint.deployed_indexes[-1].index.split(\"/\")[-1]\\nendpoint_id= my_index_endpoint.name\\n# TODO : replace 1234567890123456789 with your acutial index ID\\nmy_index = aiplatform.MatchingEngineIndex(index_id)\\n# TODO : replace 1234567890123456789 with your acutial endpoint ID\\n# Be aware that the Index ID differs from the endpoint ID\\nmy_index_endpoint = aiplatform.MatchingEngineIndexEndpoint(endpoint_id)\\n# Input texts\\ntexts= [\\n   \"The earth is spherical.\",\\n   \"The earth is a planet.\",\\n   \"I like to eat at a restaurant.\",\\n]\\n# Create a Vector Store\\nvector_store = VectorSearchVectorStore.from_components(\\n    project_id=PROJECT_ID,\\n    region=REGION,\\n    gcs_bucket_name=BUCKET,\\n    index_id=my_index.name,\\n    endpoint_id=my_index_endpoint.name,\\n    embedding=embedding_model,\\n    stream_update=True,\\n)\\n# Add vectors and mapped text chunks to your vectore store\\nvector_store.add_texts(texts=texts)\\n# Initialize the vectore_store as retriever\\nretriever = vector_store.as_retriever()\\nContinues next page...'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T20:31:04+05:30', 'trapped': '', 'modDate': \"D:20251109203104+05'30'\", 'creationDate': \"D:20250317134132-06'00'\", 'page': 42}, page_content='Embeddings & Vector Stores\\n43\\nFebrurary 2025\\nSnippet 9. Build/deploy ANN Index for Vertex AI Vector Search and use RAG with LLM prompts to generate \\ngrounded results/sources. \\nretriever=vector_store.as_retriever(search_kwargs={\\'k\\':1 })\\n#create custom prompt for your use case\\nprompt_template=\"\"\"You are David, an AI knowledge bot.\\nAnswer the questions using the facts provided. Use the provided pieces of context to answer\\nthe users question.\\nIf you don\\'t know the answer, just say that \"I don\\'t know\", don\\'t try to make up an answer.\\n{summaries}\"\"\"\\nmessages = [\\n    SystemMessagePromptTemplate.from_template(prompt_template),\\n    HumanMessagePromptTemplate.from_template(\"{question}\")\\n]\\nprompt = ChatPromptTemplate.from_messages(messages)\\nchain_type_kwargs = {\"question\": prompt}\\n#initialize your llm model\\nllm = VertexAI(model_name=\"gemini-pro\")\\n#build your chain for RAG+C\\nchain= RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", \\nretriever=retriever, return_source_documents=True)\\n#print your results with Markup language\\ndef print_result(result):\\n  output_text = f\"\"\"### Question:\\n  {query}\\n  ### Answer:\\n  {result[\\'result\\']}\\n  ### Source:\\n  {\\' \\'.join(list(set([doc.page_content for doc in result[\\'source_documents\\']])))}\\n  \"\"\"\\n  return(output_text)\\nchain= \"What shape is the planet where humans live?\"\\nresult = chain(query)\\ndisplay(Markdown(print_result(result)))'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T20:31:04+05:30', 'trapped': '', 'modDate': \"D:20251109203104+05'30'\", 'creationDate': \"D:20250317134132-06'00'\", 'page': 43}, page_content='Embeddings & Vector Stores\\n44\\nFebrurary 2025\\nimport faiss\\nM=32 #creating high degree graph:higher recall for larger index & searching time\\nd=768 # dimensions of the vectors/embeddings\\nindex = faiss.IndexHNSWFlat(d, M)\\nindex.add(embedded_test_items) #build the index using the embeddings in Snippet 9\\n#execute the ANN search\\nindex.search(np.expand_dims(embedded_query, axis=0), k=2)\\nSnippet 10. Indexing and executing ANN search with the FAISS library using HNSW\\nScaNN\\nGoogle developed the scalable approximate nearest neighbor (ScaNN)31,32 approach which \\nis used across a lot of its products and services. This includes being externally available \\nto all customers of Google Cloud through the Vertex AI Vector Search and Google Cloud \\nDatabases, including AlloyDB, Cloud Spanner, and Cloud SQL MySQL. Below is how ScaNN \\nuses a variety of steps to perform efficient vector search, with each one of them having their \\nown subset of parameters. \\nThe first step is the optional partitioning step during training: it uses one of the multiple \\nalgorithms available to partition the vector store into logical partitions/clusters where \\nthe semantically related are grouped together. The partitioning step is optional for small \\ndatasets. However, for larger datasets with >100k embedding vectors, the partitioning step \\nis crucial since by pruning the search space it cuts down the search space by magnitudes \\ntherefore significantly speeds up the query. The space pruning is configured through the \\nnumber of partitions and the number of partitions to search. A larger number leads to better \\nrecall but larger partition creation time. A good heuristic is to set the number of partitions to \\nbe the square root of the number of vectors.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T20:31:04+05:30', 'trapped': '', 'modDate': \"D:20251109203104+05'30'\", 'creationDate': \"D:20250317134132-06'00'\", 'page': 44}, page_content='Embeddings & Vector Stores\\n45\\nFebrurary 2025\\nFigure 14. Search space partitioning & pruning (left) & Approximate scoring (right)\\nAt query time ScaNN uses the user-specified distance measure to select the specified \\nnumber of top partitions (a value specified by the user), and then executes the scoring \\nstep next. In this step ScaNN compares the query with all the points in the top partitions \\nand selects the top K’. This distance computation can be configured as exact distance or \\napproximate distance. The approximate distance computation leverages either standard \\nproduct quantization or anisotropic quantization techniques, the latter of which is a specific \\nmethod employed by ScaNN which gives the better speed and accuracy tradeoffs.\\nFinally, as a last step the user can optionally choose to rescore the user specified top K \\nnumber of results more accurately. This results in an industry leading speed/accuracy \\ntradeoff ScaNN is known for as can be inferred from Figure 14. Snippet 10 shows a \\ncode example.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T20:31:04+05:30', 'trapped': '', 'modDate': \"D:20251109203104+05'30'\", 'creationDate': \"D:20250317134132-06'00'\", 'page': 45}, page_content='Embeddings & Vector Stores\\n46\\nFebrurary 2025\\nFigure 15. Accuracy/speed tradeoffs for various SOTA ANN search algorithms58'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T20:31:04+05:30', 'trapped': '', 'modDate': \"D:20251109203104+05'30'\", 'creationDate': \"D:20250317134132-06'00'\", 'page': 46}, page_content='Embeddings & Vector Stores\\n47\\nFebrurary 2025\\nimport tensorflow as tf\\nimport tensorflow_recommenders as tfrs\\nfrom vertexai.language_models import TextEmbeddingModel, TextEmbeddingInput\\n# Embed documents & query(from snip 9.) and convert them to tensors and tf.datasets\\nembedded_query = tf.constant((LM_embed(query, \"RETRIEVAL_QUERY\")))\\nembedded_docs = [LM_embed(doc, \"RETIREVAL_DOCUMENT\") for doc in searchable_docs]\\nembedded_docs = tf.data.Dataset.from_tensor_slices(embedded_docs).enumerate().batch(1)\\n# Build index from tensorflow dataset and execute ANN search based on dot product metric\\nscann = tfrs.layers.factorized_top_k.ScaNN( \\n  distance_measure= \\'dot_product\\',\\n  num_leaves = 4, #increase for higher number of partitions / latency for increased recall\\n  num_leaves_to_search= 2) # increase for higher recall but increased latency\\nscann = scann.index_from_dataset(embedded_docs)\\nscann(embedded_query, k=2)\\nSnippet 11. Using Tensorflow Recommenders33 to perform ANN search using the ScaNN algorithm\\nIn this whitepaper we have seen both current and traditional ANN search algorithms: ScaNN, \\nFAISS , LSH, KD-Tree, and Ball-tree, and examined the  great speed/accuracy tradeoffs \\nthat they provide. However, to use these algorithms they need to be deployed in a scalable, \\nsecure and production-ready manner. For that we need vector databases.\\nVector databases \\nVector embeddings embody semantic meanings of data, while vector search algorithms \\nprovide a means for efficiently querying them. Historically traditional databases lacked \\nthe means to combine semantic meaning and efficient querying. This is what gave rise to \\nvector databases, which are built ground-up to manage these embeddings for production \\nscenarios. Due to the recent popularity of Generative AI, an increasing number of traditional'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T20:31:04+05:30', 'trapped': '', 'modDate': \"D:20251109203104+05'30'\", 'creationDate': \"D:20250317134132-06'00'\", 'page': 47}, page_content='Embeddings & Vector Stores\\n48\\nFebrurary 2025\\ndatabases are starting to incorporate supporting vector search functionality in addition to \\ntraditional search (‘hybrid search’) functionalities. Let’s look at the workflow for a simple \\nVector Database, with hybrid search capabilities.\\nFigure 16. Populating and querying vector databases\\nEach vector database differs in its implementation, but the general flow is shown in Figure 16:\\n1.\\t An appropriate trained embedding model is used to embed the relevant data points as \\nvectors with fixed dimensions. \\n2.\\t The vectors are then augmented with appropriate metadata and complementary \\ninformation (such as tags) and indexed using the specified algorithm for efficient search.\\n3.\\t An incoming query gets embedded with the appropriate model, and used to search for \\nthe most semantically similar items and their associated unembedded content/metadata. \\nSome databases might provide caching and pre-filtering (based on tags) and post-filtering \\ncapabilities (reranking using another more accurate model) to further enhance the query \\nspeed and performance.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T20:31:04+05:30', 'trapped': '', 'modDate': \"D:20251109203104+05'30'\", 'creationDate': \"D:20250317134132-06'00'\", 'page': 48}, page_content='Embeddings & Vector Stores\\n49\\nFebrurary 2025\\nThere are quite a few vector databases available today, each tailored to different business \\nneeds and considerations. A few good examples of commercially managed vector \\ndatabases include Google Cloud’s Vertex Vector Search,34 Google Cloud’s AlloyDB & Cloud \\nSQL Postgres ElasticSearch,35 and Pinecone36 to name a few. Vertex AI Vector Search is \\na vector database built by Google that uses the ScaNN algorithm for fast vector search, \\nwhile still maintaining all the security and access guarantees of Google Cloud. AlloyDB & \\nCloud SQL Postgres supports vector search through the OSS pgvector37 extension, which \\nallows for SQL queries to combine ANN search with traditional predicates and the usual \\ntransactional semantics for ANN search index. AlloyDB also has a ScaNN index extension \\nthat is a native implementation of ScaNN and is pgvector-compatible. Similarly, many other \\ntraditional databases have also started to add plugins to enable vector search. Pinecone37 \\nand Weaviate39 leverage HNSW for their fast vector search in addition to the ability to filter \\ndata using traditional search. Amongst their open source peers: Weaviate38 and ChromaDB39 \\nprovide a full suite of functionality upon deployment and can be tested in memory as well \\nduring the prototyping phase.\\nOperational considerations\\nVector Databases are critical to managing the majority of technical challenges that arise \\nwith storing and querying embeddings at scale. Some of these challenges are specific to the \\nnature of vector stores, while others overlap with that of traditional databases. These include \\nhorizontal and vertical scalability, availability, data consistency, real time updates, backups, \\naccess control, compliance, and much more. However, there are also many more challenges \\nand considerations you need to take into account while using embedding and vector stores.\\nFirstly, embeddings, unlike traditional content, can mutate over time. This means that the \\nsame text, image, video or other content could and should be embedded using different \\nembedding models to optimize for the performance of the downstream applications. This is'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T20:31:04+05:30', 'trapped': '', 'modDate': \"D:20251109203104+05'30'\", 'creationDate': \"D:20250317134132-06'00'\", 'page': 49}, page_content='Embeddings & Vector Stores\\n50\\nFebrurary 2025\\nespecially true for embeddings of supervised models after the model is retrained to account \\nfor various drifts or changing objectives. Similarly, the same applies to unsupervised models \\nwhen they are updated to a newer model. However, frequently updating the embeddings \\n- especially those trained on large amounts of data - can be prohibitively expensive. \\nConsequently, a balance needs to be struck. This necessitates a well-defined automated \\nprocess to store, manage, and possibly purge embeddings from the vector databases taking \\nthe budget into consideration.\\nSecondly, while embeddings are great at representing semantic information, sometimes they \\ncan be suboptimal at representing literal or syntactic information. This is especially true for \\ndomain-specific words or IDs. These values are potentially missing or underrepresented \\nin the data the embeddings models were trained on. For example, if a user enters a query \\nthat contains the ID of a specific number along with a lot of text, the model might find \\nsemantically similar neighbors which match the meaning of the text closely, but not the ID, \\nwhich is the most important component in this context. You can overcome this challenge by \\nusing a combination of full-text search to pre-filter or post-filter the search space before \\npassing it onto the semantic search module.\\nAnother important point to consider is that depending on the nature of the workload in which \\nthe semantic query occurs, it might be worth relying on different vector databases. For \\nexample, for OLTP workloads that require frequent reads/write operations, an operational \\ndatabase like AlloyDB, Spanner, Postgres, or CloudSQL is the best choice. For large-\\nscale OLAP analytical workloads and batch use cases, using BigQuery’s vector search \\nis preferable.\\nIn conclusion, a variety of factors need to be considered when choosing a vector database. \\nThese factors include size and type of your dataset (some are good at sparse and others \\ndense), business needs, the nature of the workload,  budget, security, privacy guarantees, \\nthe needs for semantic and syntactic search as well as the database systems that are already'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T20:31:04+05:30', 'trapped': '', 'modDate': \"D:20251109203104+05'30'\", 'creationDate': \"D:20250317134132-06'00'\", 'page': 50}, page_content='Embeddings & Vector Stores\\n51\\nFebrurary 2025\\nin use. In this section we have seen the various ANN search approaches as well the need and \\nbenefits of vector databases. The next section demonstrates an example of using a Vector AI \\nVector Search for semantic search.\\nApplications\\nEmbeddings models are one of the fundamental machine learning models that power a \\nvariety of applications. We summarize some popular applications in the following table. \\nTask\\nDescription\\nRetrieval\\nGiven a query and a set of objects (for example, documents, images, \\nand videos), retrieve the most relevant objects. Based on the definition \\nof relevant objects, the subtasks include question answering and \\nrecommendations.\\nSemantic text similarity\\nDetermine whether two sentences have the same semantic meaning. \\nThe subtasks include: paraphrasing, duplicate detection, and bitext \\nmining.\\nClassification\\nClassify objects into possible categories. Based on the number of labels, \\nthe subtasks include binary classification, multi-class classification, and \\nmultilabel classifications.\\nClustering\\nCluster similar objects together.\\nReranking\\nRerank a set of objects based on a certain query.\\n \\nEmbeddings together with vector stores providing ANN are powerful tools which can be used \\nfor a variety of applications. These include Retrieval Augmented Generation (RAG) for LLMs, \\nSearch, Recommendation Systems, Anomaly detection, few shot- classification and much \\nmore.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T20:31:04+05:30', 'trapped': '', 'modDate': \"D:20251109203104+05'30'\", 'creationDate': \"D:20250317134132-06'00'\", 'page': 51}, page_content='Embeddings & Vector Stores\\n52\\nFebrurary 2025\\nFor ranking problems like search and recommendations, embeddings are normally used \\nat the first stage of the process. They retrieve the potentially good candidates that are \\nsemantically similar and consequently improve the relevance of search results. Since the \\namount of information to sort through can be quite large (in some cases even millions or \\nbillions) ANN techniques like ScaNN greatly aids in scalably narrowing the search space. This \\ninitial set of results can be further refined with a more sophisticated model on this smaller set \\nof candidates. \\nLet’s look at an application which combines both LLMs and RAG to help answer questions.\\nQ & A with sources (retrieval augmented generation)\\nRetrieval augmented generation (RAG) for Q&A is a technique that combines the best of both \\nworlds from retrieval and generation. It first retrieves relevant documents from a knowledge \\nbase and then uses prompt expansion to generate an answer from those documents. Prompt \\nexpansion is a technique that when combined with database search can be very powerful. \\nWith prompt expansion the model retrieves relevant information from the database (mostly \\nusing a combination of semantic search and business rules), and augments the original \\nprompt with it. The model uses this augmented prompt to generate much more interesting, \\nfactual, and informative content than with retrieval or generation alone.\\nRAG can help with two common problems with LLMs: 1) their tendency to ‘hallucinate’ \\nand generate factually incorrect but plausible sounding responses and 2) the high cost of \\nretraining to keep up with current information as newer data can be supplied via the prompt, \\nrather than at model training. Although RAG can reduce hallucinations, it does not completely \\neliminate them. What can help mitigate this problem further is to also return the sources from \\nthe retrieval and do a quick coherence check either by a human or an LLM. This ensures the'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T20:31:04+05:30', 'trapped': '', 'modDate': \"D:20251109203104+05'30'\", 'creationDate': \"D:20250317134132-06'00'\", 'page': 52}, page_content='Embeddings & Vector Stores\\n53\\nFebrurary 2025\\nLLM response is consistent with the semantically relevant sources. Let’s look at an example \\n(Snippet 11) of RAG with sources, which can be scalably implemented using Vertex AI LLM \\ntext embeddings and Vertex AI Vector Search in conjunction with libraries like langchain.40'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T20:31:04+05:30', 'trapped': '', 'modDate': \"D:20251109203104+05'30'\", 'creationDate': \"D:20250317134132-06'00'\", 'page': 53}, page_content='Embeddings & Vector Stores\\n54\\nFebrurary 2025\\n# Before you start run this command:\\n# pip install --upgrade --user --quiet google-cloud-aiplatform langchain_google_vertexai\\n# after running pip install make sure you restart your kernel\\n# TODO : Set values as per your requirements\\n# Project and Storage Constants\\nPROJECT_ID = \"<my_project_id>\"\\nREGION = \"<my_region>\"\\nBUCKET = \"<my_gcs_bucket>\"\\nBUCKET_URI = f\"gs://{BUCKET}\"\\n# The number of dimensions for the text-embedding-005 is 768\\n# If other embedder is used, the dimensions would probably need to change.\\nDIMENSIONS = 768\\n# Index Constants\\nDISPLAY_NAME = \"<my_matching_engine_index_id>\"\\nDEPLOYED_INDEX_ID = \"yourname01\" # you set this. Start with a letter.\\nfrom google.cloud import aiplatform\\nfrom langchain_google_vertexai import VertexAIEmbeddings\\nfrom langchain_google_vertexai import VertexAI\\nfrom langchain_google_vertexai import (\\n    VectorSearchVectorStore,\\n    VectorSearchVectorStoreDatastore,\\n)\\nfrom langchain.chains import RetrievalQA\\nfrom langchain.prompts.chat import (\\n    ChatPromptTemplate,\\n    SystemMessagePromptTemplate,\\n    HumanMessagePromptTemplate,\\n)\\nfrom IPython.display import display, Markdown\\naiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)\\nembedding_model = VertexAIEmbeddings(model_name=\"text-embedding-005\")\\n# NOTE : This operation can take upto 30 seconds\\nmy_index = aiplatform.MatchingEngineIndtex.create_tree_ah_index(\\n    display_name=DISPLAY_NAME,\\n    dimensions=DIMENSIONS,\\n    approximate_neighbors_count=150,\\n    distance_measure_type=\"DOT_PRODUCT_DISTANCE\",\\n    index_update_method=\"STREAM_UPDATE\",  # allowed values BATCH_UPDATE , STREAM_UPDATE\\n)\\nContinues next page...'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T20:31:04+05:30', 'trapped': '', 'modDate': \"D:20251109203104+05'30'\", 'creationDate': \"D:20250317134132-06'00'\", 'page': 54}, page_content='Embeddings & Vector Stores\\n55\\nFebrurary 2025\\n# Create an endpoint\\nmy_index_endpoint = aiplatform.MatchingEngineIndexEndpoint.create(\\n    display_name=f\"{DISPLAY_NAME}-endpoint\", public_endpoint_enabled=True\\n)\\n# NOTE : This operation can take upto 20 minutes\\nmy_index_endpoint = my_index_endpoint.deploy_index(\\n    index=my_index, deployed_index_id=DEPLOYED_INDEX_ID\\n)\\nmy_index_endpoint = my_index_endpoint.deploy_index(\\n    index=my_index, deployed_index_id=DEPLOYED_INDEX_ID\\n)\\nmy_index_endpoint.deployed_indexes\\n# TODO : replace 1234567890123456789 with your acutial index ID\\nmy_index = aiplatform.MatchingEngineIndex(\"1234567890123456789\")\\n# TODO : replace 1234567890123456789 with your acutial endpoint ID\\n# Be aware that the Index ID differs from the endpoint ID\\nmy_index_endpoint = aiplatform.MatchingEngineIndexEndpoint(\"1234567890123456789\")\\nfrom langchain_google_vertexai import (\\n    VectorSearchVectorStore,\\n    VectorSearchVectorStoreDatastore,\\n)\\n# Input texts\\ntexts = [\\n    \"The cat sat on\",\\n    \"the mat.\",\\n    \"I like to\",\\n    \"eat pizza for\",\\n    \"dinner.\",\\n    \"The sun sets\",\\n    \"in the west.\",\\n]\\nContinues next page...'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T20:31:04+05:30', 'trapped': '', 'modDate': \"D:20251109203104+05'30'\", 'creationDate': \"D:20250317134132-06'00'\", 'page': 55}, page_content='Embeddings & Vector Stores\\n56\\nFebrurary 2025\\nSnippet 12. Build/deploy ANN Index for Vertex AI Vector Search and use RAG with LLM prompts to generate \\ngrounded results/sources. \\n# Create a Vector Store\\nvector_store = VectorSearchVectorStore.from_components(\\n    project_id=PROJECT_ID,\\n    region=REGION,\\n    gcs_bucket_name=BUCKET,\\n    index_id=my_index.name,\\n    endpoint_id=my_index_endpoint.name,\\n    embedding=embedding_model,\\n    stream_update=True,\\n)\\n# Add vectors and mapped text chunks to your vectore store\\nvector_store.add_texts(texts=texts)\\n# Initialize the vectore_store as retriever\\nretriever = vector_store.as_retriever()\\n# perform simple similarity search on retriever\\nretriever.invoke(\"What are my options in breathable fabric?\")'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T20:31:04+05:30', 'trapped': '', 'modDate': \"D:20251109203104+05'30'\", 'creationDate': \"D:20250317134132-06'00'\", 'page': 56}, page_content='Embeddings & Vector Stores\\n57\\nFebrurary 2025\\nFigure 17. Model responses along with sources demonstrating the LLM being grounded in the database\\nAs we can infer from Figure 16, the output not only grounds LLM in the semantically similar \\nresults retrieved from the database (hence refusing to answer when context cannot be found \\nin the database). This not only significantly reduces hallucination, but also provides sources \\nfor verification, either human or using another LLM.\\nSummary\\nIn this whitepaper we have discussed various methods to create, manage, store, and retrieve \\nembeddings of various data modalities effectively in the context of production-grade \\napplications. Creating, maintaining and using embeddings for downstream applications can \\nbe a complex task that involves several roles in the organization. However, by thoroughly \\noperationalizing and automating its usage, you can safely leverage the incredible benefits \\nthey offer across some of the most important applications. Some key takeaways from this \\nwhitepaper include:\\n1.\\t Choose your embedding model wisely for your data and use case. Ensure the data used in \\ninference is consistent with the data used in training. The distribution shift from training to \\ninference can come from various areas, including domain distribution shift or downstream'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T20:31:04+05:30', 'trapped': '', 'modDate': \"D:20251109203104+05'30'\", 'creationDate': \"D:20250317134132-06'00'\", 'page': 57}, page_content='Embeddings & Vector Stores\\n58\\nFebrurary 2025\\ntask distribution shift. If no existing embedding models fit the current inference data \\ndistribution, fine-tuning the existing model can significantly help on the performance. \\nAnother tradeoff comes from the model size. The large deep neural network (large \\nmultimodal models) based models usually have better performance but can come with a \\ncost of longer serving latency. Using Cloud-based embedding services can conquer the \\nabove issue by providing both high-quality and low-latency embedding service. For most \\nbusiness applications using a pre-trained embedding model provides a good baseline, \\nwhich can be further fine-tuned or integrated in downstream models. In case the data has \\nan inherent graph structure, graph embeddings can provide superior performance.\\n2.\\t Once your embedding strategy is defined, it’s important to make the choice of the \\nappropriate vector database that suits your budget and business needs. It might seem \\nquicker to prototype with available open source alternatives, but opting for a more secure, \\nscalable, and battle-tested managed vector database can save significant developer \\ntime. There are various open source alternatives using one of the many powerful ANN \\nvector search algorithms, but ScaNN and HNSW have proven to provide some of the best \\naccuracy and performance trade offs.\\n3.\\t Embeddings combined with an appropriate ANN powered vector database is an \\nincredibly powerful tool and can be leveraged for various applications, including \\nSearch, Recommendation systems, and Retrieval Augmented Generation for LLMs. This \\napproach can mitigate the hallucination problem and bolster verifiability and trust of \\nLLM-based systems.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T20:31:04+05:30', 'trapped': '', 'modDate': \"D:20251109203104+05'30'\", 'creationDate': \"D:20250317134132-06'00'\", 'page': 58}, page_content=\"Embeddings & Vector Stores\\n59\\nFebrurary 2025\\nEndnotes\\n1.\\t Rai, A., 2020, Study of various methods for tokenization. In Advances in Natural Language Processing. \\nAvailable at: https://doi.org/10.1007/978-981-15-6198-6_18\\n2.\\t Pennington, J., Socher, R. & Manning, C., 2014, GloVe: Global Vectors for Word Representation. [online] \\nAvailable at: https://nlp.stanford.edu/pubs/glove.pdf.\\n3.\\t Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q. V. & Hinton, G., 2016, Swivel: Improving embeddings \\nby noticing what's missing. ArXiv, abs/1602.02215. Available at: https://arxiv.org/abs/1602.02215.\\n4.\\t Mikolov, T., Sutskever, I., Chen, K., Corrado, G. & Dean, J., 2013, Efficient estimation of word representations \\nin vector space. ArXiv, abs/1301.3781. Available at: https://arxiv.org/pdf/1301.3781.pdf.\\n5.\\t Rehurek, R., 2021, Gensim: open source python library for word and document embeddings. Available \\nat: https://radimrehurek.com/gensim/intro.html.\\n6.\\t Bojanowski, P., Grave, E., Joulin, A. & Mikolov, T., 2016, Enriching word vectors with subword information. \\nArXiv, abs/1607.04606. Available at: https://arxiv.org/abs/1607.04606.\\n7.\\t Deerwester, S., Dumais, S. T., Furnas, G. W., Landauer, T. K., & Harshman, R., 1990, Indexing by latent \\nsemantic analysis. Journal of the American Society for Information Science, 41(6), pp. 391-407.\\n8.\\t Blei, D. M., Ng, A. Y., & Jordan, M. I., 2001, Latent Dirichlet allocation. In T. G. Dietterich, S. Becker, & Z. \\nGhahramani (Eds.), Advances in Neural Information Processing Systems 14. MIT Press, pp. 601-608. Available \\nat: https://proceedings.neurips.cc/paper/2001/hash/296472c9542ad4d4788d543508116cbc-Abstract.html.\\n9.\\t Muennighoff, N., Tazi, N., Magne, L., & Reimers, N., 2022, Mteb: Massive text embedding benchmark. ArXiv, \\nabs/2210.07316. Available at: https://arxiv.org/abs/2210.07316.\\n10.\\tLe, Q. V., Mikolov, T., 2014, Distributed representations of sentences and documents. ArXiv, abs/1405.4053. \\nAvailable at: https://arxiv.org/abs/1405.4053.\\n11.\\t Devlin, J., Chang, M. W., Lee, K., & Toutanova, K., 2019, BERT: Pre-training deep Bidirectional Transformers \\nfor Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the \\nAssociation for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), \\npp. 4171-4186. Available at: https://www.aclweb.org/anthology/N19-1423/.\\n12.\\t Reimers, N. & Gurevych, I., 2020, Making monolingual sentence embeddings multilingual using knowledge \\ndistillation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing \\n(EMNLP), pp. 254-265. Available at: https://www.aclweb.org/anthology/2020.emnlp-main.21/.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T20:31:04+05:30', 'trapped': '', 'modDate': \"D:20251109203104+05'30'\", 'creationDate': \"D:20250317134132-06'00'\", 'page': 59}, page_content='Embeddings & Vector Stores\\n60\\nFebrurary 2025\\n13.\\t Gao, T., Yao, X. & Chen, D., 2021, Simcse: Simple contrastive learning of sentence embeddings. ArXiv, \\nabs/2104.08821. Available at: https://arxiv.org/abs/2104.08821.\\n14.\\t Wang, L., Yang, N., Huang, X., Jiao, B., Yang, L., Jiang, D., Majumder, R. & Wei, F., 2022, Text embeddings by \\nweakly supervised contrastive pre-training. ArXiv. Available at: https://arxiv.org/abs/2201.01279.\\n15.\\t Khattab, O. & Zaharia, M., 2020, colBERT: Efficient and effective passage search via contextualized late \\ninteraction over BERT. In Proceedings of the 43rd International ACM SIGIR Conference on Research and \\nDevelopment in Information Retrieval, pp. 39-48. Available at: https://dl.acm.org/doi/10.1145/3397271.3401025.\\n16.\\t Lee, J., Dai, Z., Duddu, S. M. K., Lei, T., Naim, I., Chang, M. W. & Zhao, V. Y., 2023, Rethinking the role of token \\nretrieval in multi-vector retrieval. ArXiv, abs/2304.01982. Available at: https://arxiv.org/abs/2304.01982.\\n17.\\t TensorFlow, 2021, TensorFlow hub, a model zoo with several easy to use pre-trained models. Available \\nat: https://tfhub.dev/.\\n18.\\t Zhang, W., Xiong, C., & Zhao, H., 2023, Introducing BigQuery text embeddings for NLP tasks.  \\nGoogle Cloud Blog. Available at: https://cloud.google.com/blog/products/data-analytics/introducing \\n-bigquery-text-embeddings.\\n19.\\t Google Cloud, 2024, Get multimodal embeddings. Available at:  \\nhttps://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-multimodal-embeddings.\\n20.\\tPinecone, 2024, IT Threat Detection. [online] Available at: https://docs.pinecone.io/docs/it-threat-detection.\\n21.\\t Cai, H., Zheng, V. W., & Chang, K. C., 2020, A survey of algorithms and applications related with graph \\nembedding. In Proceedings of the 29th ACM International Conference on Information & Knowledge \\nManagement. Available at: https://dl.acm.org/doi/10.1145/3444370.3444568.\\n22.\\tCai, H., Zheng, V. W., & Chang, K. C., 2017, A comprehensive survey of graph embedding: problems, \\ntechniques and applications. ArXiv, abs/1709.07604. Available at: https://arxiv.org/pdf/1709.07604.pdf.\\n23.\\tHamilton, W. L., Ying, R. & Leskovec, J., 2017, Inductive representation learning on large graphs.  \\nIn Advances in Neural Information Processing Systems 30. Available at:  \\nhttps://cs.stanford.edu/people/jure/pubs/graphsage -nips17.pdf.\\n24.\\tDong, Z., Ni, J., Bikel, D. M., Alfonseca, E., Wang, Y., Qu, C. & Zitouni, I., 2022, Exploring dual encoder \\narchitectures for question answering. ArXiv, abs/2204.07120. Available at: https://arxiv.org/abs/2204.07120.\\n25.\\tGoogle Cloud, 2021, Vertex AI Generative AI: Tune Embeddings. Available at:  \\nhttps://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-embeddings.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T20:31:04+05:30', 'trapped': '', 'modDate': \"D:20251109203104+05'30'\", 'creationDate': \"D:20250317134132-06'00'\", 'page': 60}, page_content='Embeddings & Vector Stores\\n61\\nFebrurary 2025\\n26.\\tMatsui, Y., 2020, Survey on approximate nearest neighbor methods. ACM Computing Surveys (CSUR), 53(6), \\nArticle 123. Available at: https://wangzwhu.github.io/home/file/acmmm-t-part3-ann.pdf.\\n27.\\t Friedman, J. H., Bentley, J. L. & Finkel, R. A., 1977, An algorithm for finding best matches in logarithmic \\nexpected time. ACM Transactions on Mathematical Software (TOMS), 3(3), pp. 209-226. Available at: \\nhttps://dl.acm.org/doi/pdf/10.1145/355744.355745.\\n28.\\tScikit-learn, 2021, Scikit-learn, a library for unsupervised and supervised neighbors-based learning methods. \\nAvailable at: https://scikit-learn.org/.\\n29.\\tlshashing, 2021, An open source python library to perform locality sensitive hashing. Available at: \\nhttps://pypi.org/project/lshashing/.\\n30.\\tMalkov, Y. A., Yashunin, D. A., 2016, Efficient and robust approximate nearest neighbor search using \\nhierarchical navigable small world graphs. ArXiv, abs/1603.09320. Available at:  \\nhttps://arxiv.org/pdf/1603.09320.pdf.\\n31.\\t Google Research, 2021, A library for fast ANN by Google using the ScaNN algorithm. Available at:  \\nhttps://github.com/google-research/google-research/tree/master/scann.\\n32.\\tGuo, R., Zhang, L., Hinton, G. & Zoph, B., 2020, Accelerating large-scale inference with anisotropic vector \\nquantization. ArXiv, abs/1908.10396. Available at: https://arxiv.org/pdf/1908.10396.pdf.\\n33.\\tTensorFlow, 2021, TensorFlow Recommenders, an open source library for building ranking & recommender \\nsystem models. Available at: https://www.tensorflow.org/recommenders.\\n34.\\tGoogle Cloud, 2021, Vertex AI Vector Search, Google Cloud’s high-scale low latency vector database. \\nAvailable at: https://cloud.google.com/vertex-ai/docs/vector-search/overview.\\n35.\\tElasticsearch, 2021, Elasticsearch: a RESTful search and analytics engine. Available at:  \\nhttps://www.elastic.co/elasticsearch/.\\n36.\\tPinecone, 2021, Pinecone, a commercial fully managed vector database. Available at:  \\nhttps://www.pinecone.io.\\n37.\\t pgvector, 2021, Open Source vector similarity search for Postgres. Available at:  \\nhttps://github.com/pgvector/pgvector.\\n38.\\tWeaviate, 2021, Weaviate, an open source vector database. Available at: https://weaviate.io/.\\n39. ChromaDB, 2021, ChromaDB, an open source vector database. Available at: https://www.trychroma.com/.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T20:31:04+05:30', 'trapped': '', 'modDate': \"D:20251109203104+05'30'\", 'creationDate': \"D:20250317134132-06'00'\", 'page': 61}, page_content=\"Embeddings & Vector Stores\\n62\\nFebrurary 2025\\n40. LangChain, 2021.,LangChain, an open source framework for developing applications powered by language \\nmodel. Available at: https://langchain.com.\\n42. Thakur, N., Reimers, N., Ruckl'e, A., Srivastava, A., & Gurevych, I. (2021). BEIR: A Heterogenous Benchmark for \\nZero-shot Evaluation of Information Retrieval Models. ArXiv, abs/2104.08663.  \\nAvailable at: https://github.com/beir-cellar/beir\\n43. Niklas Muennighoff, Nouamane Tazi, Loic Magne, and Nils Reimers. 2023. MTEB: Massive Text Embedding \\nBenchmark. In Proceedings of the 17th Conference of the European Chapter of the Association for \\nComputational Linguistics, pages 2014–2037, Dubrovnik, Croatia. Association for Computational Linguistics. \\nAvailable at: https://github.com/embeddings-benchmark/mteb\\n44. Chris Buckley. trec_eval IR evaluation package. Available from https://github.com/usnistgov/trec_eval\\n45. Christophe Van Gysel and Maarten de Rijke. 2018. Pytrec_eval: An Extremely Fast Python Interface to trec_\\neval. In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval (SIGIR \\n'18). Association for Computing Machinery, New York, NY, USA, 873–876.  \\nAvailalbe at: https://doi.org/10.1145/3209978.3210065 \\n46. Boteva, Vera & Gholipour Ghalandari, Demian & Sokolov, Artem & Riezler, Stefan. (2016). A Full-Text Learning \\nto Rank Dataset for Medical Information Retrieval. 9626. 716-722. 10.1007/978-3-319-30671-1_58. Available \\nat https://www.cl.uni-heidelberg.de/statnlpgroup/nfcorpus/\\n47. Douze, M., Guzhva, A., Deng, C., Johnson, J., Szilvasy, G., Mazaré, P.E., Lomeli, M., Hosseini, L. and Jégou, H., \\n2024. The Faiss library. arXiv preprint arXiv:2401.08281. Available at https://arxiv.org/abs/2401.08281\\n48. Lee, J., Dai, Z., Ren, X., Chen, B., Cer, D., Cole, J.R., Hui, K., Boratko, M., Kapadia, R., Ding, W. and Luan, Y., \\n2024. Gecko: Versatile text embeddings distilled from large language models. arXiv preprint arXiv:2403.20327. \\nAvailable at: https://arxiv.org/abs/2403.20327\\n49. Okapi BM25: a non-binary model” Christopher D. Manning, Prabhakar Raghavan, Hinrich Schütze. An \\nIntroduction to Information Retrieval, Cambridge University Press, 2009, p. 232.\\n50. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei \\nLi, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. \\nLearn. Res. 21, 1, Article 140 (January 2020), 67 pages.  \\nAvailable at https://dl.acm.org/doi/abs/10.5555/3455716.3455856\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T20:31:04+05:30', 'trapped': '', 'modDate': \"D:20251109203104+05'30'\", 'creationDate': \"D:20250317134132-06'00'\", 'page': 62}, page_content='Embeddings & Vector Stores\\n63\\nFebrurary 2025\\n51. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, \\nPaul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sashank \\nTsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, \\nEmily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, \\nPengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, \\nVedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret \\nZoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, \\nThanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr \\nPolozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, \\nJason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2023. PaLM: scaling \\nlanguage modeling with pathways. J. Mach. Learn. Res. 24, 1, Article 240 (January 2023), 113 pages.  \\nAvailable at https://dl.acm.org/doi/10.5555/3648699.3648939\\n52. Gemini: A Family of Highly Capable Multimodal Models, Gemini Team, Dec 2023.  \\nAvailable at: https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf\\n53. Radford, Alec and Karthik Narasimhan. “Improving Language Understanding by Generative Pre-Training.” \\n(2018). Available at:  \\nhttps://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf\\n54. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix, T., Rozière, B., Goyal, N., Hambro, \\nE., Azhar, F. and Rodriguez, A., 2023. Llama: Open and efficient foundation language models. arXiv preprint \\narXiv:2302.13971. Available at: https://arxiv.org/abs/2302.13971\\n55. Kusupati, A., Bhatt, G., Rege, A., Wallingford, M., Sinha, A., Ramanujan, V., Howard-Snyder, W., Chen, K., \\nKakade, S., Jain, P. and Farhadi, A., 2022. Matryoshka representation learning. Advances in Neural Information \\nProcessing Systems, 35, pp.30233-30249. Available at: \\nhttps://proceedings.neurips.cc/paper_files/paper/2022/file/c32319f4868da7613d78af9993100e42-Paper- \\n-Conference.pdf\\n56. Nair, P., Datta, P., Dean, J., Jain, P. and Kusupati, A., 2025. Matryoshka Quantization. arXiv preprint \\narXiv:2502.06786. Available at: https://arxiv.org/abs/2502.06786\\n57. Faysse, M., Sibille, H., Wu, T., Omrani, B., Viaud, G., Hudelot, C. and Colombo, P., 2024. Colpali: Efficient \\ndocument retrieval with vision language models. arXiv preprint arXiv:2407.01449.  \\nAvailable at: https://arxiv.org/abs/2407.01449\\n58. Aumüller, M., Bernhardsson, E. and Faithfull, A., 2020. ANN-Benchmarks: A benchmarking tool for \\napproximate nearest neighbor algorithms. Information Systems, 87, p.101374.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:41:32-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_emebddings_vectorstores_v2.pdf', 'total_pages': 64, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T20:31:04+05:30', 'trapped': '', 'modDate': \"D:20251109203104+05'30'\", 'creationDate': \"D:20250317134132-06'00'\", 'page': 63}, page_content='Embeddings & Vector Stores\\n64\\nFebrurary 2025'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 0}, page_content='Foundational \\nLarge Language \\nModels & \\nText Generation\\nAuthors: Mohammadamin Barektain,  \\nAnant Nawalgaria, Daniel J. Mankowitz,  \\nMajd Al Merey, Yaniv Leviathan, Massimo Mascaro,  \\nMatan Kalman, Elena Buchatskaya,                                     \\nAliaksei Severyn, Irina Sigler, and Antonio Gulli'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 1}, page_content='Foundational Large Language Models & Text Generation\\n2\\nFebruary 2025\\nAcknowledgements\\nContent contributors\\nAdam Sadvovsky\\nYonghui Wu\\nAndrew Dai\\nEfi Kokiopolou\\nChuck Sugnet\\nAleksey Vlasenko\\nErwin Huizenga\\nAida Nematzadeh\\nIra Ktena\\nOlivia Wiles\\nLavi Nigam\\nCurators and Editors\\nAntonio Gulli\\nAnant Nawalgaria\\nGrace Mollison \\nTechnical Writer\\nMark Iverson\\nDesigner\\nMichael Lanning'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 2}, page_content='Introduction\\x08\\n6\\nWhy language models are important\\x08\\n7\\nLarge language models\\x08\\n8\\nTransformer\\x08\\n9\\nInput preparation and embedding\\t\\n11\\nMulti-head attention\\t\\n12\\nUnderstanding self-attention\\t\\n12\\nMulti-head attention: power in diversity\\t\\n14\\nLayer normalization and residual connections\\t\\n15\\nFeedforward layer \\t\\n15\\nEncoder and decoder\\t\\n16\\nMixture of Experts (MoE)\\t\\n17\\nTraining the transformer\\t\\n20\\nData preparation\\t\\n21\\nTraining and loss function\\t\\n21\\nThe evolution of transformers\\x08\\n23\\nGPT-1\\x08\\n23\\nBERT\\x08\\n25\\nTable of contents'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 3}, page_content='GPT-2\\x08\\n25\\nGPT-3/3.5/4\\x08\\n27\\nLaMDA\\x08\\n28\\nGopher\\x08\\n29\\nGLaM\\x08\\n31\\nChinchilla\\x08\\n31\\nPaLM\\x08\\n33\\nPaLM 2\\t\\n33\\nGemini\\x08\\n34\\nGemma\\x08\\n37\\nLLaMA\\x08\\n38\\nMixtral\\x08\\n39\\nOpenAI O1\\x08\\n40\\nDeepSeek\\x08\\n40\\nOther open models\\x08\\n41\\nComparison\\x08\\n43\\nFine-tuning large language models\\x08\\n45\\nSupervised fine-tuning \\x08\\n46\\nReinforcement learning from human feedback\\x08\\n47\\nParameter Efficient Fine-Tuning\\x08\\n49\\nUsing large language models\\x08\\n52\\nPrompt engineering \\x08\\n52\\nSampling Techniques and Parameters\\x08\\n53\\nTask-based Evaluation\\x08\\n54\\nAccelerating inference\\x08\\n57'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 4}, page_content='Trade offs\\x08\\n58\\nThe Quality vs Latency/Cost Tradeoff\\t\\n58\\nThe Latency vs Cost Tradeoff\\t\\n59\\nOutput-approximating methods\\x08\\n60\\nQuantization\\t\\n60\\nDistillation\\t\\n61\\nOutput-preserving methods\\x08\\n62\\nFlash Attention\\t\\n63\\nPrefix Caching\\t\\n63\\nSpeculative Decoding\\t\\n65\\nBatching and Parallelization\\x08\\n67\\nApplications\\x08\\n68\\nCode and mathematics\\x08\\n71\\nMachine translation\\x08\\n72\\nText summarization\\x08\\n73\\nQuestion-answering\\x08\\n73\\nChatbots\\x08\\n74\\nContent generation\\x08\\n75\\nNatural language inference\\x08\\n75\\nText classification\\x08\\n76\\nText analysis\\x08\\n77\\nMultimodal applications\\x08\\n78\\nSummary\\x08\\n80\\nEndnotes\\x08\\n82'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 5}, page_content='Foundational Large Language Models & Text Generation\\n6\\nFebruary 2025\\nIntroduction\\nThe advent of Large Language Models (LLMs) represents a seismic shift in the world of \\nartificial intelligence. Their ability to process, generate, and understand user intent is \\nfundamentally changing the way we interact with information and technology. \\nAn LLM is an advanced artificial intelligence system that specializes in processing, \\nunderstanding, and generating human-like text. These systems are typically implemented as \\na deep neural network and are trained on massive amounts of text data. This allows them to \\nlearn the intricate patterns of language, giving them the ability to perform a variety of tasks, \\nlike machine translation, creative text generation, question answering, text summarization, \\nand many more reasoning and language oriented tasks. This whitepaper dives into the \\ntimeline of the various architectures and approaches building up to the large language \\nmodels and the architectures being used at the time of publication. It also discusses fine-\\nWe believe that this new crop of \\ntechnologies has the potential to \\nassist, complement, empower, \\nand inspire people at any time \\nacross almost any field.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 6}, page_content='Foundational Large Language Models & Text Generation\\n7\\nFebruary 2025\\ntuning techniques to customize an LLM to a certain domain or task, methods to make the \\ntraining more efficient, as well as methods to accelerate inference. These are then followed \\nby various applications and code examples. \\nWhy language models are important\\nLLMs achieve an impressive performance boost from the previous state of the art NLP \\nmodels across a variety of different and complex tasks which require answering questions \\nor complex reasoning, making feasible many new applications. These include language \\ntranslation, code generation and completion, text generation, text classification, and \\nquestion-answering, to name a few. Although foundational LLMs trained in a variety of \\ntasks on large amounts of data perform very well out of the box and display emergent \\nbehaviors (e.g. the ability to perform tasks they have not been directly trained for) they can \\nalso be adapted to solve specific tasks where performance out of the box is not at the level \\ndesired through a process known as fine-tuning. This requires significantly less data and \\ncomputational resources than training an LLM from scratch. LLMs can be further nudged \\nand guided towards the desired behavior by the discipline of prompt engineering: the art and \\nscience of composing the prompt and the parameters of an LLM to get the desired response.\\nThe big question is: how do these large language models work? The next section explores the \\ncore building blocks of LLMs, focusing on transformer architectures and their evolution from \\nthe original ‘Attention is all you need’ paper1 to the latest models such as Gemini, Google’s \\nmost capable LLM. We also cover training and fine-tuning techniques, as well as methods to \\nimprove the speed of response generation. The whitepaper concludes with a few examples \\nof how language models are used in practice.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 7}, page_content='Foundational Large Language Models & Text Generation\\n8\\nFebruary 2025\\nLarge language models\\nA language model predicts the probability of a sequence of words. Commonly, when given \\na prefix of text, a language model assigns probabilities to subsequent words. For example, \\ngiven the prefix “The most famous city in the US is…”, a language model might predict high \\nprobabilities to the words “New York” and “Los Angeles” and low probabilities to the words \\n“laptop” or “apple”. You can create a basic language model by storing an n-gram table,2 while \\nmodern language models are often based on neural models, such as transformers.\\nBefore the invention of transformers1, recurrent neural networks (RNNSs) were the popular \\napproach for modeling sequences. In particular, “long short-term memory” (LSTM) and \\n“gated recurrent unit” (GRU) were common architectures.3 This area includes language \\nproblems such as machine translation, text classification, text summarization, and question-\\nanswering, among others. RNNs process input and output sequences sequentially. They \\ngenerate a sequence of hidden states based on the previous hidden state and the current \\ninput. The sequential nature of RNNs makes them compute-intensive and hard to parallelize \\nduring training (though recent work in state space modeling is attempting to overcome \\nthese challenges).\\nTransformers, on the other hand, are a type of neural network that can process sequences \\nof tokens in parallel thanks to the self-attention mechanism.1 This means that transformers \\ncan model long-term contexts more effectively and are easier to parallelize than RNNs. This \\nmakes them significantly faster to train, and more powerful compared to RNNs for handling \\nlong-term dependencies in long sequence tasks. However, the cost of self-attention in the \\noriginal transformers is quadratic in the context length which limits the size of the context, \\nwhile RNNs have a theoretically infinite context length. Although they have infinite context \\nlength, in practice they struggle to utilize it due to vanishing gradient problem. Transformers \\nhave become the most popular approach for sequence modeling and transfer learning \\nproblems in recent years.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 8}, page_content='Foundational Large Language Models & Text Generation\\n9\\nFebruary 2025\\nHerein, we discuss the first version of the transformer model and then move on to the more \\nrecent advanced models and algorithms.\\nTransformer\\nThe transformer architecture was developed at Google in 2017 for use in a translation model.1 \\nIt’s a sequence-to-sequence model capable of converting sequences from one domain \\ninto sequences in another domain. For example, translating French sentences to English \\nsentences. The original transformer architecture consists of two parts: an encoder and a \\ndecoder. The encoder converts the input text (e.g., a French sentence) into a representation, \\nwhich is then passed to the decoder. The decoder uses this representation to generate the \\noutput text (e.g., an English translation) autoregressively.1 Notably, the size of the output of \\nthe transformer encoder is linear in the size of its input. Figure 1 shows the design of the \\noriginal transformer architecture.\\nThe transformer consists of multiple layers. A layer in a neural network comprises a set of \\nparameters that perform a specific transformation on the data. In the diagram you can see \\nan example of some layers which include Multi-Head Attention, Add & Norm, Feed-Forward, \\nLinear, Softmax etc. The layers can be sub-divided into the input, hidden and output layers. \\nThe input layer (e.g., Input/Output Embedding) is the layer where the raw data enters the \\nnetwork. Input embeddings are used to represent the input tokens to the model. Output \\nembeddings are used to represent the output tokens that the model predicts. For example, in \\na machine translation model, the input embeddings would represent the words in the source \\nlanguage, while the output embeddings would represent the words in the target language. \\nThe output layer (e.g., Softmax) is the final layer that produces the output of the network. The \\nhidden layers (e.g., Multi-Head Attention) are between the input and output layers and are \\nwhere the magic happens!'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 9}, page_content='Foundational Large Language Models & Text Generation\\n10\\nFebruary 2025\\nFigure 1. Original Transformer1 (P.C:5)'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 10}, page_content='Foundational Large Language Models & Text Generation\\n11\\nFebruary 2025\\nTo better understand the different layers in the transformer, let’s use a French-to-English \\ntranslation task as an example. Here, we explain how a French sentence is input into the \\ntransformer and a corresponding English translation is output. We will also describe each of \\nthe components inside the transformer from Figure 1.\\nInput preparation and embedding\\nTo prepare language inputs for transformers, we convert an input sequence into tokens \\nand then into input embeddings. At a high level, an input embedding is a high-dimensional \\nvector68 that represents the meaning of each token in the sentence. This embedding is \\nthen fed into the transformer for processing. Generating an input embedding involves the \\nfollowing steps:\\n1.\\t Normalization (Optional): Standardizes text by removing redundant whitespace, \\naccents, etc.\\n2.\\t Tokenization: Breaks the sentence into words or subwords and maps them to integer \\ntoken IDs from a vocabulary.\\n3.\\t Embedding: Converts each token ID to its corresponding high-dimensional vector, \\ntypically using a lookup table. These can be learned during the training process.\\n4.\\t Positional Encoding: Adds information about the position of each token in the sequence \\nto help the transformer understand word order.\\nThese steps help to prepare the input for the transformers so that they can better \\nunderstand the meaning of the text.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 11}, page_content='Foundational Large Language Models & Text Generation\\n12\\nFebruary 2025\\nMulti-head attention\\nAfter converting input tokens into embedding vectors, you feed these embeddings into \\nthe multi-head attention module (see Figure 1). Self-attention is a crucial mechanism in \\ntransformers; it enables them to focus on specific parts of the input sequence relevant to \\nthe task at hand and to capture long-range dependencies within sequences more effectively \\nthan traditional RNNs. \\nUnderstanding self-attention\\nConsider the following sentence: “The tiger jumped out of a tree to get a drink because it \\nwas thirsty.” Self-attention helps to determine relationships between different words and \\nphrases in sentences. For example, in this sentence, “the tiger” and “it” are the same object, \\nso we would expect these two words to be strongly connected. Self-attention achieves this \\nthrough the following steps (Figure 2):\\n1.\\t Creating queries, keys, and values: Each input embedding is multiplied by three learned \\nweight matrices (Wq, Wk, Wv) to generate query (Q), key (K), and value (V) vectors. These \\nare like specialized representations of each word.\\n•\\t Query: The query vector helps the model ask, “Which other words in the sequence are \\nrelevant to me?”\\n•\\t Key: The key vector is like a label that helps the model identify how a word might be \\nrelevant to other words in the sequence.\\n•\\t Value: The value vector holds the actual word content information.\\n2.\\t Calculating scores: Scores are calculated to determine how much each word should \\n‘attend’ to other words. This is done by taking the dot product of the query vector of one \\nword with the key vectors of all the words in the sequence.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 12}, page_content='Foundational Large Language Models & Text Generation\\n13\\nFebruary 2025\\n3.\\t Normalization: The scores are divided by the square root of the key vector dimension (dk) \\nfor stability, then passed through a softmax function to obtain attention weights. These \\nweights indicate how strongly each word is connected to the others.\\n4.\\t Weighted values: Each value vector is multiplied by its corresponding attention weight. \\nThe results are summed up, producing a context-aware representation for each word.\\nFigure 2. The process of computing self-attention in the multi-head attention module1 (P.C:5)'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 13}, page_content='Foundational Large Language Models & Text Generation\\n14\\nFebruary 2025\\nIn practice, these computations are performed at the same time, by stacking the query, key \\nand value vectors for all the tokens into Q, K and V matrices and multiplying them together as \\nshown in Figure 3.\\nFigure 3. The basic operation of attention,1  with Q=query, K=Keys and V=Value, Z=Attention, d_k = dimension \\nof queries and keys (P.C:5)\\nMulti-head attention: power in diversity\\nMulti-head attention employs multiple sets of Q, K, V weight matrices. These run in parallel, \\neach ‘head’ potentially focusing on different aspects of the input relationships. The outputs \\nfrom each head are concatenated and linearly transformed, giving the model a richer \\nrepresentation of the input sequence.\\nThe use of multi-head attention improves the model’s ability to handle complex language \\npatterns and long-range dependencies. This is crucial for tasks that require a nuanced \\nunderstanding of language structure and content, such as machine translation, text \\nsummarization, and question-answering. The mechanism enables the transformer to consider \\nmultiple interpretations and representations of the input, which enhances its performance on \\nthese tasks.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 14}, page_content='Foundational Large Language Models & Text Generation\\n15\\nFebruary 2025\\nLayer normalization and residual connections\\nEach layer in a transformer, consisting of a multi-head attention module and a feed-forward \\nlayer, employs layer normalization and residual connections. This corresponds to the Add \\nand Norm layer in Figure 1, where ‘Add’ corresponds to the residual connection and ‘Norm’ \\ncorresponds to layer normalization. Layer normalization computes the mean and variance \\nof the activations to normalize the activations in a given layer. This is typically performed to \\nreduce covariate shift as well as improve gradient flow to yield faster convergence during \\ntraining as well as improved overall performance. \\nResidual connections propagate the inputs to the output of one or more layers. This has the \\neffect of making the optimization procedure easier to learn and also helps deal with vanishing \\nand exploding gradients. \\nThe Add and Norm layer is applied to both the multi-head attention module and the feed-\\nforward layer described in the following section.\\nFeedforward layer \\nThe output of the multi-head attention module and the subsequent ‘Add and Norm’ layer is \\nfed into the feedforward layer of each transformer block. This layer applies a position-wise \\ntransformation to the data, independently for each position in the sequence, which allows the \\nincorporation of additional non-linearity and complexity into the model’s representations. The \\nfeedforward layer typically consists of two linear transformations with a non-linear activation \\nfunction, such as ReLU or GELU, in between. This structure adds further representational \\npower to the model. After processing by the feedforward layer, the data undergoes \\nanother ‘Add and Norm’ step, which contributes to the stability and effectiveness of deep \\ntransformer models.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 15}, page_content='Foundational Large Language Models & Text Generation\\n16\\nFebruary 2025\\nEncoder and decoder\\nThe original transformer architecture relies on a combination of encoder and decoder \\nmodules. Each encoder and decoder consists of a series of layers, with each layer \\ncomprising key components: a multi-head self-attention mechanism, a position-wise feed-\\nforward network, normalization layers, and residual connections. \\nThe encoder’s primary function is to process the input sequence into a continuous \\nrepresentation that holds contextual information for each token. The input sequence is first \\nnormalized, tokenized, and converted into embeddings. Positional encodings are added to \\nthese embeddings to retain sequence order information. Through self-attention mechanisms, \\neach token in the sequence can dynamically attend to any other token, thus understanding \\nthe contextual relationships within the sequence. The output from the encoder is a series of \\nembedding vectors Z representing the entire input sequence. \\nThe decoder is tasked with generating an output sequence based on the context provided \\nby the encoder’s output Z. It operates in a token-by-token fashion, beginning with a start-\\nof-sequence token. The decoder layers employ two types of attention mechanisms: masked \\nself-attention and encoder-decoder cross-attention. Masked self-attention ensures that \\neach position can only attend to earlier positions in the output sequence, preserving the \\nauto-regressive property. This is crucial for preventing the decoder from having access to \\nfuture tokens in the output sequence. The encoder-decoder cross-attention mechanism \\nallows the decoder to focus on relevant parts of the input sequence, utilizing the contextual \\nembeddings generated by the encoder. This iterative process continues until the decoder \\npredicts an end-of-sequence token, thereby completing the output sequence generation.\\nMajority of recent LLMs adopted a decoder-only variant of transformer architecture. This \\napproach forgoes the traditional encoder-decoder separation, focusing instead on directly \\ngenerating the output sequence from the input. The input sequence undergoes a similar'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 16}, page_content='Foundational Large Language Models & Text Generation\\n17\\nFebruary 2025\\nprocess of embedding and positional encoding before being fed into the decoder. The \\ndecoder then uses masked self-attention to generate predictions for each subsequent \\ntoken based on the previously generated tokens. This streamlined approach simplifies the \\narchitecture for specific tasks where encoding and decoding can be effectively merged.\\nMixture of Experts (MoE)\\nA Mixture of Experts (MoE) is a an architecture that combines multiple specialized sub-\\nmodels (the “experts”) to improve overall performance, particularly on complex tasks. It’s \\na form of ensemble learning, but with a key difference: instead of simply aggregating the \\npredictions of all experts, it learns to route different parts of the input to different experts. \\nThis allows the model to specialize, with each expert becoming proficient in a specific \\nsub-domain or aspect of the data. Here’s a more technical breakdown describing the main \\ncomponents of an MoE:\\n•\\t Experts: These are the individual sub-models, each designed to handle a specific subset \\nof the input data or a particular task. They can be any type of model (e.g., neural networks, \\ndecision trees, etc.), but in the context of large language models, they are typically \\nthemselves transformer-based architectures.  \\n•\\t Gating Network (Router): This is a crucial component that learns to route the input to \\nthe appropriate expert(s). It takes the input and produces a probability distribution over \\nthe experts. This distribution determines how much each expert should “contribute” to the \\nfinal prediction. The gating network is also typically a neural network.  \\n•\\t Combination Mechanism: This combines the outputs of the experts, weighted by the \\nprobabilities from the gating network, to produce the final prediction. A common approach \\nis a weighted average.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 17}, page_content='Foundational Large Language Models & Text Generation\\n18\\nFebruary 2025\\nIn practice, A Mixture of Experts (MoE) architecture combines multiple specialized sub-\\nmodels, called “experts,” to tackle complex tasks. Instead of simply averaging all expert \\npredictions, an MoE uses a “gating network” to intelligently route different parts of the \\ninput to the mostt relevant experts. Both the experts and the gating network receive the \\ninput.  Each expert processes the input and generates its output. Simultaneously, the \\ngating network analyzes the input and produces a probability distribution over the experts, \\nindicating how much each expert should contribute to the final result.  These probabilities \\nthen weight the outputs of the experts, and the weighted combination becomes the final \\nprediction. This allows different experts to specialize in handling specific types of data or \\nsub-tasks, improving overall performance and, through “sparse activation,” potentially \\nreducing computational cost by only activating a subset of experts for any given input.\\nFigure 4. Mixture of experts ensembling70'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 18}, page_content='Foundational Large Language Models & Text Generation\\n19\\nFebruary 2025\\nLarge Reasoning Models\\nAchieving robust reasoning capabilities in Large Models is a complex endeavor that involves \\na combination of architectural designs, training methodologies, and prompting strategies. \\nOne crucial aspect is incorporating inductive biases that favor reasoning-conducive patterns. \\nTransformer architectures, with their self-attention mechanisms, are foundational, allowing \\nthe model to weigh the importance of different parts of the input sequence when generating \\nan output. However, vanilla Transformers alone are not sufficient for complex reasoning.\\nChain-of-Thought prompting explicitly encourages the model to generate intermediate \\nreasoning steps before arriving at a final answer. By providing examples of step-by-step \\nreasoning in the prompt, the model learns to decompose complex problems into smaller, \\nmanageable sub-problems. This mimics human reasoning processes and significantly \\nimproves performance on tasks requiring multi-step inference. Tree-of-Thoughts takes this \\nfurther, exploring multiple reasoning paths and using a search algorithm to find the most \\npromising solution. This technique is useful with game trees or combinatorial problems. \\nLeast-to-Most prompting guides the model to solve subproblems, which get progressively \\nmore complex, and the output of one subproblem is used as part of the prompt of the more \\ncomplex, subsequent problem.\\nFine-tuning on datasets specifically designed for reasoning tasks is also crucial. These \\ndatasets may contain logical puzzles, mathematical problems, or commonsense reasoning \\nchallenges. Instruction tuning, where the model is trained to follow natural language \\ninstructions, further enhances its ability to understand and respond to complex reasoning \\nprompts. Reinforcement Learning from Human Feedback (RLHF) refines the model’s outputs \\nbased on human preferences, improving the quality and coherence of its reasoning. RLHF \\nhelps in reward models that score reasoning ability as well as “helpfulness.”'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 19}, page_content='Foundational Large Language Models & Text Generation\\n20\\nFebruary 2025\\nKnowledge distillation, transferring knowledge from a larger, more capable “teacher” model \\nto a smaller, more efficient “student” model, can be used to improve the reasoning abilities \\nof smaller models while maintaining efficiency. This approach allows the student model to \\nlearn the reasoning patterns of the teacher model without requiring the same computational \\nresources. During inference, techniques like beam search, which explores multiple candidate \\noutputs simultaneously, can improve the quality of reasoning by considering different \\npossibilities. Temperature scaling, adjusting the randomness of the model’s output, can also \\ninfluence the exploration-exploitation trade-off in reasoning. Finally, incorporating external \\nknowledge sources, such as knowledge graphs or structured databases, can provide the \\nmodel with additional information to support its reasoning process. This can be achieved \\nthrough techniques like retrieval-augmented generation, where the model retrieves relevant \\ninformation from an external source before generating an output. These techniques all \\ncombined, across many domains of reasoning, create the best performing reasoning large \\nlanguage models.\\nTraining the transformer\\nWhen talking about machine learning models, it’s important to differentiate between \\ntraining and inference. Training typically refers to modifying the parameters of the model, \\nand involves loss functions and backpropagation. Inference is when model is used only \\nfor the predicted output, without updating the model weights. The model parameters are \\nfixed during inference. Up until now we learned how transformers generate outputs during \\ninference. Next, we focus on how to train transformers to perform one or more given tasks.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 20}, page_content='Foundational Large Language Models & Text Generation\\n21\\nFebruary 2025\\nData preparation\\nThe first step is data preparation, which involves a few important steps itself. First, clean the \\ndata by applying techniques such as filtering, deduplication, and normalization. The next \\nstep is tokenization where the dataset is converted into tokens using techniques such as \\nByte-Pair Encoding8, 9 and Unigram tokenization.8, 10 Tokenization generates a vocabulary, \\nwhich is a set of unique tokens used by the LLM. This vocabulary serves as the model’s \\n’language’ for processing and understanding text. Finally, the data is typically split into a \\ntraining dataset for training the model as well as a test dataset which is used to evaluate the \\nmodels performance.\\nTraining and loss function\\nA typical transformer training loop consists of several parts: First, batches of input \\nsequences are sampled from a training dataset. For each input sequence, there is a \\ncorresponding target sequence. In unsupervised pre-training, the target sequence is \\nderived from the input sequence itself. The batch of input sequences is then fed into the \\ntransformer. The transformer generates predicted output sequences. The difference \\nbetween the predicted and target sequences is measured using a loss function (often cross-\\nentropy loss)11. Gradients of this loss are calculated, and an optimizer uses them to update \\nthe transformer’s parameters. This process is repeated until the transformer converges to a \\ncertain level of performance or until it has been trained on a pre-specified number of tokens. \\nThere are different approaches to formulating the training task for transformers depending \\non the architecture used:\\n•\\t Decoder-only models are typically pre-trained on the language modeling task (e.g., see \\nendnote12, 13). The target sequence for the decoder is simply a shifted version of the input \\nsequence. Given a training sequence like ‘the cat sat on the mat’ various input/target'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 21}, page_content='Foundational Large Language Models & Text Generation\\n22\\nFebruary 2025\\npairs can be generated for the model. For example the input “the cat sat on” should \\npredict “the” and subsequently the input “the cat sat on the” should predict target \\nsequence “mat”.\\n•\\t Encoder-only models (like BERT)14 are often pre-trained by corrupting the input sequence \\nin some way and having the model try to reconstruct it. One such approach is masked \\nlanguage modeling (MLM).14 In our example, the input sequence could be “The [MASK] sat \\non the mat” and the sequence target would be the original sentence.\\n•\\t Encoder-decoder models (like the original transformer) are trained on sequence-to-\\nsequence supervised tasks such as translation (input sequence “Le chat est assis sur \\nle tapis” and target “The cat sat on the mat”), question-answering (where the input \\nsequence is a question and the target sequence is the corresponding answer), and \\nsummarization (where the input sequence is a full article and the target sequence is its \\ncorresponding summary). These models could also be trained in an unsupervised way by \\nconverting other tasks into sequence-to-sequence format. For example, when training \\non Wikipedia data, the input sequence might be the first part of an article, and the target \\nsequence comprises the remainder of the article.\\nAn additional factor to consider during training is the ‘context length’. This refers to the \\nnumber of previous tokens the model can ‘remember’ and use to predict the next token in \\nthe sequence. Longer context lengths allow the model to capture more complex relationships \\nand dependencies within the text, potentially leading to better performance. However, longer \\ncontexts also require more computational resources and memory, which can slow down \\ntraining and inference. Choosing an appropriate context length involves balancing these \\ntrade-offs based on the specific task and available resources.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 22}, page_content='Foundational Large Language Models & Text Generation\\n23\\nFebruary 2025\\nThe evolution of transformers\\nThe next sections provide an overview of the various transformer architectures. These \\ninclude encoder-only, encoder-decoder, as well as decoder-only transformers. We start with \\nGPT-1 and BERT and end with Google’s latest family of LLMs called Gemini.\\nGPT-1\\nGPT-1 (Generative pre-trained transformer version 1)15 was a decoder-only model developed \\nby OpenAI in 2018. It was trained on the BooksCorpus dataset (containing approximately \\nseveral billion words) and is able to generate text, translate languages, write different kinds \\nof creative content, and answer questions in an informative way. The main innovations in \\nGPT-1 were:\\n•\\t Combining transformers and unsupervised pre-training: Unsupervised pre-training \\nis a process of training a language model on a large corpus of unlabeled data. Then, \\nsupervised data is used to fine-tune the model for a specific task, such as translation \\nor sentiment classification. In prior works, most language models were trained using a \\nsupervised learning objective. This means that the model was trained on a dataset of \\nlabeled data, where each example had a corresponding label. This approach has two main \\nlimitations. First, it requires a large amount of labeled data, which can be expensive and \\ntime-consuming to collect. Second, the model can only generalize to tasks that are similar \\nto the tasks that it was trained on. Semi-supervised sequence learning was one of the first \\nworks that showed that unsupervised pre-training followed by supervised training was \\nsuperior than supervised training alone.\\nUnsupervised pre-training addresses these limitations by training the model on a large \\ncorpus of unlabeled data. This data can be collected more easily and cheaply than labeled \\ndata. Additionally, the model can generalize to tasks that are different from the tasks that'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 23}, page_content='Foundational Large Language Models & Text Generation\\n24\\nFebruary 2025\\nit was trained on. The BooksCorpus dataset is a large (5GB) corpus of unlabeled text that \\nwas used to train the GPT-1 language model. The dataset contains over 7,000 unpublished \\nbooks, which provides the model with a large amount of data to learn from. Additionally, \\nthe corpus contains long stretches of contiguous text, which helps the model learn long-\\nrange dependencies. Overall, unsupervised pre-training is a powerful technique that can \\nbe used to train language models that are more accurate and generalizable than models \\nthat are trained using supervised learning alone. \\n•\\t Task-aware input transformations: There are different kinds of tasks such as textual \\nentailment and question-answering that require a specific structure. For example, \\ntextual entailment requires a premise and a hypothesis; question-answering requires a \\ncontext document; a question and possible answers. One of the contributions of GPT-1 \\nis converting these types of tasks which require structured inputs into an input that the \\nlanguage model can parse, without requiring task-specific architectures on top of the \\npre-trained architecture. For textual entailment, the premise p and the hypothesis h are \\nconcatenated with a delimiter token ($) in between - [p, $, h]. For question answering, the \\ncontext document c is concatenated with the question q and a possible answer a with a \\ndelimiter token in between the question and answer - [c,q,$,a].\\nGPT-1 surpassed previous models on several benchmarks, achieving excellent results. While \\nGPT-1 was a significant breakthrough in natural language processing (NLP), it had some \\nlimitations. For example, the model was prone to generating repetitive text, especially when \\ngiven prompts outside the scope of its training data. It also failed to reason over multiple \\nturns of dialogue and could not track long-term dependencies in text. Additionally, its \\ncohesion and fluency were limited to shorter text sequences, and longer passages would \\nlack cohesion. Despite these limitations, GPT-1 demonstrated the power of unsupervised \\npre-training, which laid the foundation for larger and more powerful models based on the \\ntransformer architecture.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 24}, page_content='Foundational Large Language Models & Text Generation\\n25\\nFebruary 2025\\nBERT\\nBERT14 which stands for Bidirectional Encoder Representations from Transformers, \\ndistinguishes itself from traditional encoder-decoder transformer models by being an \\nencoder-only architecture. Instead of translating or producing sequences, BERT focuses \\non understanding context deeply by training on a masked language model objective. In \\nthis setup, random words in a sentence are replaced with a [MASK] token, and BERT tries \\nto predict the original word based on the surrounding context. Another innovative aspect \\nof BERT’s training regime is the next sentence prediction loss, where it learns to determine \\nwhether a given sentence logically follows a preceding one. By training on these objectives, \\nBERT captures intricate context dependencies from both the left and right of a word, and \\nit can discern the relationship between pairs of sentences. Such capabilities make BERT \\nespecially good at tasks that require natural language understanding, such as question-\\nanswering, sentiment analysis, and natural language inference, among others. Since this is an \\nencoder-only model, BERT cannot generate text.\\nGPT-2\\nGPT-2,12 the successor to GPT-1, was released in 2019 by OpenAI. The main innovation of \\nGPT-2 was a direct scale-up, with a tenfold increase in both its parameter count and the size \\nof its training dataset:\\n•\\t Data: GPT-2 was trained on a large (40GB) and diverse dataset called WebText, which \\nconsists of 45 million webpages from Reddit with a Karma rating of at least three. Karma \\nis a rating metric used on Reddit and a value of three means that all the posts were of a \\nreasonable level of quality.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 25}, page_content='Foundational Large Language Models & Text Generation\\n26\\nFebruary 2025\\n•\\t Parameters: GPT-2 had 1.5 billion parameters, which was an order of magnitude larger \\nthan the previous model. More parameters increase the model’s learning capacity. The \\nauthors trained four language models with 117M (the same as GPT-1), 345M, 762M, and 1.5B \\n(GPT-2) parameters, and found that the model with the most parameters performed better \\non every subsequent task.\\nThis scaling up resulted in a model that was able to generate more coherent and realistic text \\nthan GPT-1. Its ability to generate human-like responses made it a valuable tool for various \\nnatural language processing tasks, such as content creation and translation. Specifically, \\nGPT-2 demonstrated significant improvement in capturing long-range dependencies and \\ncommon sense reasoning. While it performed well in some tasks, it did not outperform state-\\nof-the-art reading comprehension, summarization, and translation. GPT-2’s most significant \\nachievement was its ability to perform zero-shot learning on a variety of tasks. Zero-shot task \\ntransfer is the ability of a model to generalize to a new task without being trained on it, which \\nrequires the model to understand the task based on the given instruction. For example, for \\nan English to German translation task, the model might be given an English sentence followed \\nby the word “German” and a prompt (“:”). The model would then be expected to understand \\nthat this is a translation task and generate the German translation of the English sentence. \\nGPT-2 was able to perform tasks such as machine translation, text summarization, and \\nreading comprehension without any explicit supervision.\\nThe study discovered that performance on zero-shot tasks increased in a log-linear manner \\nas the model’s capacity increased. GPT-2 showed that training on a larger dataset and having \\nmore parameters improved the model’s ability to understand tasks and surpass the state-of-\\nthe-art on many tasks in zero-shot settings.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 26}, page_content='Foundational Large Language Models & Text Generation\\n27\\nFebruary 2025\\nGPT-3/3.5/4\\nGPT-3,13 or the third iteration of the Generative Pre-trained Transformer model, represents a \\nsignificant evolution from its predecessor, GPT-2, primarily in terms of scale, capabilities, and \\nflexibility. The most noticeable difference is the sheer size of GPT-3, boasting a whopping \\n175 billion parameters, compared to GPT-2’s largest model which had 1.5 billion parameters. \\nThis increase in model size allowed GPT-3 to store and recall an even more vast amount of \\ninformation, understand nuanced instructions, and generate more coherent and contextually \\nrelevant text over longer passages.\\nWhile GPT-2 could be fine-tuned on specific tasks with additional training data, GPT-3 can \\nunderstand and execute tasks with just a few examples, or sometimes even without any \\nexplicit examples—simply based on the instruction provided. This highlights GPT-3’s more \\ndynamic understanding and adaptation abilities, reducing the need for task-specific fine-\\ntuning which was more prevalent in GPT-2.\\nFinally, GPT-3’s large model scale and diverse training corpus have led to better \\ngeneralization across a broader range of tasks. This means that out-of-the-box, without \\nany further training, GPT-3 exhibits improved performance on diverse NLP challenges, from \\ntranslation to question-answering, compared to GPT-2. It’s also worth noting that the release \\napproach differed: while OpenAI initially held back GPT-2 due to concerns about misuse, \\nthey chose to make GPT-3 available as a commercial API, reflecting both its utility and the \\norganization’s evolving stance on deployment.\\nInstruction tuning was then introduced with InstructGPT17, a version of GPT-3 that was fine-\\ntuned, using Supervised Fine-Tuning, on a dataset of human demonstrations of desired \\nmodel behaviors. Outputs from this model were then ranked and it was then further fine-\\ntuned using Reinforcement Learning from Human Feedback. This led to improved instruction'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 27}, page_content='Foundational Large Language Models & Text Generation\\n28\\nFebruary 2025\\nfollowing in the model. A 1.3B parameter InstructGPT model had better human evaluations \\nthan the 175B parameter GPT-3 model. It also showed improvements in truthfulness and \\nreductions in toxicity.\\nGPT-3.5 models, including GPT-3.5 turbo, improve over GPT-3 as it is capable of \\nunderstanding and generating code. It’s been optimized for dialogue. And it’s capable of \\nreceiving context windows of up to 16,385 tokens and can generate outputs of up to 4,096 \\ntokens. \\nGPT-4 extends GPT-3.5 as a large multimodal model capable of processing image and \\ntext inputs and producing text outputs.19 Specifically, accepting text or images as input \\nand outputting text. This model has broader general knowledge and advanced reasoning \\ncapabilities. It can receive context windows of up to 128,000 tokens and has a maximum \\noutput of 4,096 tokens. GPT-4 demonstrates remarkable versatility by solving complex tasks \\nacross diverse fields like mathematics, coding, vision, medicine, law, and psychology – all \\nwithout specialized instructions. Its performance often matches or even exceeds human \\ncapabilities and significantly outperforms earlier models like GPT-3.5.\\nLaMDA\\nGoogle’s LaMDA,20 which stands for ‘Language Model for Dialogue Applications’ is another \\ncontribution to the arena of large-scale language models, designed primarily to engage in \\nopen-ended conversations. Unlike traditional chatbots which operate in more constrained \\nand predefined domains, LaMDA is engineered to handle a wide array of topics, delivering \\nmore natural and flowing conversations. LaMDA was trained on dialogue-focused data to \\nencourage ongoing conversational flow, not just isolated responses, ensuring users can have \\nmore extensive and explorative dialogues.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 28}, page_content='Foundational Large Language Models & Text Generation\\n29\\nFebruary 2025\\nWhile GPT models, especially the later iterations like GPT-3, have strived to address a \\nmultitude of tasks simultaneously, from text generation to code writing, LaMDA’s primary \\nfocus is on maintaining and enhancing conversational depth and breadth. GPT models \\nshine on their ability to produce coherent long-form content and perform various tasks \\nwith minimal prompting, whereas LaMDA emphasizes the flow and progression of dialogue, \\nstriving to mimic the unpredictability and richness of human conversations. \\nGopher\\nGopher22 is a 280 billion parameter language model based on the decoder-only transformer \\narchitecture, developed by DeepMind in 2021.22 It can generate text, translate languages, \\nwrite different kinds of creative content, and answer your questions in an informative way. \\nSimilar to GPT-3, Gopher focused on improving dataset quality and optimization techniques:\\n•\\t Dataset: The researchers curated a high-quality text dataset called MassiveText, which \\ncontains over 10 terabytes of data and 2.45B documents from web pages, books, news \\narticles, and code (GitHub). They only trained on 300B tokens, which is 12% of the dataset. \\nImportantly, they improved the quality of the data by filtering it, such as by removing \\nduplicate text and deduplicating similar documents. This significantly improved the \\nmodel’s performance on downstream tasks.\\n•\\t Optimization: The researchers used a warmup learning rate for 1,500 steps and then \\ndecayed it using a cosine schedule. They also had an interesting rule that as they \\nincreased the model size, they decreased the learning rate and increased the number of \\ntokens in each batch. Additionally, they found that clipping gradients to be a maximum of 1 \\nbased on the global gradient norm helped stabilize the training.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 29}, page_content='Foundational Large Language Models & Text Generation\\n30\\nFebruary 2025\\nGopher was evaluated on a variety of tasks, including mathematics, common sense, logical \\nreasoning, general knowledge, scientific understanding, ethics, and reading comprehension. \\nGopher outperformed previous state-of-the-art models on 81% of the tasks. Specifically, \\nGopher performed well on knowledge-intensive tasks but struggled on reasoning-heavy \\ntasks such as abstract algebra.\\nThe authors also conducted a study on the effect of model size on different types of \\ntasks. Figure 4 shows the results of this ablation study. Specifically, the authors found that \\nincreasing the number of parameters had a significant impact on logical reasoning and \\nreading comprehension, but it did not improve performance as much on tasks such as \\ngeneral knowledge, where performance eventually almost plateaued.\\nFigure 5. Ablation study22 on the effect of model size on the performance of Gopher on different types \\nof tasks'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 30}, page_content='Foundational Large Language Models & Text Generation\\n31\\nFebruary 2025\\nGLaM\\nGLaM (Generalist Language Model)23 was the first sparsely-activated mixture-of-experts \\nlanguage model. Mixture-of-experts based models are much more computationally efficient \\ngiven their parameter count. This is achieved by only activating a subset of their parameters \\n(i.e. experts) for each input token. GLaM consists of 1.2 trillion parameters but uses only ⅓ \\nof the energy used to train GPT-3 and half of the FLOPs for inference while achieving better \\noverall performance compared to GPT-3.\\nChinchilla\\nUntil 2022, LLMs were primarily scaled by increasing the model size and using datasets that \\nare relatively small by current standards (up to 300 billion tokens for the largest models). \\nThis approach was informed by the Kaplan et al.24 study, which examined how performance \\nof a language model, measured by cross-entropy loss, varies with changes in computational \\nbudget, model size, and dataset size. Specifically, given a 100-fold increase in computational \\nresources (C), Kaplan et al.24 recommended scaling model size by approximately 28.8 times \\n(Nopt∝ C0.73), while increasing dataset size by only 3.5 times (Dopt∝ C0.27). \\nThe Chinchilla paper,25 revisited the compute optimal scaling laws and used three different \\napproaches to find that near equal scaling in parameters and data is optimal with increasing \\ncompute. Thus, a 100-fold increase in compute should translate into a tenfold increase in \\nboth data size and model size.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 31}, page_content='Foundational Large Language Models & Text Generation\\n32\\nFebruary 2025\\nFigure 6. Overlaid predictions from three different approaches from Chinchilla paper,25 along with \\nprojections from Kaplan et al24 \\nTo verify the updated scaling law, DeepMind trained a 70B parameter model (called \\nChinchilla) using the same compute budget as the previously trained Gopher model. \\nChinchilla uniformly and significantly outperformed Gopher (280B),21 GPT-3 (175B),13 and \\nMegatron-Turing NLG (530B)26 on a large range of downstream evaluation tasks. Due to being \\n4x smaller than Gopher, both the memory footprint and the inference cost of Chinchilla are \\nalso smaller.\\nThe findings of Chinchilla had significant ramifications for the development of future LLMs. \\nFocus shifted into finding ways to scale dataset size (while maintaining quality) alongside \\nincreasing parameter count. Extrapolating this trend suggests that training dataset size \\nmay soon be limited by the amount of text data available. This has led to new research by \\nMuennighoff et al.27 exploring scaling laws in data-constrained regimes.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 32}, page_content='Foundational Large Language Models & Text Generation\\n33\\nFebruary 2025\\nPaLM\\nPathways language model (PaLM)28 is a 540-billion parameter transformer-based large \\nlanguage model developed by Google AI. It was trained on a massive dataset of text and \\ncode and is capable of performing a wide range of tasks, including common sense reasoning, \\narithmetic reasoning, joke explanation, code generation, and translation.\\nAt the time of its release, PaLM was also able to achieve state-of-the-art performance on \\nmany language benchmarks, for example GLUE and SuperGLUE.29\\nOne of the key features of PaLM is its ability to scale efficiently. This is thanks to the \\nPathways system, which Google developed to distribute the training of large language \\nmodels across two TPU v4 Pods.\\nPaLM 2\\nPaLM 230 is a successor to PaLM that was announced in May 2023. Thanks to a number of \\narchitectural and training enhancements, PaLM 2 is even more capable than PaLM, with \\nfewer total parameters. It excels at advanced reasoning tasks, including code generation, \\nmath, classification, question answering, and translation.\\nPaLM 2 has also been shown to be more efficient than PaLM and became the basis for a \\nnumber of commercial models Google released as part of Google Cloud Generative AI.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 33}, page_content='Foundational Large Language Models & Text Generation\\n34\\nFebruary 2025\\nGemini\\nFigure 7. Gemini can receive multi-modal inputs including text, audio, images, and video data. These are all \\ntokenized and fed into its transformer model. The transformer generates an output that can contain images \\nand text.\\nGemini31 (Figure 6) is a state-of-the-art multimodal language family of models that can \\ntake interleaved sequences of text, image, audio, and video as input. It’s built on top of \\ntransformer decoders and has architectural improvements for scale as well as optimized \\ninference on Google’s Tensor Processing Units (TPUs). In its current version, these models \\nare trained to support contexts of different sizes, up to 2M tokens in the Gemini Pro version \\non Vertex AI and employ mechanisms such as multi-query attention for efficiency. Gemini \\nmodels also employ a Mixture of Experts architecture to optimize efficiency and capabilities \\nof the models. Multimodality allows the models to process text, images and video in input, \\nwith more modalities in input and output expected in the future.\\nThe Gemini models are trained on Google’s TPUv5e and TPUv4 processors, depending on \\nsize and configuration. The pre-training data consists of web documents, books, code, and \\nimage, audio, and video data.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 34}, page_content='Foundational Large Language Models & Text Generation\\n35\\nFebruary 2025\\nLarger models are trained for the compute-optimal number of tokens using the same \\napproach as in Chinchilla paper,25 while small models are trained on significantly more tokens \\nthan compute optimal to improve performance for a given inference budget.\\nThe Gemini family of models is optimized for different sizes: Gemini Ultra, Gemini Pro, Gemini \\nNano and Flash. Gemini Ultra is used for highly complex tasks and achieves state-of-the-\\nart results in 30 out of 32 benchmark tasks. Gemini Pro enables deployment at scale and \\nGemini Nano is designed for on-device applications. The Gemini Nano models leverage \\nadvancements such as distillation to produce state-of-the-art performance for small \\nlanguage models on tasks such as summarization and reading comprehension. As the Gemini \\nmodels are natively multi-modal, it can be seen that training across multiple modalities does \\nindeed lead to a model that is capable of achieving strong capabilities in each domain. \\nDuring the initial part of 2024, Google introduced the latest model of the Gemini family, \\nGemini 1.5 Pro,32 a highly compute-efficient multimodal mixture-of-experts model. This \\nmodel  also dramatically increased the size of the context window to millions of tokens \\nand is capable of recalling and reasoning over those millions of tokens, including multiple \\nlong documents and hours of video and audio. Gemini 1.5 Pro demonstrates remarkable \\ncapabilities across different domains:\\n•\\t Code understanding: It can process massive codebases and answer highly specific \\ncode-related questions.\\n•\\t Language learning: The model can learn new languages never observed at training time \\nsolely based on reference materials provided within its input\\n•\\t Multimodal reasoning: It understands images and text, allowing it to locate a famous scene \\nfrom the novel ‘Les Misérables’ based on a simple sketch.\\n•\\t Video comprehension: It can analyze entire movies, answering detailed questions and \\npinpointing specific timestamps with remarkable accuracy.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 35}, page_content='Foundational Large Language Models & Text Generation\\n36\\nFebruary 2025\\nGoogle’s Gemini 1.5 Pro model excels at retrieving information from even very long \\ndocuments. In their study,32 it demonstrated 100% recall on documents up to 530,000 \\ntokens, and over 99.7% recall on those up to 1 million tokens. Impressively, it maintains 99.2% \\naccuracy when finding information in documents up to 10 million tokens.\\nMoreover, Gemini 1.5 Pro demonstrates a major leap forward in how well LLMs follow complex \\ninstructions. In a rigorous test with 406 multi-step prompts, it significantly outperformed \\nprevious Gemini models. The model accurately followed almost 90% of instructions and fully \\ncompleted 66% of the complex tasks. \\nGemini Flash is a new addition to the Gemini model family and the fastest Gemini model \\nserved in the API. It’s optimized for high-volume, high-frequency tasks at scale, is more \\ncost-efficient to serve and features a breakthrough long context window of 1 million tokens. \\nAlthough it is a lighter weight model than 1.5 Pro, it is highly capable of multimodal reasoning \\nacross vast amounts of information and delivers impressive quality for its size.\\nGemini 2.0 represents a significant leap forward in Google’s multimodal AI models. It builds \\nupon the foundation of Gemini 1.0 with enhanced capabilities and a focus on efficiency and \\nnew modalities. In more details :\\nGemini 2.0 Flash: This version is designed for speed and efficiency, exceeding the \\nperformance of Gemini 1.5 Pro while maintaining the responsiveness developers expect. It \\nshowcases improvements in multimodal understanding, text processing, code generation, \\nvideo analysis, and spatial reasoning. Notably, it has enhanced spatial understanding, leading \\nto more accurate object identification and captioning, especially for small objects in complex \\nscenes. Gemini 2.0 Flash was introduced in late 2024.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 36}, page_content='Foundational Large Language Models & Text Generation\\n37\\nFebruary 2025\\nGemini 2.0 Pro is positioned as a highly capable model for a broad range of tasks. It likely \\nserves as a workhorse for various applications, balancing performance and efficiency. \\nIt is likely an evolution of the original Gemini Pro model with improvements across \\nmultiple domains.\\nGemini 2.0 Nano: As with the previous generation, Nano focuses on on-device deployment. \\nIt is optimized for resource efficiency and speed, enabling AI capabilities directly on devices \\nlike smartphones. \\nGemini 2.0 Flash Thinking Experimental is a fast, high-performance reasoning model, \\nenhanced with explainability through visible “thought processes,” particularly excelling \\nin complex science and math problems; it accepts text and image inputs, produces text \\noutputs, supports a 1 million token input context and a 64,000 token output, utilizes code \\nexecution, has a knowledge cutoff of August 2024, is best suited for complex tasks where \\nlatency is not a primary concern, and is available via Google AI Studio, the Gemini API, and \\nVertex AI, though currently in an experimental deployment status.\\nGemma\\nFurthermore, recently advanced Gemma is a family of lightweight, state-of-the-art open \\nmodels built from the same research and technology used to create the Gemini models.33 The \\nfirst model by Gemma boasts a large vocabulary of 256,000 words and has been trained on \\na massive 6 trillion token dataset. This makes it a valuable addition to the openly-available \\nLLM collection. Additionally, the 2B parameter version is intriguing as it can run efficiently on \\na single GPU.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 37}, page_content='Foundational Large Language Models & Text Generation\\n38\\nFebruary 2025\\nGemma 2,33 developed by Google AI, represents a significant advancement in the field of \\nopen large language models. Designed with a focus on efficiency, the 27-billion parameter \\nmodel boasts performance comparable to much larger models like Llama 3 70B33 on standard \\nbenchmarks. This makes Gemma 2 a powerful and accessible tool for a wide range of AI \\ndevelopers. Its compatibility with diverse tuning toolchains, from cloud-based solutions \\nto popular community tools, further enhances its versatility. With its strong performance, \\nefficient architecture, and accessible nature, Gemma 2 plays a vital role in driving innovation \\nand democratizing AI capabilities.\\nGemma 3 represents Google’s latest advancement in its family of open models, built upon \\nthe research and technology that also powers the Gemini models. A key feature of Gemma \\n3 is its multimodality, enabling it to process both text and image inputs and generate text \\noutputs. This version significantly expands capabilities with a large, 128K context window, and \\nbroad multilingual support encompassing over 140 languages. To cater to diverse hardware \\nand performance needs, Gemma 3 is available in various sizes, including 1B, 4B, 12B, and 27B \\nparameter models. These varieties allow developers to select the most suitable model for \\ntheir specific applications, ranging from resource-constrained devices to high-performance \\ncomputing environments.\\nLLaMA\\nLlama models are transformer-based language models, similar in high-level architecture \\nto other large language models (LLMs) like GPT. They are primarily based on the decoder-\\nonly architecture, meaning they focus on predicting the next token in a sequence given the \\npreceding tokens.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 38}, page_content='Foundational Large Language Models & Text Generation\\n39\\nFebruary 2025\\nMeta has released several key versions of Llama. The original Llama 1 models came in various \\nsizes, from 7B to 65B parameters, and were notable for their strong performance compared \\nto other open-source models of similar size. Llama 234 represented a major advancement, \\nfeaturing a larger context window extended to 4096 tokens for handling longer texts, and \\nimportantly, it was fine-tuned for chat applications, significantly improving its conversational \\nabilities. Llama 2 was offered in 7B, 13B, and 70B parameter versions and, unlike Llama 1, was \\nreleased with a license allowing commercial use. Llama 3 builds upon these advancements \\nwith enhanced performance across reasoning, coding, and general knowledge, and is \\nexpected to include a wider range of sizes. A key focus of Llama 3 is increased safety, with \\nefforts to reduce harmful outputs through improved training and alignment techniques. Llama \\n2 is a family of pretrained and fine-tuned LLMs, ranging from 7B to 70B parameters, with \\nimprovements like a 40% larger training dataset, doubled context length, and grouped-query \\nattention. The fine-tuned version, Llama 2-Chat, excels in dialogue. The next generation, \\nLlama 3.2, includes multilingual text-only models and vision LLMs, with quantized versions for \\non-device deployment. Llama 3.2 uses grouped-query attention and a 128K token vocabulary.\\nMixtral\\nDeveloped by Mistral AI35, Mixtral 8x7B is a Sparse Mixture of Experts (SMoE) model. While its \\ntotal parameter count is 47B, it utilizes only 13B active parameters per token during inference, \\nleading to faster inference and higher throughput. This model excels in mathematics, code \\ngeneration, and multilingual tasks, often outperforming LLaMA 2 70B in these domains. \\nMixtral also supports a 32k token context length, enabling it to handle significantly longer \\nsequences. Its instruction-tuned version, Mixtral 8x7B- Instruct, surpasses several closed-\\nsource models on human evaluation benchmarks. Mistral makes several of its models \\navailable as open source under the Apache 2.0 license, emphasizing open access to model \\nweights. In addition, Mistral offers a range of models through its API, providing various sizes \\nand capabilities to suit diverse requirements.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 39}, page_content='Foundational Large Language Models & Text Generation\\n40\\nFebruary 2025\\nOpenAI O1\\nOpenAI’s new “o1” series represents a significant advancement in models, focusing on \\ncomplex reasoning abilities honed through reinforcement learning. These models employ an \\ninternal “chain-of-thought” process, engaging in extensive deliberation before generating \\na response. This deliberate approach results in exceptional performance on challenging \\nscientific reasoning tasks. Benchmarks demonstrate their proficiency: o1 models achieve \\nan 89th percentile ranking on Codeforces programming competitions, score within the top \\n500 nationally on the AIME (a USA Math Olympiad qualifier), and surpass PhD-level human \\naccuracy on a comprehensive physics, biology, and chemistry benchmark (GPQA). The API \\noffers two variants: o1: The flagship model, optimized for tackling difficult problems that \\nrequire broad, general world knowledge, and  o1-mini: A faster, more cost-effective version, \\nexcelling in domains like coding, mathematics, and scientific tasks where deep specialized \\nknowledge is more critical than extensive general knowledge.\\nDeepSeek\\nDeepSeek has demonstrated that competitive reasoning performance, comparable to \\nOpenAI’s “o1” series, can be achieved through a novel reinforcement learning approach, \\neven without relying on extensive labeled data. This is exemplified by their DeepSeek-R1-\\nZero model, trained purely with RL. Traditional RL methods for LLMs often depend on a \\n“critic” model, trained on labeled data, to provide feedback. DeepSeek’s innovation, called \\nGroup Relative Policy Optimization (GRPO), eliminates this critic. Instead, GRPO uses a set \\nof predefined rules (assessing coherence, completeness, and fluency) to score the model’s \\noutputs across multiple rounds. The model learns by comparing its performance against the \\ngroup average, effectively learning from its own “self-play” without explicit human-provided'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 40}, page_content='Foundational Large Language Models & Text Generation\\n41\\nFebruary 2025\\nlabels. This pure-RL approach, while successful in achieving high reasoning scores (matching \\n“o1” on the AIME 2024 mathematics competition), initially resulted in outputs with poor \\nreadability and language mixing.\\nTo address these shortcomings, DeepSeek developed a multi-stage training process for \\ntheir DeepSeek-R1 model. This process begins with supervised fine-tuning (SFT) on a small \\n“cold start” dataset, providing a basic foundation of language understanding. Next, pure-RL \\n(using GRPO) is applied to enhance reasoning abilities, similar to the R1-Zero model. Critically, \\nnear the end of the RL phase, rejection sampling (e.g. filtering)  is employed. The model \\ngenerates multiple outputs, and only the best, according to the GRPO rules, are selected. \\nThis creates a high-quality “synthetic” dataset generated by the model itself. This synthetic \\ndata is then combined with supervised data from the original base model (covering areas \\nlike writing and factual knowledge). A final round of fine-tuning and further RL is performed, \\nleveraging both the synthetic and supervised data, refining the model’s overall performance \\nand generalization capabilities. This multi-stage approach leverages the strengths of each \\ntraining method: the initial SFT provides a basic linguistic foundation; pure-RL fosters strong \\nreasoning skills; rejection sampling creates high-quality training data; and the final SFT \\nand RL steps ensure a polished, well-rounded model. The result is the DeepSeek-R1 model \\nthat matches or exceeds the o1 model in many areas. Chain-of-thought (CoT) reasoning at \\ninference time is intrinsically linked to this RL-based training. The model learns to generate \\nintermediate reasoning steps during training, which are essential for its strong performance \\non complex tasks at inference. Despite providing model weights, DeepSeek’s models are \\neffectively closed-source due to the lack of transparency regarding training data, processing \\nscripts, and data curation methods.\\nOther open models\\nThe landscape of open LLMs is rapidly evolving, with a growing number of models where \\nboth the code and pre-trained weights are publicly accessible. Below we highlight some of \\nthe known examples:'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 41}, page_content='Foundational Large Language Models & Text Generation\\n42\\nFebruary 2025\\n•\\t Qwen 1.536: This LLM series from Alibaba comes in six sizes: 0.5B, 1.8B, 4B, 7B, 14B, and \\n72B. Qwen 1.5 models uniformly support a context length of up to 32k tokens and show \\nstrong performance across various benchmarks. Notably, Qwen 1.5-72B outperforms \\nLLaMA2-70B on all evaluated benchmarks, demonstrating exceptional capabilities in \\nlanguage understanding, reasoning, and math.\\n•\\t Yi37: Created by 01.AI, the Yi model family includes 6B and 34B base models pre-trained \\non a massive 3.1 trillion token English and Chinese dataset. Yi emphasizes data quality \\nthrough rigorous cleaning and filtering processes. The 34B model achieves performance \\ncomparable to GPT-3.5 on many benchmarks and can be efficiently served on consumer-\\ngrade GPUs with 4-bit quantization. Yi also offers extensions like a 200k context model, a \\nvision-language model (Yi-VL), and a depth-upscaled 9B model.\\n•\\t Grok 3: Developed by xAI, Grok-3 is released in Grok 3 (Think) and Grok 3 mini (Think). \\nBoth models were trained using reinforcement learning. Grok 3 (Think) learned to refine its \\nproblem-solving strategies, correct errors through backtracking, simplify steps, and utilize \\nthe knowledge it picked up during pretraining. With a context window of 1 million tokens its \\n8 times larger than previous Grok models.\\nThe pace of innovation with LLMs has been rapid and shows no signs of slowing down. There \\nhave been many contributions to the field in both the academic and commercial settings. \\nWith over 20,000 papers published about LLMs in arxiv.org it is impossible to name all \\nof the models and teams that have contributed to the development of LLMs. However, an \\nabbreviated list of open models of interest could include EleutherAI’s GPT-NeoX and GPT-J, \\nStanford’s Alpaca, Vicuna from LMSYS, Grok from xAI, Falcon from TII, PHI from Microsoft, \\nNVLM from Nvidia, DBRX from Databricks, Qwen from Alibaba, Yi from 01.ai, Llama from \\nMeta mentioned above and many others. Some of notable companies developing commercial \\nfoundation LLM models include Anthropic, Cohere, Character.ai, Reka, AI21, Perplexity, xAI \\nand many others in addition to Google and OpenAI mentioned in previous sections. It is \\nimportant when using a model to confirm that the license is appropriate for your use case as \\nmany models are provided with very specific terms of use.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 42}, page_content='Foundational Large Language Models & Text Generation\\n43\\nFebruary 2025\\nComparison\\nIn this section, we observed how transformer-based language models have evolved. They \\nstarted as encoder-decoder architectures with hundreds of millions of parameters trained \\non hundreds of millions of tokens, and have grown to be massive decoder-only architectures \\nwith billions of parameters and trained on trillions of tokens. Table 1 shows how the \\nimportant hyperparameters for all the models discussed in this whitepaper have evolved \\nover time. The scaling of data and parameters has not only improved the performance of \\nLLMs on downstream tasks, but has also resulted in emergent behaviors and zero- or few-\\nshot generalizations to new tasks. However, even the best of these LLMs still have many \\nlimitations. For example, they are not good at engaging in human-like conversations, their \\nmath skills are limited, and they might not be aligned with human ethics (e.g., they might be \\nbiased or generate toxic responses). In the next section, we learn how a lot of these issues \\nare being addressed.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 43}, page_content='Foundational Large Language Models & Text Generation\\n44\\nFebruary 2025\\nModel\\nAttention\\n(2017)\\nGPT \\n(2018)\\nGPT-2\\n(2019)\\nGPT-3\\n(2020)\\nLaMDA  \\n(2021)\\nGopher\\n(2021)\\nChinchilla\\n(2022)\\nOptimizer\\nADAM\\nADAM\\nADAM\\nADAM\\nADAM\\nADAM\\nADAM-W\\n# Parameters\\n213M\\n117M\\n1.5B\\n175B\\n137B\\n280B\\n70B\\nVocab size\\n~37K \\n~40K\\n~50K\\n~50K\\n~32K\\n~32K\\n~32K\\nEmbedding \\ndimension \\n1024\\n768\\n1600\\n12288\\n8192\\n16384\\n8192\\nKey dimension\\n64\\n64\\n64\\n128\\n128\\n128\\n128\\n# heads (H)\\n16\\n12\\n25\\n96\\n128\\n128\\n64\\n# encoder \\nlayers \\n6\\nN/A \\nN/A\\nN/A\\nN/A\\nN/A\\nN/A\\n# decoder \\nlayers \\n6\\n12\\n48\\n96\\n64\\n80\\n80\\nFeed forward \\ndimension \\n4 * 1024\\n4 * 768\\n4 * 1600\\n4 * 12288\\n8 * 8192\\n4 * 16384\\n4 * 8192\\nContext Token \\nSize\\nN/A\\n512\\n1024\\n2048\\nN/A\\n2048\\n2048\\nPre-Training \\ntokens \\n~160MA \\n~1.25BA \\n~10B \\n~300B\\n~168B\\n~300B \\n~1.4T\\nTable 1. Important hyperparameters for transformers-based large language models\\nA.\\t This number is an estimate based on the reported size of the dataset.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 44}, page_content='Foundational Large Language Models & Text Generation\\n45\\nFebruary 2025\\nFine-tuning large language models\\nLarge language models typically undergo multiple training stages. The first stage, often \\nreferred to as pre-training, is the foundational stage where an LLM is trained on large, \\ndiverse, and unlabelled text datasets where it’s tasked to predict the next token given the \\nprevious context. The goal of this stage is to leverage a large, general distribution of data \\nand to create a model that is good at sampling from this general distribution. After language \\nmodel pretraining, the resulting LLM usually demonstrates a reasonable level of language \\nunderstanding and language generation skills across a variety of different tasks which \\nare typically tested through zero-shot or few-shot prompting (augmenting the instruction \\nwith a few examples / demonstrations). Pretraining is the most expensive in terms of time \\n(from weeks to months depending on the size of the model) and the amount of required \\ncomputational resources, (GPU/TPU hours).\\nAfter training, the model can be further specialized via fine-tuning, typically called \\ninstruction-tuning or simply supervised fine-tuning (SFT). SFT involves training an LLM on a \\nset of task-specific demonstration datasets where its performance is also measured across \\na set of domain-specific tasks. The following are some examples of behaviors that can be \\nimproved using fine-tuning:\\n•\\t Instruction-tuning/instruction following: The LLM is provided as input an instruction to \\nfollow which might include summarizing a piece of text, writing a piece of code, or writing \\na poem in a certain style.17\\n•\\t Dialogue-tuning: This is a special case of instruction tuning where the LLM is fine-tuned \\non conversational data in the form of questions and responses. This is often called \\nmulti-turn dialogue.39'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 45}, page_content='Foundational Large Language Models & Text Generation\\n46\\nFebruary 2025\\n•\\t Safety tuning: This is crucial for mitigating risks associated with bias, discrimination, and \\ntoxic outputs. It involves a multi-pronged approach encompassing careful data selection, \\nhuman-in-the-loop validation, and incorporating safety guardrails. Techniques like \\nreinforcement learning with human feedback (RLHF)40 enable the LLM to prioritize safe \\nand ethical responses.\\nFine-tuning is considerably less costly and more data efficient compared to pre-training. \\nNumerous techniques exist to optimize the costs further which are discussed later in \\nthis whitepaper.\\nSupervised fine-tuning \\nAs mentioned in the previous section, SFT is the process of improving an LLM’s performance \\non a specific task or set of tasks by further training it on domain-specific, labeled data. The \\ndataset is typically significantly smaller than the pre-training datasets, and is usually human-\\ncurated and of high quality. \\nIn this setting, each data point consists of an input (prompt) and a demonstration (target \\nresponse). For example, questions (prompt) and answers (target response), translations from \\none language (prompt) to another language (target response), a document to summarize \\n(prompt), and the corresponding summary (target response). \\nIt’s important to note that, while fine-tuning can be used to improve the performance on \\nparticular tasks as mentioned above, it can also serve the purpose of helping the LLM \\nimprove its behavior to be safer, less toxic, more conversational, and better at following \\ninstructions.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 46}, page_content='Foundational Large Language Models & Text Generation\\n47\\nFebruary 2025\\nReinforcement learning from human feedback\\nTypically, after performing SFT, a second stage of fine-tuning occurs which is called \\nreinforcement learning from human feedback (RLHF). This is a very powerful fine-tuning \\ntechnique that enables an LLM to better align with human-preferred responses (i.e. making \\nits responses more helpful, truthful, safer, etc.). \\nFigure 8. An example RLHF procedure \\nIn contrast to SFT, where an LLM is only exposed to positive examples (e.g. high-quality \\ndemonstration data), RLHF makes it possible to also leverage negative outputs thus \\npenalizing an LLM when it generates responses that exhibit undesired properties. Penalizing \\nnegative output makes it less likely to generate unhelpful or unsafe responses. \\nTo leverage RLHF, a reward model (RM) typically needs to be trained with a procedure similar \\nto that in Figure 8. An RM is usually initialized with a pretrained transformer model, often also \\none that is SFT. Then it is tuned on human preference data which is either single sided (with a \\nprompt, response and a score) or composed of a prompt and a pair of responses along with'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 47}, page_content='Foundational Large Language Models & Text Generation\\n48\\nFebruary 2025\\na preference label indicating which of the two responses was preferred. For example, given \\ntwo summaries, A and B, of the same article, a human rater selects a preferred summary \\n(relying on the detailed guidance). We refer to the provided preference labels as human \\nfeedback. Preferences can be in the binary form (e.g. ‘good’ or ‘bad’), on the Likert scale42, \\nrank order when more than 2 candidates are evaluated, or a more detailed assessment of the \\nsummary quality. The preference signal can also incorporate many dimensions that capture \\nvarious aspects that define a high quality response, e.g., as safety, helpfulness, fairness, and \\ntruthfulness. \\nFigure 8 shows a typical RLHF pipeline where a Reward model is initialized and finetuned on \\npreference pairs. Once an RM has been trained, it’s then used by a Reinforcement Learning \\n(RL)43 policy gradient algorithm, which further finetunes a previously instruction-tuned LLM to \\ngenerate responses that are better aligned with human preferences. \\nTo better scale RLHF, RL from AI Feedback (RLAIF)44 leverages AI feedback instead of human \\nfeedback to generate preference labels. It’s also possible to remove the need for training \\nRLHF by leveraging approaches such as direct preference optimization (DPO).45 Both RLHF \\nand RLAIF can be used on Google Cloud.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 48}, page_content='Foundational Large Language Models & Text Generation\\n49\\nFebruary 2025\\nParameter Efficient Fine-Tuning\\nBoth SFT and RLHF are still very costly in terms of compute time and accelerators required, \\nespecially when full-fine tuning entire LLMs on the orders of billions of parameters. Luckily, \\nthere are some really useful and effective techniques that can make fine-tuning significantly \\ncheaper and faster compared to pre-training and full fine-tuning. One such family of \\nmethods is parameter efficient fine-tuning (PEFT) techniques. \\nAt a high-level, PEFT approaches append a significantly smaller set of weights (e.g., on the \\norder of thousands of parameters) that are used to ‘perturb’ the pre-trained LLM weights. \\nThe perturbation has the effect of fine-tuning the LLM to perform a new task or set of tasks. \\nThis has the benefit of training a significantly smaller set of weights, compared to traditional \\nfine-tuning of the entire model. \\nSome common PEFT techniques include the adapter, low-rank adaptation, and \\nsoft prompting:\\n•\\t Adapter-based fine-tuning46 employs small modules, called adapters, to the pre-\\ntrained model. Only the adapter parameters are trained, resulting in significantly fewer \\nparameters than traditional SFT. \\n•\\t Low-Rank Adaptation (LoRA)47 tackles efficiency differently. It uses two smaller matrices \\nto approximate the original weight matrix update instead of fine-tuning the whole LLM. \\nThis technique freezes the original weights and trains these update matrices, significantly \\nreducing resource requirements with minimum additional inference latency. Additionally, \\nLoRA has improved variants such as QLoRA,48 which uses quantized weights for even \\ngreater efficiency. A nice advantage of LoRA modules is that they can be plug-and-play, \\nmeaning you can train a LoRA module that specializes in one task and easily replace it with \\nanother LoRA module trained on a different task. It also makes it easier to transfer the \\nmodel since assuming the receiver has the original matrix, only the update matrices need \\nto be provided.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 49}, page_content='Foundational Large Language Models & Text Generation\\n50\\nFebruary 2025\\n•\\t Soft prompting49 is a technique for conditioning frozen large language models with \\nlearnable vectors instead of hand-crafted text prompts. These vectors, called soft \\nprompts, are optimized on the training data and can be as few as five tokens, making them \\nparameter-efficient and enabling mixed-task inference. \\nFor most tasks, full fine-tuning is still the most performant, followed by LoRA and Soft \\nprompting, but the order is reversed when it comes to cost. All three approaches are more \\nmemory efficient than traditional fine-tuning and achieve comparable performance.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 50}, page_content='Foundational Large Language Models & Text Generation\\n51\\nFebruary 2025\\nPython\\n# Before you start run this command:\\n# pip install --upgrade --user --quiet google-cloud-aiplatform\\n# after running pip install make sure you restart your kernel\\nimport vertexai\\nfrom vertexai.generative_models import GenerativeModel\\nfrom vertexai.preview.tuning import sft\\n# TODO : Set values as per your requirements\\n# Project and Storage Constants\\nPROJECT_ID = ‘<project_id>’\\nREGION = ‘<region>’\\nvertexai.init(project=PROJECT_ID, location=REGION)\\n# define training & eval dataset.\\nTRAINING_DATASET = ‘gs://cloud-samples-data/vertex-ai/model-evaluation/\\npeft_train_sample.jsonl’\\n# set base model and specify a name for the tuned model\\nBASE_MODEL = ‘gemini-1.5-pro-002’\\nTUNED_MODEL_DISPLAY_NAME = ‘gemini-fine-tuning-v1’\\n# start the fine-tuning job\\nsft_tuning_job = sft.train(\\n   source_model=BASE_MODEL,\\n   train_dataset=TRAINING_DATASET,\\n   # # Optional:\\n   tuned_model_display_name=TUNED_MODEL_DISPLAY_NAME,\\n)\\n# Get the tuning job info.\\nsft_tuning_job.to_dict()\\n# tuned model endpoint name\\ntuned_model_endpoint_name = sft_tuning_job.tuned_model_endpoint_name\\n# use the tuned model\\ntuned_genai_model = GenerativeModel(tuned_model_endpoint_name)\\nprint(tuned_genai_model.generate_content(contents=’What is a LLM?’))\\nSnippet 1. SFT fine tuning on Google cloud'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 51}, page_content='Foundational Large Language Models & Text Generation\\n52\\nFebruary 2025\\nUsing large language models\\nPrompt engineering and sampling techniques have a strong influence on the performance of \\nLLMs. Prompt engineering is the process of designing and refining the text inputs (prompts) \\nthat you feed into an LLM to achieve desired and relevant outputs. Sampling techniques \\ndetermine the way in which output tokens are chosen and influence the correctness, \\ncreativity and diversity of the resulting output. We next discuss different variants of prompt \\nengineering and sampling techniques as well as touch on some important parameters that \\ncan have a significant impact on LLM performance.\\nPrompt engineering \\nLLMs are very powerful, but they still need guidance to unleash their full potential. Prompt \\nengineering is a critical component in guiding an LLM to yield desired outputs. This might \\ninclude grounding the model to yield factual responses or unleashing the creativity of the \\nmodel to tell a story or write a song. Examples of prompt engineering include providing \\nclear instructions to the LLM, giving examples, using keywords, and formatting to emphasize \\nimportant information, providing additional background details etc. \\nYou will often hear the terms zero-shot, few-shot, and chain-of-thought prompting in the \\ncontext of prompt engineering. We define these terms below: \\n•\\t Few-shot prompting: This is when you provide the LLM with a task description, as well \\nas a few (e.g. three to five) carefully chosen examples, that will help guide the LLM’s \\nresponse. For example, you might provide the model with the name of a few countries \\nand their capital cities, then ask it to generate the capital for a new country that isn’t in \\nthe examples.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 52}, page_content='Foundational Large Language Models & Text Generation\\n53\\nFebruary 2025\\n•\\t Zero-shot prompting: This is when you provide the LLM directly with a prompt with \\ninstructions. You usually give the LLM a task description and the LLM relies heavily on its \\nexisting knowledge to output the correct response. This requires no additional data or \\nexamples, hence the name ‘Zero-shot’ but can be less reliable than few-shot prompting.\\n•\\t Chain-of-thought prompting: This technique aims to improve performance on complex \\nreasoning tasks. Rather than simply asking the LLM a question, you provide a prompt \\nthat demonstrates how to solve similar problems using step-by-step reasoning. The \\nLLM then generates its own chain of thought for the new problem, breaking it down into \\nsmaller steps and explaining its reasoning. Finally, it provides an answer based on its \\nreasoning process.\\nPrompt engineering is an active area of research.\\nSampling Techniques and Parameters\\nA variety of sampling techniques can be employed to determine how the model chooses \\nthe next token in a sequence. They are essential for controlling the quality, creativity, and \\ndiversity of the LLM’s output. The following is a breakdown of different sampling techniques \\nand their important parameters:\\n•\\t Greedy search50: Selects the token with the highest probability at each step. This is the \\nsimplest option but it can lead to repetitive and predictable outputs.\\n•\\t Random sampling:50 Selects the next token according to the probability distribution, where \\neach token is sampled proportionally to its predicted probability. This can produce more \\nsurprising and creative text, but also a higher chance of nonsensical output.\\n•\\t Temperature sampling:50 Adjusts the probability distribution by a temperature parameter. \\nHigher temperatures promote diversity, lower temperatures favor high-probability tokens.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 53}, page_content='Foundational Large Language Models & Text Generation\\n54\\nFebruary 2025\\n•\\t Top-K sampling: Randomly samples from the top K most probable tokens. The value of K \\ncontrols the degree of randomness.\\n•\\t Top-P sampling (nucleus sampling):51 Samples from a dynamic subset of tokens whose \\ncumulative probability adds up to P. This allows the model to adapt the number of potential \\ncandidates depending on its confidence, favoring more diversity when uncertain and \\nfocusing on a smaller set of highly probable words when confident.\\n•\\t Best-of-N sampling: Generates N separate responses and selects the one deemed best \\naccording to a predetermined metric (e.g., a reward model or a logical consistency check). \\nThis is particularly useful for short snippets or situations where logic and reasoning \\nare key.\\nBy combining prompt engineering with sampling techniques and correctly calibrated \\nhyperparameters, you can greatly influence the LLM’s response, making it more relevant, \\ncreative, and consistent for your specific needs.\\nUntil now, we have seen the various types of LLM architectures, their underlying technology, \\nas well as the approaches used to train, tune, and adapt these models for various tasks. Let’s \\nnow look at some key research about how the decoding process in LLMs can be sped up \\nconsiderably to generate faster responses.\\nTask-based Evaluation\\nThe emergence of LLMs has reduced the obstacles to building AI applications, but moving \\nfrom MVP to production introduces challenges such as prompt engineering, model selection, \\nand performance monitoring. A tailored evaluation framework is essential for navigating \\nLLM application development by validating functionality and user experience, identifying'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 54}, page_content='Foundational Large Language Models & Text Generation\\n55\\nFebruary 2025\\npotential issues, facilitating communication about capabilities, and establishing a roadmap \\nfor improvement. For building a tailored evaluation framework, application builders need to \\nprovide their own evaluation data, development context, a definition of good performance.  \\n•\\t Evaluation data: Public leaderboards that showcase LLM capabilities often fall short for \\napplication developers who require a more tailored approach. A dedicated evaluation \\ndataset that mirrors the expected production traffic as closely as possible is needed. \\nDuring prototyping this can be a manually curated dataset, one that is continuously \\nenriched with real user interactions, production logs, and synthetically generated data to \\ntest specific scenarios.\\n•\\t Development Context: The evaluation should extend beyond just the model’s output \\nto analyze the entire system, including components like data augmentation (e.g., \\nRetrieval Augmented Generation or RAG) and agentic workflows. This approach ensures \\nunderstanding of how all components interact and contribute to the application’s \\noverall performance.\\n•\\t Definition of “Good”: Traditional metrics that prioritize matching a single “correct” \\nanswer can unfairly penalize unexpected solutions. When working with LLMs, we can \\naddress this by moving beyond similarity to ground truth as a definition of good, but \\nrather establish dataset level criteria that reflect desired business outcomes or even \\nrubrics that capture the core elements of the desired outputs depending on the input user \\ninstructions. \\nApplication builders can evaluate LLM performance using three methods:\\n•\\t Traditional Evaluation Methods: Similar to evaluating predictive models, these methods \\nuse quantitative metrics to compare model outputs to ideal responses, aiming to offer \\nobjective insights. However, they may penalize creative or unexpected outputs, limiting \\ntheir effectiveness for generative tasks that have multiple possible solutions.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 55}, page_content='Foundational Large Language Models & Text Generation\\n56\\nFebruary 2025\\n•\\t Human Evaluation: Considered the gold standard, human judgment provides nuanced \\nassessment of complex generative outputs.\\n•\\t LLM-Powered Autoraters: LLM-powered autoraters try to mimic human judgment, \\noffering scalable and efficient evaluations. Unlike computation-based methods, they \\ncan operate with or without reference data. A basic setup involves providing the task, \\ncriteria, and candidate responses (with optional references), which the autorater uses to \\ngenerate and parse LLM output for final evaluation results. In addition to the final outputs, \\nautoraters can provide rationales to explain a given decision to the user. While generative \\nmodels are common autoraters, reward and discriminative models are also used. Crucially, \\nlike any measurement tool, autoraters require calibration. Meta-evaluation, i.e. comparing \\nautorater outputs to human judgments, ensures the autorater aligns with desired \\npreferences. This calibration typically involves agreement in terms of model preference \\nor correlation measures, tailored to the evaluation task. In this meta-evaluation, it is \\nimportant to keep in mind potential limitations of autorater models.69\\nWhile the basic setup described above focuses on providing autoraters with fixed criteria \\nthat can be used to evaluate an entire dataset of examples, approaches are emerging to \\nleverage rubrics and multi-step processes in order to obtain interpretable evaluation metrics. \\nAt a high level, an LLM breaks an example down into multiple evaluation subtasks and then \\nevaluates each subtask to give an interpretable, detailed report for this example. As the \\ntask has been broken down, domain-specialized models can be leveraged for specific tasks \\nto improve reliability if necessary. Results are then aggregated for a given example to yield \\nan overall score, or across related subtasks to evaluate how well a model performs along a \\nparticular axis. This setup is especially useful in media generation, where different examples \\nmay require vastly different skills (e.g. object generation vs text generation) and a single \\nscore obfuscates how relative strengths and weaknesses of different models contribute to \\nthe final result.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 56}, page_content='Foundational Large Language Models & Text Generation\\n57\\nFebruary 2025\\nAccelerating inference\\nThe scaling laws for LLMs which were initially explored by the Kaplan et al.24 study continue \\nto hold today. Language models have been consistently increasing in size and this has been \\na direct contributor to the vast improvement in these models’ quality and accuracy over the \\nlast few years. As increasing the number of parameters has improved the quality of LLMs it \\nhas also increased the computational resources needed to run them. Numerous approaches \\nhave been used to try and improve the efficiency of LLMs for different tasks as developers \\nare incentivized to reduce cost and latency for model users. Balancing the expense of \\nserving a model in terms of time, money, energy is known as the cost-performance tradeoff \\nand often needs adjusting for particular use cases.\\nTwo of the main resources used by LLMs are memory and computation. Techniques for \\nimproving the efficiency or speed of inference focus primarily on these resources. The \\nspeed of the connection between memory and compute is also critical, but usually hardware \\nconstrained.  As LLMs have grown in size 1000x from millions to billions of parameters. \\nAdditional parameters increase both the size of memory required to hold the model and \\ncomputations needed to produce the model results.\\nWith LLMs being increasingly adopted for large-scale and low-latency use cases, finding \\nways to optimize their inference performance has become a priority and an active research \\ntopic with significant advancements. We will explore a number of methods and a few \\ntradeoffs for accelerating inference.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 57}, page_content='Foundational Large Language Models & Text Generation\\n58\\nFebruary 2025\\nTrade offs\\nMany of the high yielding inference optimisation methods mandate trading off a number of \\nfactors, this can be tweaked on a case-by-case basis allowing for tailored approaches to \\ndifferent inference use cases and requirements. A number of the optimization methods we \\nwill discuss later fall somewhere on the spectrum of these tradeoffs. \\nTrading off one factor against the other (e.g. latency vs quality or cost) doesn’t mean that \\nwe’re completely sacrificing that factor, it just means that we’re accepting what might be \\na marginal degradation in quality, latency or cost for the benefit of substantially improving \\nanother factor.\\nThe Quality vs Latency/Cost Tradeoff\\nIt is possible to improve the speed and cost of inference significantly through accepting \\nwhat might be marginal to negligible drops in the model’s accuracy. One  example of this \\nis using a smaller model to perform the task. Another example is quantisation where we \\ndecrease the precision of the model’s parameters thereby leading to faster and less memory \\nintensive calculations.\\nOne important distinction when approaching this trade-off is between the theoretical \\npossibility of a quality loss versus the practical capability of the model to perform the desired \\ntask. This is use case specific and exploring it will often lead to significant speedups without \\nsacrificing quality in a meaningful or noticeable way. For example, if the task we want the \\nmodel to perform is simple, then a smaller model or a quantised one will likely be able to \\nperform this task well. Reduction in parametric capacity or precision does not automatically \\nmean that the model is less capable at that specific task.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 58}, page_content='Foundational Large Language Models & Text Generation\\n59\\nFebruary 2025\\nThe Latency vs Cost Tradeoff\\nAnother name for this tradeoff is the latency vs throughput tradeoff. Where throughput refers \\nto the system’s ability at handling multiple requests efficiently. Better throughput on the same \\nhardware means that our LLM inference cost is reduced, and vice versa.\\nMuch like traditional software systems, there are often multiple opportunities to tradeoff \\nlatency against the cost of LLM inference. This is an important tradeoff since LLM inference \\ntends to be the slowest and most expensive component in the entire stack; balancing latency \\nand cost intentionally is key to making sure we tailor LLM performance to the product or use \\ncase it’s being used in. An example would be bulk inference use cases (e.g. offline labeling) \\nwhere cost can be a more important factor than the latency of any particular request. On the \\nother hand, an LLM chatbot product will place much higher importance on request latency.\\nNow that we’ve covered some of the important tradeoffs to consider when optimizing \\ninference, let’s examine some of the most effective inference acceleration techniques. As \\ndiscussed in the tradeoffs section, some optimization techniques can have an impact on the \\nmodel’s output. Therefore we will split the methods into two types: output-approximating \\nand output-preserving.\\nAs of this writing, Gemini 2.0 Flash Thinking offers an unparalleled balance of quality, as \\nmeasured by its ELO score, and affordability, with a cost per million tokens that is ten times \\nlower than comparable models; its position on a quality-versus-cost graph (where further \\nup and to the right is superior) demonstrates its transformative development. Moreover, the \\npicture highlights the rapid advancements in reasoning and thinking capabilities within the AI \\nfield, with a 27-fold improvement observed in the last three months.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 59}, page_content='Foundational Large Language Models & Text Generation\\n60\\nFebruary 2025\\nOutput-approximating methods\\nQuantization\\nLLMs are fundamentally composed of multiple numerical matrices (a.k.a the model weights). \\nDuring inference, matrix operations are then applied to these model weights to produce \\nnumerical outputs (a.k.a activations). Quantization is the process of decreasing the numerical \\nprecision in which weights and activations are stored, transferred and operated upon. The \\ndefault representation of weights and activations is usually 32 bits floating numbers, with \\nquantization we can drop the precision to 8 or even 4 bit integers. \\nQuantization has multiple performance benefits, it reduces the memory footprint of \\nthe model, allowing to fit larger models on the same hardware, it also reduces the \\ncommunication overhead of weights and activations within one chip and across chips in \\na distributed inference setup- therefore speeding up inference as communication is a \\nmajor contributor to latency. In addition, decreasing the precision of weights/activations \\ncan enable faster arithmetic operations on these models as some accelerator hardware \\n(e.g. TPUs/GPUs) natively supports faster matrix multiplication operations for some lower \\nprecision representations.\\nQuantization’s impact on quality can be very mild to non-existent depending on the use \\ncase and model.  Further, in cases where quantisation might introduce a quality regression, \\nthat regression can be small compared to the performance gain, therefore allowing for an \\neffective Quality vs Latency/Cost Tradeoff. For example, Benoit Jacob et al.55 reported a 2X \\nspeed-up for a 2% drop in accuracy for the FaceDetection task on MobileNet SSD.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 60}, page_content='Foundational Large Language Models & Text Generation\\n61\\nFebruary 2025\\nQuantization can be either applied as an inference-only operation, or it can be incorporated \\ninto the training (referred to as Quantisation Aware Training QAT). QAT is generally \\nconsidered to be a more resilient approach as the model is able to recover some of the \\nquantisation-related quality losses during training. To make sure we get the best cost/quality \\ntradeoff, we tweak the quantization strategy (e.g. select different precisions for weights \\nvs activations) and the granularity in which we apply quantisation to Tensors (e.g. channel \\nor group-wise58).\\nDistillation\\nUsing a smaller model to perform a task is one of the most efficient inference optimization \\ntechniques, however, smaller models can demonstrate significant regressions on quality \\ncompared to their larger counterparts.\\nDistillation is a set of training techniques that targets improving the quality of a smaller model \\n(the student) using a larger model (the teacher). This method can be effective because larger \\nmodels outperform smaller ones even if both are trained on the same data, mainly due to \\nparametric capacity and training dynamics. The gap in performance continues as the training \\ndataset grows as illustrated by Figure 9.\\nIt is worth noticing that even at low volumes of training data, large models can already \\ndemonstrate better performance than the correspondingly trained smaller models, this fact \\nleads us to the first variant of distillation which is referred to as data distillation or model \\ncompression.56 We use a large model which was trained on the data we have to generate \\nmore synthetic data to train the smaller student model, the increase in data volume will help \\nmove the the student further along the quality line compared to only training on the original \\ndata. Synthetic data needs to be approached carefully as it needs to be of high quality and \\ncan lead to negative effects otherwise.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 61}, page_content='Foundational Large Language Models & Text Generation\\n62\\nFebruary 2025\\nFigure 9. An illustration of the performance of models of various sizes as a function of the training \\ndataset’s size\\nOther distillation techniques attempt to bring the student model closer to the teacher \\non a more granular level than just synthetic data generation. One prominent technique is \\nknowledge distillation57, in this approach we attempt to align the output token distribution \\nof the student model to that of the teacher’s, this can be much more sample efficient than \\ndata distillation. On-policy distillation59 is another technique that leverages feedback from \\nthe teacher model on each sequence generated by the student in a reinforcement learning \\nsetup. \\nOutput-preserving methods\\nThese methods are guaranteed to be quality neutral, they cause no changes to the model \\noutput which often makes them obvious first steps to optimize inference before facing the \\nmore nuanced tradeoffs of the approximating methods'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 62}, page_content='Foundational Large Language Models & Text Generation\\n63\\nFebruary 2025\\nFlash Attention\\nScaled Dot-product Attention, which is the predominant attention mechanism in the \\ntransformer architecture, is a quadratic operation on the input length. Optimizing the self-\\nattention calculation can bring significant latency and cost wins.\\nFlash Attention, introduced in by Tri Dao et al.62, optimizes the attention calculation by making \\nthe attention algorithm IO Aware, particularly trying to minimize the amount of data we move \\nbetween the slow HBM (high bandwidth memory) to the faster memory tier (SRAM/VMEM) in \\nTPUs and GPUs. When calculating attention, the order of operations is changed and multiple \\nlayers are fused so we can utilize the faster memory tiers as efficiently as possible.\\nFlash Attention is an exact algorithm, it maintains the numerical output of the attention \\ncomputation and can yield significant latency benefits due to reducing the IO overhead, Tri \\nDao et al.62 showed 2-4X latency improvements in the attention computation.\\nPrefix Caching\\nOne of the most compute intensive, and thus slowest, operations in LLM inference is \\ncalculating the attention key and value scores (a.k.a KV) for the input we’re passing to the \\nLLM, this operation is often referred to as prefill. The final output of prefill is what is termed \\nKV Cache which includes the attention key and value scores for each layer of the transformer \\nfor the entire input. This cache is vital during the decoding phase which produces the output \\ntokens, the KV cache allows us to avoid recalculating attention scores for the input on each \\nautoregressive decode step.\\nPrefix Caching refers to the process of caching the KV Cache itself between subsequent \\ninference requests in order to reduce the latency and cost of the prefill operation. The way \\nthe self-attention mechanism works makes reusing KV caches possible because tokens will'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 63}, page_content='Foundational Large Language Models & Text Generation\\n64\\nFebruary 2025\\nonly pay attention to tokens that came before them in the sequence. If there’s new input \\nbeing appended to input that the model has seen before, then we can potentially avoid \\nrecalculating the prefill for the older input.\\nFigure 10. An illustration of Prefix Caching in a chat scenario\\nFigure 10 illustrates how prefix caching works in a multi-turn scenario with a document \\nupload. On the first user turn, the prefill operation has to process the entire document \\ntherefor taking 500ms, the resulting KV cache is then stored so that on the second user turn, \\nwe can retrieve the cache directly from storage and avoid recomputing it for the long doc, \\ntherefore saving substantial amounts of compute and latency.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 64}, page_content='Foundational Large Language Models & Text Generation\\n65\\nFebruary 2025\\nPrefix caches can be stored either in memory or on disk and fetched on-demand. One \\nimportant consideration is making sure that the input structure/schema remains prefix-\\ncaching friendly, we should avoid changing the prefix in subsequent requests as that will \\ninvalidate the cache for all the tokens that follow For example, putting a fresh timestamp at \\nthe very beginning of each request will invalidate the cache completely as every subsequent \\nrequest will have a new prefix.\\nMany LLM use cases lend themselves naturally to prefix caching. For example, LLM Chatbots \\nwhere users will have a multi-turn conversation that can span 10s of 1000s of tokens and \\nwe can avoid recalculating the KV cache for the previous parts of the conversation. Large \\ndocument/code uploads is another use case where the artifact the user uploads will remain \\nunchanged from one request to the next. All that’s changing are the questions the user is \\nasking, so caching the KV cache for the document (especially for larger artifacts) can result \\nin significant latency and cost savings.\\nPrefix caching is available as a service called Context Caching on Google AI studio52 and  \\nVertex AI on Google Cloud53.\\nSpeculative Decoding\\nThe first phase of LLM inference, known as prefill, is compute bound due large matrix \\noperations on many tokens occurring in parallel. The second phase, known as decode, is \\ngenerally memory bound as tokens are auto-regressively decoded one at a time. \\nIt is not easy to naively use additional parallel compute capacity to speed up decode \\ngiven the  need to wait for the current token to be produced before we can calculate what \\nthe next token should be (as per the self-attention mechanism), the decode process is \\ninherently serial.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 65}, page_content='Foundational Large Language Models & Text Generation\\n66\\nFebruary 2025\\nSpeculative decoding (Leviathan at al.63) aims to overcome this limitation in decode by finding \\na way to utilize the spare compute capacity to make each decode step faster. The main idea \\nis to use a much smaller secondary model (often referred to as the drafter) to run ahead of \\nthe main model and predict more tokens. (e.g. 4 tokens ahead). This will happen very quickly \\nas the drafter is much faster and smaller than the main model. We then use the main model to \\nverify the hypotheses of the drafter in parallel for each of the 4 steps (i.e. the first token, the \\nfirst two tokens, the first 3 tokens and finally all 4 tokens), and we then select the accepted \\nhypothesis with the maximum number of tokens. For example:\\nFigure 11. An illustration of speculative decoding over 3 tokens\\nNote that the 3 main model steps run in parallel. And because we are not compute bound in \\ndecode, we can use the spare capacity to get much better decode latencies. In the example \\nabove, let’s say a single main model step needs 10ms, while the drafter needs 1ms. Without \\nspeculative decoding, we need 3 * 10ms = 30ms to produce the response, with speculative \\ndecoding, there’s only one main model step on the critical path due to parallelization, so we \\nneed 3 * 1ms + 10ms = 13ms. A significant latency improvement. This technique is completely'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 66}, page_content='Foundational Large Language Models & Text Generation\\n67\\nFebruary 2025\\nquality neutral, the main model will reject any tokens that it wouldn’t have predicted itself \\nin the first place, so the only thing speculative decoding does is run ahead and present \\nhypotheses that the main model can accept or reject in parallel.\\nOne important condition for speculative decoding to work effectively is that the drafter model \\nhas good levels of alignment with the main model, otherwise we won’t be able to accept any \\nof the tokens. So investing in the training quality of the drafter model is worthwhile to get \\nbetter latencies.\\nNow that we have seen some methods to make LLM generate their responses faster, let’s \\nlook at some examples of how these models can be applied to various tasks to get an idea \\nhow to use them.\\nBatching and Parallelization\\nMost of the optimization techniques we’ve discussed so far are specific to Machine Learning \\nand Transformer architecture in particular. However, much like any software system, there \\nare opportunities to improve throughput and latency through a combination of 1) batching \\nless compute-intensive operations (i.e. we can run multiple requests on the same hardware \\nsimultaneously to better utilize the spare compute) and 2) parallelizing the more compute-\\nintensive parts of the computations (i.e. we can divide the computation and split it amongst \\nmore hardware instances to get more compute capacity and therefore better latencies\\nBatching in LLMs is most useful on the decode side - as we explained in the Speculative \\nDecoding section, decode is not compute-bound and therefore there’s an opportunity \\nto batch more requests. We need to be careful that we batch computations in a way that \\nenables utilization of the spare capacity which is possible to do on accelerators (e.g. TPUs \\nand GPUs). We also need to make sure we remain within the memory limits, as decode is a'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 67}, page_content='Foundational Large Language Models & Text Generation\\n68\\nFebruary 2025\\nmemory intensive operations, batching more requests will put more pressure on the free \\nmemory available. Batching has become an important component in most high-throughput \\nLLM inference setups.\\nParallelization is also a widely used technique given the variety of opportunities in \\ntransformers for horizontal scaling across more hardware instances. There are multiple \\nparallelism techniques across the model input (Sequence parallelism) the model layers \\n(Pipeline parallelism), and within a single layer (Tensor parallelism). One of the most important \\nconsiderations for parallelism is the cost of communication and synchronization between \\nthe different shards that we distribute to other machines. Communication is a significant \\noverhead and can erode the benefits of adding more computational capacity if we’re not \\ncareful about which parallelization strategy to use. On the other hand, selecting the right \\nstrategy to balance the need for additional compute and the communication cost can yield \\nsignificant latency wins.\\nNow that we have seen some methods to make LLM generate their responses faster, let’s \\nlook at some examples of how these models can be applied to various tasks to get an idea \\nhow to use them.\\nApplications\\nLarge language models are revolutionizing the way we interact with and process information. \\nWith their unprecedented ability to understand context and generate content, they’re \\ntransforming numerous applications in the worlds of text, code, images, audio and video. \\nHere we collected a few examples of application areas, but the reader should keep in mind \\nthat this is not a comprehensive list and that many new ideas are emerging continuously \\nabout how to best utilize the capabilities of these new tools. For more information about \\noptimally building and deploying functioning applications based on the following mentioned \\nuse cases, refer to the subsequent whitepapers.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 68}, page_content='Foundational Large Language Models & Text Generation\\n69\\nFebruary 2025\\nIt is also very simple to generate text-based responses for your use case using either \\nthe Google Cloud Vertex AI SDK or the Developer focused AI studio. Snippet 3 shows \\ncode examples from these SDKs to generate responses to text prompts using the Gemini \\nmodel. Note that the multimodal aspects of Gemini are covered in their respective \\ndedicated whitepapers.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 69}, page_content='Foundational Large Language Models & Text Generation\\n70\\nFebruary 2025\\nPython\\n%pip install --upgrade --quiet google-genai\\nimport sys \\nif “google.colab” in sys.modules:\\n    from google.colab import auth \\n\\t\\nauth.authenticate_user() \\nfrom IPython.display import HTML, Markdown, display\\nfrom google import genai\\nfrom google.genai.types import (\\n    FunctionDeclaration,\\n    GenerateContentConfig,\\n    GoogleSearch,\\n    HarmBlockThreshold,\\n    HarmCategory,\\n    MediaResolution,\\n    Part,\\n    Retrieval,\\n    SafetySetting,\\n    Tool,\\n    ToolCodeExecution,\\n    VertexAISearch,\\n)\\nimport os\\nPROJECT_ID = “[your-project-id]”  # @param {type: “string”, placeholder: “[your-project-\\nid]”, isTemplate: true}\\nif not PROJECT_ID or PROJECT_ID == “[your-project-id]”:\\n    PROJECT_ID = str(os.environ.get(“GOOGLE_CLOUD_PROJECT”)) \\nLOCATION = os.environ.get(“GOOGLE_CLOUD_REGION”, “us-central1”)  \\nclient = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION) \\nMODEL_ID = “gemini-2.0-flash-001”  # @param {type: “string”} \\nresponse = client.models.generate_content(\\n    model=MODEL_ID, contents=”What’s the largest planet in our solar system?” \\n)\\ndisplay(Markdown(response.text))\\nSnippet 2. Using Vertex AI and Google AI studio SDKs for unimodal text generation'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 70}, page_content='Foundational Large Language Models & Text Generation\\n71\\nFebruary 2025\\nCode and mathematics\\nGenerative models can comprehend and generate code and algorithms to supercharge \\ndevelopers by assisting them across many application areas. Some of the popular use cases \\nfor code include:\\n•\\t Code generation: LLMs can be prompted in natural language to generate code in a \\nspecific programming language to perform certain operations. The output can be used as \\na draft.\\n•\\t Code completion: LLMS can proactively suggest useful code as the user types it. This \\ncan save developers time and improve code quality.\\n•\\t Code refactoring and debugging: LLMs can help reduce technical debt by refactoring \\nand debugging code to improve quality, efficiency and correctness.\\n•\\t Code translation: LLMs can significantly help developer time and effort by helping to \\nconvert code from one programming language to another. For example, an LLM might \\nconvert Python code to Java.\\n•\\t Test case generation: LLMs can be prompted to generate unit tests for a provided \\ncodebase which saves considerable time and reduces errors.\\n•\\t Code documentation and understanding: LLMs can be used in a conversational manner \\nto engage in a natural language chat to help you understand a codebase. They can also \\ngenerate appropriate comments, understand copyright status, and create release notes.\\nRecently, a number of exciting advancements have been made in the space of competitive \\ncoding and mathematics. AlphaCode 2,64 combines Gemini’s reasoning capabilities with \\nsearch and the use of tools to solve competitive coding problems. It receives as input a \\ndescription of a problem to solve, and outputs a code solution that solves the problem. It'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 71}, page_content='Foundational Large Language Models & Text Generation\\n72\\nFebruary 2025\\nnow ranks among the top 15% competitive coders on the popular Codeforces competitive \\ncoding platform. FunSearch65 uses an evolutionary procedure which is based on pairing \\na pre-trained LLM with a systematic evaluator. It solved the cap set problem66, an open \\nproblem in mathematics, and also discovered more efficient bin-packing algorithms which \\nare used in many applications such as making data centers more efficient. Another recent \\napproach called AlphaGeometry tackles the problem of finding proofs for complex geometric \\ntheorems. It comprises a neuro-symbolic system made up of a neural language model and \\na symbolic deduction engine. AlphaGeometry managed to solve 25 out of 30 Olympiad \\ngeometry problems, where the average human gold medalist scores on average 25.9. 67\\nMachine translation\\nLLMs are capable of generating fluid, high-quality and contextually accurate translations. \\nThis is possible due to the LLM’s deep understanding of linguistic nuances, idioms, and \\ncontext. The following are some possible real world use cases:\\n•\\t Instant messaging apps: In messaging platforms, LLMs can provide on-the-fly \\ntranslations that feel natural. Unlike previous algorithms that might translate word-\\nfor-word, LLMs understand slang, colloquialisms, and regional differences, enhancing \\ncross-language communication.\\n•\\t E-commerce: On global platforms like AliExpress, product descriptions are automatically \\ntranslated. LLMs help with ensuring cultural nuances and idiomatic expressions in product \\ndetails are appropriately translated, leading to fewer misunderstandings.\\n•\\t Travel apps: In apps like Google Translate, travelers get real-time spoken translations. \\nWith LLMs, the translated conversations are smoother, making interactions in foreign \\ncountries more effortless.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 72}, page_content='Foundational Large Language Models & Text Generation\\n73\\nFebruary 2025\\nText summarization\\nText summarization is a core capability of many of the LLMs mentioned in this whitepaper. \\nThere are a number of natural potential use cases which include:\\n•\\t News aggregators: LLMs could craft summaries that capture not only the main \\nevents but also the sentiment and tone of the article, providing readers with a more \\nholistic understanding.\\n•\\t Research databases: LLMs could help researchers generate abstracts that encapsulate \\nthe core findings and implications of scientific papers.\\n•\\t Chat management: In platforms like Google Chat, LLM-based systems could generate \\nthread summaries that capture the urgency and tone, aiding users in prioritizing \\ntheir responses.\\nQuestion-answering\\nThe older generation of QA systems often worked by keyword matching, frequently missing \\nout on the contextual depth of user queries. LLMs, however, dive deep into context. They can \\ninfer user intent, traverse vast information banks, and provide answers that are contextually \\nrich and precise. Some of the examples where this could be used include:\\n•\\t Virtual assistants: LLMs can offer detailed explanations of a weather forecast \\nconsidering the user’s location, time of year, and recent weather trends.\\n•\\t Customer support: In business platforms, LLM-based bots could provide answers that \\ntake into account the user’s purchase history, past queries, and potential issues, offering \\npersonalized assistance.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 73}, page_content='Foundational Large Language Models & Text Generation\\n74\\nFebruary 2025\\n•\\t Academic platforms: On academic platforms like Wolfram Alpha, LLMs could cater to \\nuser queries by understanding the depth and context of academic questions, offering \\nanswers that suit everyone from a high school student to a postgraduate researcher.\\nThe quality of the generated answers, as well as the corresponding citations and sources \\ncan be significantly improved by using advanced search systems (such as those based on \\nRetrieval Augmented Generation (RAG) architectures) to expand the prompt with relevant \\ninformation, as well as post-hoc grounding after the response has been generated. Clear \\ninstructions, roles of what should and should not be used to answer the question, and \\nadvanced prompt engineering approaches (such as chain of thought and search/RAG \\narchitectures), combined with a lower temperature value amongst other things can also \\nhelp greatly.\\nChatbots\\nEarlier chatbots followed scripted pathways, leading to ‘mechanical’ conversations. LLMs \\ntransform this space by offering dynamic, human-like interactions. They can analyze \\nsentiment, context, and even humor, making digital conversations feel more authentic. Some \\nexamples of where this can be used include:\\n•\\t Customer service: A chatbot on retail platforms like Zara could not only answer product-\\nrelated queries but also offer fashion advice based on current trends.\\n•\\t Entertainment: On Media LLM-driven chatbots could engage with users dynamically, \\nreacting to live events in the stream and moderating chats with contextual understanding.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 74}, page_content='Foundational Large Language Models & Text Generation\\n75\\nFebruary 2025\\nContent generation\\nText generation isn’t new, but what LLMs bring to the table is the unprecedented ability \\nto generate human-like text that’s contextually relevant and rich in detail. Earlier models \\nwould often lose context or coherence over longer passages. LLMs, with their vast \\nknowledge and nuanced understanding, can craft text spanning various styles, tones, and \\ncomplexities, mixing factuality with creativity (depending on the context) effectively bridging \\nthe gap between machine-generated and human-written content. The following are some \\nreal-world examples:\\n•\\t Content creation: Platforms could utilize LLMs to help marketers develop advertisements. \\nInstead of generic content, the LLMs could generate creative, targeted, and \\naudience-specific messages.\\n•\\t Scriptwriting: LLMs could potentially assist with producing scripts for movies or TV \\nshows. Writers could input themes or plot points, and the model can suggest dialogues or \\nscene descriptions, enhancing the creative process.\\nText generation is a wide task encompassing a variety of use cases that might range from \\nthose where correctness of the generated output is more or less important than its creativity/\\ndiversity of the language. The sampling methods and parameters like temperature should be \\ntuned accordingly. For more information, see the prompt engineering and architecting for \\nLLM applications whitepapers.\\nNatural language inference\\nNatural language inference (NLI) is the task of determining whether a given textual \\nhypothesis can be logically inferred from a textual premise.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 75}, page_content='Foundational Large Language Models & Text Generation\\n76\\nFebruary 2025\\nTraditional models struggled with nuanced relationships or those that require a deeper \\nunderstanding of context. LLMs, with their intricate grasp of semantics and context, excel \\nat tasks like these, bringing accuracy levels close to human performance. The following are \\nsome real-world examples:\\n•\\t Sentiment analysis: Businesses could utilize LLMs to infer customer sentiment from \\nproduct reviews. Instead of just basic positive or negative tags, they could extract \\nnuanced emotions like ‘satisfaction,’ ‘disappointment,’ or ‘elation’.\\n•\\t Legal document review: Law firms could employ LLMs to infer implications \\nand intentions in contracts, ensuring there are no contradictions or potentially \\nproblematic clauses.\\n•\\t Medical diagnoses: By analyzing patient descriptions and histories, LLMs could assist \\ndoctors in inferring potential diagnoses or health risks, ensuring early intervention.\\nThe whitepapers on domain specific LLMs, prompt engineering, and architecting for LLM \\napplications give further insight into these use cases.\\nText classification\\nText classification involves categorizing text into predefined groups. While traditional \\nalgorithms were efficient, they often struggled with ambiguous or overlapping categories. \\nLLMs, given their deep understanding of context, can classify text with higher precision, even \\nwhen faced with subtle distinctions. Some examples of this include:\\n•\\t Spam detection: Email services could utilize LLMs to classify emails as spam or \\nlegitimate. Instead of just keyword-based detection, the models understand the context \\nand intent, potentially reducing false positives.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 76}, page_content='Foundational Large Language Models & Text Generation\\n77\\nFebruary 2025\\n•\\t News categorization: News platforms could employ LLMs to categorize articles into \\ntopics like ‘technology,’ ‘politics,’ or ‘sports,’ even when articles blur the boundaries \\nbetween categories.\\n•\\t Customer feedback sorting: Businesses could analyze customer feedback through \\nLLMs to categorize them into areas like ‘product design,’ ‘customer service,’ or ‘pricing,’ \\nensuring targeted responses.\\n•\\t Evaluating LLMs as autorater: LLMs could be used to rate, compare and rank the \\ngenerated outputs of other LLMs as well.\\nText analysis\\nLLMs excel at deep text analysis – extracting patterns, understanding themes, and gleaning \\ninsights from vast textual datasets. Where traditional tools would scratch the surface, LLMs \\ndelve deep, offering rich and actionable insights. Some potential real-world examples are:\\n•\\t Market research: Companies could leverage LLMs to analyze consumer conversations on \\nsocial media, extracting trends, preferences, and emerging needs.\\n•\\t Literary analysis: Academics could employ LLMs to understand themes, motifs, and \\ncharacter developments in literary works, offering fresh perspectives on classic and \\ncontemporary literature.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 77}, page_content='Foundational Large Language Models & Text Generation\\n78\\nFebruary 2025\\nMultimodal applications\\nMultimodal LLMs, capable of processing and generating text, images, audio, and video, have \\nopened up a new frontier in AI, offering a range of exciting and innovative applications across \\nvarious sectors. The following are some examples: \\nCreative content generation:\\n•\\t Storytelling: An AI system could watch an image or video and spin a captivating narrative, \\nintegrating details from the visual with its knowledge base.\\n•\\t Advertising and marketing: Generating targeted and emotionally resonant \\nadvertisements based on product photos or videos.\\nEducation and accessibility:\\n•\\t Personalized learning: Tailoring educational materials to individual learning styles by \\ncombining text with interactive visual and audio elements.\\n•\\t Assistive technology: Multimodal LLMs could power tools that describe images, videos, \\nand audio for visually or hearing-impaired individuals.\\nBusiness and industry:\\n•\\t Document understanding and summarization: Automatically extracting key information \\nfrom complex documents, combining text and visuals like invoices and contracts.\\n•\\t Customer service: Multimodal chatbots can understand and respond to customer \\nqueries combining text and images, offering a richer and more personalized experience. \\nScience and research:'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 78}, page_content='Foundational Large Language Models & Text Generation\\n79\\nFebruary 2025\\n•\\t Medical diagnosis: Analyzing medical scans and reports together, identifying potential \\nissues and providing insights for doctors.\\n•\\t Bioinformatics and drug discovery: Integrating knowledge from diverse data sources \\nlike medical images, protein structures, and research papers to accelerate research.\\nThese examples are just the tip of the iceberg. As research progresses, the applications \\nof multimodal LLMs are only expected to grow, transforming our daily lives in diverse and \\nprofound ways. Multimodal LLMs also benefit greatly from the existing methodologies of \\nUnimodal LLMs ( i.e., text based LLMs).\\nLLMs, thanks to their ability to understand and process language, are reshaping how we \\ninteract with, generate, and analyze text across diverse sectors. As they continue to evolve, \\ntheir applications will only grow, boosting the ability for machines and humans to have rich \\nnatural language interactions.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 79}, page_content='Foundational Large Language Models & Text Generation\\n80\\nFebruary 2025\\nSummary\\nIn this whitepaper we have discussed the basics of transformers, upon which all modern-day \\nLLMs are based. We detailed the evolution of the various LLM model architectures and their \\ncomponents. We’ve also seen the various methodologies you can use to train and fine-tune \\nmodels efficiently and effectively. We briefly discussed prompt engineering and sampling \\ntechniques that greatly influence the output of an LLM, and also touched on possible \\napplications of this technology. There are a number of key takeaways to keep in mind:\\n•\\t The transformer architecture is the basis for all modern-day LLMs. Across the various \\narchitectures mentioned in this whitepaper we see that it’s important not only to add more \\nparameters to the model, but the composition of the dataset is equally important. \\n•\\t The order and strategies used for fine-tuning is important and may include multiple steps \\nsuch as Instruction Tuning, Safety Tuning, etc. Supervised Fine Tuning (SFT) is important \\nin capturing the essence of a task. RLHF, and potentially RLAIF, can be used to shift the \\ndistribution from the pretraining distribution to a more desired one through the power of \\nthe reward function, that can reward desirable behaviors and penalize undesirable ones.\\n•\\t Making inference from neural models efficient is an important problem and an active \\nfield of research. Many methods exist to reduce serving costs and latency with minimal \\nimpact to model performance, and some exact acceleration methods guarantee identical \\nmodel outputs.\\n•\\t Large language models can be used for a variety of tasks including summarization, \\ntranslation, question answering, chat, code generation, and many more. You can \\ncreate your own tasks using the Vertex and Makersuite text generation services which \\nleverage Google’s latest language models. After the model has been trained and tuned, \\nit is important to experiment with engineering prompts. You should use the technique \\nmost appropriate for the task-at-hand because LLMs can be sensitive to prompts k.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 80}, page_content='Foundational Large Language Models & Text Generation\\n81\\nFebruary 2025\\nFurthermore, it is also possible to enhance task specific performance or creativity and \\ndiversity by tweaking the parameters corresponding to sampling techniques such as \\nTop-K, Top-P, and Max decoding steps to find the optimum mix of correctness, diversity, \\nand creativity required for the task at hand.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 81}, page_content='Foundational Large Language Models & Text Generation\\n82\\nFebruary 2025\\nEndnotes\\n1.\\t Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin,\\xa0I., 2017, Attention is \\nall you need. Advances in Neural Information Processing Systems, 30.\\n2.\\t Wikipedia, 2024, Word n-gram language model. Available at:  \\nhttps://en.wikipedia.org/wiki/Word_n-gram_language_model.\\n3.\\t Sutskever, I., Vinyals, O., & Le, Q. V., 2014, Sequence to sequence learning with neural networks. Advances in \\nNeural Information Processing Systems, 27.\\n4.\\t Gu, A., Goel, K., & Ré, C., 2021, Efficiently modeling long sequences with structured state spaces.  \\narXiv preprint arXiv:2111.00396.\\n5.\\t Jalammar, J. (n.d.). The illustrated transformer. Available at:  \\nhttps://jalammar.github.io/illustrated-transformer/.\\n6.\\t Ba, J. L., Kiros, J. R., & Hinton, G. E., 2016, Layer normalization.  \\narXiv preprint arXiv:1607.06450.\\n7.\\t He, K., Zhang, X., Ren, S., & Sun, J., 2016, Deep residual learning for image recognition. Proceedings of the \\nIEEE Conference on Computer Vision and Pattern Recognition.\\n8.\\t HuggingFace., 2024, Byte Pair Encoding. Available at:  \\nhttps://huggingface.co/learn/nlp-course/chapter6/5?fw=pt.\\n9.\\t Kudo, T., & Richardson, J., 2018, Sentencepiece: A simple and language independent subword tokenizer and \\ndetokenizer for neural text processing. arXiv preprint arXiv:1808.06226.\\n10.\\tHuggingFace, 2024, Unigram tokenization. Available at:  \\nhttps://huggingface.co/learn/nlp-course/chapter6/7?fw=pt.\\n11.\\t Goodfellow et. al., 2016, Deep Learning. MIT Press. Available at: http://www.deeplearningbook.org.\\n12.\\t Radford, Alec et al., 2019, Language models are unsupervised multitask learners.\\n13.\\t Brown, Tom, et al., 2020, Language models are few-shot learners. Advances in Neural Information \\nProcessing Systems, 33, 1877-1901.\\n14.\\t Devlin, Jacob, et al., 2018, BERT: Pre-training of deep bidirectional transformers for language understanding. \\narXiv preprint arXiv:1810.04805.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 82}, page_content='Foundational Large Language Models & Text Generation\\n83\\nFebruary 2025\\n15.\\t Radford, A., & Narasimhan, K., 2018, Improving language understanding by generative pre-training.\\n16.\\t Dai, A., & Le, Q., 2015, Semi-supervised sequence learning. Advances in Neural Information \\nProcessing Systems.\\n17.\\t Ouyang, Long, et al., 2022, Training language models to follow instructions with human feedback. Advances \\nin Neural Information Processing Systems, 35, 27730-27744.-27744.\\n18.\\t OpenAI., 2023, GPT-3.5. Available at: https://platform.openai.com/docs/models/gpt-3-5.\\n19.\\t OpenAI., 2023, GPT-4 Technical Report. Available at: https://arxiv.org/abs/2303.08774.\\n20.\\tThoppilan, Romal, et al., 2022, Lamda: Language models for dialog applications. \\narXiv\\xa0preprint arXiv:2201.08239.\\n21.\\t Llama 3.2: Revolutionizing edge AI and vision with open, customizable models. Available \\nat: https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/.\\n22.\\tRae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, F., ... & Irving, G., 2021, Scaling language \\nmodels: Methods, analysis & insights from training Gopher. Available at: https://arxiv.org/pdf/2112.11446.pdf.\\n23.\\tDu, N., He, H., Dai, Z., Mccarthy, J., Patwary, M. A., & Zhou, L., 2022, GLAM: Efficient scaling of language \\nmodels with mixture-of-experts. In International Conference on Machine Learning (pp. 2790-2800). PMLR.\\n24.\\tKaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., ... & Amodei, D., 2020, Scaling laws \\nfor neural language models. arXiv preprint arXiv:2001.08361.\\n25.\\tHoffmann, Jordan, et al., 2022, Training compute-optimal large language models. arXiv \\npreprint arXiv:2203.15556.\\n26.\\tShoeybi, Mohammad, et al., 2019, Megatron-LM: Training multi-billion parameter language models using \\nmodel parallelism. arXiv preprint arXiv:1909.08053.\\n27.\\t Muennighoff, N. et al., 2023, Scaling data-constrained language models. arXiv preprint arXiv:2305.16264.\\n28.\\tChowdhery, Aakanksha, et al., 2023, Palm: Scaling language modeling with pathways. Journal of Machine \\nLearning Research, 24(240), 1-113.\\n29.\\tWang, Alex, et al.,2019, SuperGLUE: A stickier benchmark for general-purpose language understanding \\nsystems. Advances in Neural Information Processing Systems, 32.\\n30.\\tAnil, Rohan, et al., 2023, Palm 2 technical report. arXiv preprint arXiv:2305.10403.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 83}, page_content='Foundational Large Language Models & Text Generation\\n84\\nFebruary 2025\\n31.\\t DeepMind, 2023, Gemini: A family of highly capable multimodal models. Available at:  \\nhttps://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf.\\n32.\\tDeepMind, 2024, Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. \\nAvailable at: https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf.\\n33.\\tGoogle Developers, 2024, Introducing PaLi-Gemma, Gemma 2, and an upgraded responsible AI toolkit. \\nAvailable at: https://developers.googleblog.com/en/gemma-family-and-toolkit-expansion-io-2024/.\\n34.\\tTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M., Lacroix, T., ... & Jegou, H., 2023, Llama 2: Open \\nfoundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.\\n35.\\tJiang, A. Q., 2024, Mixtral of experts. arXiv preprint arXiv:2401.04088.\\n36.\\tQwen, 2024, Introducing Qwen1.5. Available at: https://qwenlm.github.io/blog/qwen1.5/.\\n37.\\t Young, A., 2024, Yi: Open foundation models by 01.AI. arXiv preprint arXiv:2403.04652.\\n38.\\tGrok-1, 2024, Available at: https://github.com/xai-org/grok-1.\\n39.\\tDuan, Haodong, et al., 2023, BotChat: Evaluating LLMs’ capabilities of having multi-turn dialogues. \\narXiv\\xa0preprint arXiv:2310.13650.\\n40.\\tGoogle Cloud, 2024, Tune text models with reinforcement learning from human feedback. Available at:  \\nhttps://cloud.google.com/vertex-ai/generative-ai/docs/models/tune-text-models-rlhf.\\n41.\\t Bai, Yuntao, et al., 2022, Constitutional AI: Harmlessness from AI feedback. arXiv preprint arXiv:2212.08073.\\n42.\\tWikipedia, 2024, Likert scale. Available at: https://en.wikipedia.org/wiki/Likert_scale.\\n43.\\tSutton, R. S., & Barto, A. G., 2018, Reinforcement learning: An introduction. MIT Press.\\n44.\\tBai, Yuntao, et al, 2022, Constitutional AI: Harmlessness from AI feedback. arXiv preprint arXiv:2212.08073.\\n45.\\tRafailov, Rafael, et al., 2023, Direct preference optimization: Your language model is secretly a reward \\nmodel. arXiv preprint arXiv:2305.18290.\\n46.\\tHoulsby, Neil, et al., 2019, Parameter-efficient transfer learning for NLP. In International Conference on \\nMachine Learning (pp. 2790-2799). PMLR.\\n47.\\t Hu, Edward J., et al., 2021, LoRA: Low-rank adaptation of large language models. \\narXiv\\xa0preprint arXiv:2106.09685.\\n48.\\tDettmers, Tim, et al., 2023, QLoRA: Efficient finetuning of quantized LLMs. arXiv preprint arXiv:2305.14314.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 84}, page_content='Foundational Large Language Models & Text Generation\\n85\\nFebruary 2025\\n49.\\tLester, B., Al-Rfou, R., & Constant, N., 2021, The power of scale for parameter-efficient prompt tuning. arXiv \\npreprint arXiv:2104.08691.\\n50.\\tHuggingFace., 2020, How to generate text? Available at: https://huggingface.co/blog/how-to-generate.\\n51.\\t Google AI Studio Context caching. Available \\nat: https://ai.google.dev/gemini-api/docs/caching?lang=python.\\n52.\\tVertex AI Context caching overview. Available \\nat: https://cloud.google.com/vertex-ai/generative-ai/docs/context-cache/context-cache-overview.\\n53.\\tGu, A., Goel, K., & Ré, C., 2021, Efficiently modeling long sequences with structured state spaces.  \\nAvailable at: https://arxiv.org/abs/2111.00396.\\n54.\\tHubara et al., 2016, Quantized neural networks: Training neural networks with low precision weights and \\nactivations. Available at: https://arxiv.org/abs/1609.07061.\\n55.\\tBenoit Jacob et al., 2017, Quantization and training of neural networks for efficient integer-arithmetic-only \\ninference. Available at: https://arxiv.org/abs/1712.05877.\\n56.\\tBucila, C., Caruana, R., & Niculescu-Mizil, A., 2006, Model compression. Knowledge Discovery and Data \\nMining. Available at: https://www.cs.cornell.edu/~caruana/compression.kdd06.pdf.\\n57.\\t Hinton, G., Vinyals, O., & Dean, J., 2015, Distilling the knowledge in a neural network.  \\nAvailable at: https://arxiv.org/abs/1503.02531.\\n58.\\tZhang, L., Fei, W., Wu w., He Y., Lou Z., Zhou H., 2023, Dual Grained Quantisation: Efficient Finegrained \\nQuantisation for LLM. Available at: https://arxiv.org/abs/2310.04836.\\n59.\\tAgarwal, R., Vieillard, N., Zhou, Y., Stanczyk, P., Ramos, S., Geist, M., Bachem, O., 2024, On-\\nPolicy Distillation of Language Models: Learning from Self-Generated Mistakes. Available \\nat: https://arxiv.org/abs/2306.13649.\\n60.\\tShazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G., & Dean, J., 2017, Outrageously large neural \\nnetworks: The sparsely-gated mixture-of-experts layer. Available at: https://arxiv.org/abs/1701.06538.\\n61.\\t Schuster, T., Fried, D., & Jurafsky, D., 2022, Confident adaptive language modeling. Available at:  \\nhttps://arxiv.org/abs/2207.07061.\\n62.\\tTri Dao et al. “FlashAttention. Available at:  \\nhttps://arxiv.org/abs/2205.14135.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:39:19-06:00', 'source': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'file_path': 'c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\\\\PDFs\\\\whitepaper_Foundational Large Language models & text generation_v2.pdf', 'total_pages': 86, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-11-09T14:49:31+05:30', 'trapped': '', 'modDate': \"D:20251109144931+05'30'\", 'creationDate': \"D:20250317133919-06'00'\", 'page': 85}, page_content='Foundational Large Language Models & Text Generation\\n86\\nFebruary 2025\\n63.\\tLeviathan, Y., Ram, O., Desbordes, T., & Haussmann, E., 2022, Fast inference from transformers via \\nspeculative decoding. Available at: https://arxiv.org/abs/2211.17192.\\n64.\\tLi, Y., Humphreys, P., Sun, T., Carr, A., Cass, S., Hawkins, P., ... & Bortolussi, L., 2022, Competition-level code \\ngeneration with AlphaCode. Science, 378(1092-1097). DOI: 10.1126/science.abq1158.\\n65.\\tRomera-Paredes, B., Barekatain, M., Novikov, A., Novikov, A., Rashed, S., & Yang, J., 2023, Mathematical \\ndiscoveries from program search with large language models. Nature. DOI: 10.1038/s41586-023-06924-6.\\n66.\\tWikipedia., 2024, Cap set. Available at: https://en.wikipedia.org/wiki/Cap_set.\\n67.\\t Trinh, T. H., Wu, Y., & Le, Q. V. et al., 2024, Solving olympiad geometry without human demonstrations. Nature, \\n625, 476–482. DOI: 10.1038/s41586-023-06747-5.\\n68.\\tMikolov, T., Chen, K., Corrado, G., Dean, J., 2013, Efficient Estimation of Word Representations in Vector \\nSpace. Available at: https://arxiv.org/pdf/1301.3781.\\n69.\\tShi, L., Ma, C., Liang, W., Ma, W., Vosoughi, S., 2024, Judging the Judges: A Systematic Study of Position Bias \\nin LLM-as-a-Judge. Available at: https://arxiv.org/abs/2406.07791\\n70.\\tPandit, B., 2024, What Is Mixture of Experts (MoE)? How It Works, Use Cases & More. Available \\nat: https://www.datacamp.com/blog/mixture-of-experts-moe')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ingesting Pdfs\n",
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "\n",
    "## loading Pdfs\n",
    "## loading all the text files from the directory\n",
    "dir_loader = DirectoryLoader(\"c:\\\\Users\\\\gunas\\\\python_projects\\\\Projects_git\",\n",
    "                             glob = \"**\\\\*.pdf\", # pattern to match text files\n",
    "                             loader_cls = PyMuPDFLoader,# PDFLoader class to use\n",
    "                             show_progress = True\n",
    "                            )\n",
    "\n",
    "documents = dir_loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eccbc532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding and Vectore store DB\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings \n",
    "import uuid\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bde326c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18ee67f4904b4b4b94a29430a0c64761",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gunas\\anaconda3\\envs\\New_world\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\gunas\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e19157f28a6949f4b0d3aca161879c9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a81193a0fde4ca799a900cb9532f220",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ab7f87f0a984fbea5a4d98de80d2f74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0819e5754ce54d999e84134ed6c4a53f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd213281de874a8fb081e929ed9d6b12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6036291257594f949017d1d3912a5b98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b6d1c665a824806ad32d2edb754f58b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b57a28c7732c44cf9c26391fc98fddf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f366f11c0c524c4b887f9d46b74c8c60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03872998a8d1498f9c26bb3633252925",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully. Embedding dimension: 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x1fb749f5d60>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    \"\"\"Handles document embedding generation using SentenceTransformer\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize the embedding manager\n",
    "        \n",
    "        Args:\n",
    "            model_name: HuggingFace model name for sentence embeddings\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the SentenceTransformer model\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading embedding model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model loaded successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {self.model_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate embeddings for a list of texts\n",
    "        \n",
    "        Args:\n",
    "            texts: List of text strings to embed\n",
    "            \n",
    "        Returns:\n",
    "            numpy array of embeddings with shape (len(texts), embedding_dim)\n",
    "        \"\"\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "        \n",
    "        print(f\"Generating embeddings for {len(texts)} texts...\")\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        print(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "## initialize the embedding manager\n",
    "\n",
    "embedding_manager=EmbeddingManager()\n",
    "embedding_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64d25959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store initialized. Collection: pdf_documents\n",
      "Existing documents in collection: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x1fb74b2caa0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# VectorStore\n",
    "class VectorStore:\n",
    "    \"\"\"Manages document embeddings in a ChromaDB vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, collection_name: str = \"pdf_documents\", persist_directory: str = \"../data/vector_store\"):\n",
    "        \"\"\"\n",
    "        Initialize the vector store\n",
    "        \n",
    "        Args:\n",
    "            collection_name: Name of the ChromaDB collection\n",
    "            persist_directory: Directory to persist the vector store\n",
    "        \"\"\"\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        \"\"\"Initialize ChromaDB client and collection\"\"\"\n",
    "        try:\n",
    "            # Create persistent ChromaDB client\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "            \n",
    "            # Get or create collection\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\": \"PDF document embeddings for RAG\"}\n",
    "            )\n",
    "            print(f\"Vector store initialized. Collection: {self.collection_name}\")\n",
    "            print(f\"Existing documents in collection: {self.collection.count()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "        \"\"\"\n",
    "        Add documents and their embeddings to the vector store\n",
    "        \n",
    "        Args:\n",
    "            documents: List of LangChain documents\n",
    "            embeddings: Corresponding embeddings for the documents\n",
    "        \"\"\"\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents must match number of embeddings\")\n",
    "        \n",
    "        print(f\"Adding {len(documents)} documents to vector store...\")\n",
    "        \n",
    "        # Prepare data for ChromaDB\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text = []\n",
    "        embeddings_list = []\n",
    "        \n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            # Generate unique ID\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "            \n",
    "            # Prepare metadata\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['content_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "            \n",
    "            # Document content\n",
    "            documents_text.append(doc.page_content)\n",
    "            \n",
    "            # Embedding\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "        \n",
    "        # Add to collection\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_text\n",
    "            )\n",
    "            print(f\"Successfully added {len(documents)} documents to vector store\")\n",
    "            print(f\"Total documents in collection: {self.collection.count()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "vectorstore=VectorStore()\n",
    "vectorstore\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd54427",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323defb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Convert the text to embeddings\n",
    "texts=[doc.page_content for doc in chunks]\n",
    "\n",
    "## Generate the Embeddings\n",
    "\n",
    "embeddings=embedding_manager.generate_embeddings(texts)\n",
    "\n",
    "##store int he vector dtaabase\n",
    "vectorstore.add_documents(chunks,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28551608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retriever pipeline from vectorstore\n",
    "class RAGRetriever:\n",
    "    \"\"\"Handles query-based retrieval from the vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store: VectorStore, embedding_manager: EmbeddingManager):\n",
    "        \"\"\"\n",
    "        Initialize the retriever\n",
    "        \n",
    "        Args:\n",
    "            vector_store: Vector store containing document embeddings\n",
    "            embedding_manager: Manager for generating query embeddings\n",
    "        \"\"\"\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.0) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents for a query\n",
    "        \n",
    "        Args:\n",
    "            query: The search query\n",
    "            top_k: Number of top results to return\n",
    "            score_threshold: Minimum similarity score threshold\n",
    "            \n",
    "        Returns:\n",
    "            List of dictionaries containing retrieved documents and metadata\n",
    "        \"\"\"\n",
    "        print(f\"Retrieving documents for query: '{query}'\")\n",
    "        print(f\"Top K: {top_k}, Score threshold: {score_threshold}\")\n",
    "        \n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "        \n",
    "        # Search in vector store\n",
    "        try:\n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=top_k\n",
    "            )\n",
    "            \n",
    "            # Process results\n",
    "            retrieved_docs = []\n",
    "            \n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "                \n",
    "                for i, (doc_id, document, metadata, distance) in enumerate(zip(ids, documents, metadatas, distances)):\n",
    "                    # Convert distance to similarity score (ChromaDB uses cosine distance)\n",
    "                    similarity_score = 1 - distance\n",
    "                    \n",
    "                    if similarity_score >= score_threshold:\n",
    "                        retrieved_docs.append({\n",
    "                            'id': doc_id,\n",
    "                            'content': document,\n",
    "                            'metadata': metadata,\n",
    "                            'similarity_score': similarity_score,\n",
    "                            'distance': distance,\n",
    "                            'rank': i + 1\n",
    "                        })\n",
    "                \n",
    "                print(f\"Retrieved {len(retrieved_docs)} documents (after filtering)\")\n",
    "            else:\n",
    "                print(\"No documents found\")\n",
    "            \n",
    "            return retrieved_docs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during retrieval: {e}\")\n",
    "            return []\n",
    "\n",
    "rag_retriever=RAGRetriever(vectorstore,embedding_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5346580",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e787f01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_retriever.retrieve(\"What is attention is all you need\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cb9b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rag pipeline Vector DB to LLM output generation\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(os.getenv(\"GROQ_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b4517e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf08957",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroqLLM:\n",
    "    def __init__(self, model_name: str = \"gemma2-9b-it\", api_key: str =None):\n",
    "        \"\"\"\n",
    "        Initialize Groq LLM\n",
    "        \n",
    "        Args:\n",
    "            model_name: Groq model name (qwen2-72b-instruct, llama3-70b-8192, etc.)\n",
    "            api_key: Groq API key (or set GROQ_API_KEY environment variable)\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.api_key = api_key or os.environ.get(\"GROQ_API_KEY\")\n",
    "        \n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"Groq API key is required. Set GROQ_API_KEY environment variable or pass api_key parameter.\")\n",
    "        \n",
    "        self.llm = ChatGroq(\n",
    "            groq_api_key=self.api_key,\n",
    "            model_name=self.model_name,\n",
    "            temperature=0.1,\n",
    "            max_tokens=1024\n",
    "        )\n",
    "        \n",
    "        print(f\"Initialized Groq LLM with model: {self.model_name}\")\n",
    "\n",
    "    def generate_response(self, query: str, context: str, max_length: int = 500) -> str:\n",
    "        \"\"\"\n",
    "        Generate response using retrieved context\n",
    "        \n",
    "        Args:\n",
    "            query: User question\n",
    "            context: Retrieved document context\n",
    "            max_length: Maximum response length\n",
    "            \n",
    "        Returns:\n",
    "            Generated response string\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create prompt template\n",
    "        prompt_template = PromptTemplate(\n",
    "            input_variables=[\"context\", \"question\"],\n",
    "            template=\"\"\"You are a helpful AI assistant. Use the following context to answer the question accurately and concisely.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: Provide a clear and informative answer based on the context above. If the context doesn't contain enough information to answer the question, say so.\"\"\"\n",
    "        )\n",
    "        \n",
    "        # Format the prompt\n",
    "        formatted_prompt = prompt_template.format(context=context, question=query)\n",
    "        \n",
    "        try:\n",
    "            # Generate response\n",
    "            messages = [HumanMessage(content=formatted_prompt)]\n",
    "            response = self.llm.invoke(messages)\n",
    "            return response.content\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error generating response: {str(e)}\"\n",
    "        \n",
    "    def generate_response_simple(self, query: str, context: str) -> str:\n",
    "        \"\"\"\n",
    "        Simple response generation without complex prompting\n",
    "        \n",
    "        Args:\n",
    "            query: User question\n",
    "            context: Retrieved context\n",
    "            \n",
    "        Returns:\n",
    "            Generated response\n",
    "        \"\"\"\n",
    "        simple_prompt = f\"\"\"Based on this context: {context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        try:\n",
    "            messages = [HumanMessage(content=simple_prompt)]\n",
    "            response = self.llm.invoke(messages)\n",
    "            return response.content\n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282addfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Groq LLM (you'll need to set GROQ_API_KEY environment variable)\n",
    "try:\n",
    "    groq_llm = GroqLLM(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "    print(\"Groq LLM initialized successfully!\")\n",
    "except ValueError as e:\n",
    "    print(f\"Warning: {e}\")\n",
    "    print(\"Please set your GROQ_API_KEY environment variable to use the LLM.\")\n",
    "    groq_llm = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27bd240",
   "metadata": {},
   "outputs": [],
   "source": [
    "### get the context from the retriever and pass it to the LLM\n",
    "\n",
    "rag_retriever.retrieve(\"Unified Multi-task Learning Framework\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491251b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integrating Vectordb Context pipeline with LLM output\n",
    "\n",
    "### Simple RAG pipeline with Groq LLM\n",
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "### Initialize the Groq LLM (set your GROQ_API_KEY in environment)\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "llm=ChatGroq(groq_api_key=groq_api_key,model_name=\"gemma2-9b-it\",temperature=0.1,max_tokens=1024)\n",
    "\n",
    "## 2. Simple RAG function: retrieve context + generate response\n",
    "def rag_simple(query,retriever,llm,top_k=3):\n",
    "    ## retriever the context\n",
    "    results=retriever.retrieve(query,top_k=top_k)\n",
    "    context=\"\\n\\n\".join([doc['content'] for doc in results]) if results else \"\"\n",
    "    if not context:\n",
    "        return \"No relevant context found to answer the question.\"\n",
    "    \n",
    "    ## generate the answwer using GROQ LLM\n",
    "    prompt=f\"\"\"Use the following context to answer the question concisely.\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {query}\n",
    "\n",
    "        Answer:\"\"\"\n",
    "    \n",
    "    response=llm.invoke([prompt.format(context=context,query=query)])\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f829bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer=rag_simple(\"What is attention mechanism?\",rag_retriever,llm)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2305dff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced rag pipeline features\n",
    "\n",
    "# --- Enhanced RAG Pipeline Features ---\n",
    "def rag_advanced(query, retriever, llm, top_k=5, min_score=0.2, return_context=False):\n",
    "    \"\"\"\n",
    "    RAG pipeline with extra features:\n",
    "    - Returns answer, sources, confidence score, and optionally full context.\n",
    "    \"\"\"\n",
    "    results = retriever.retrieve(query, top_k=top_k, score_threshold=min_score)\n",
    "    if not results:\n",
    "        return {'answer': 'No relevant context found.', 'sources': [], 'confidence': 0.0, 'context': ''}\n",
    "    \n",
    "    # Prepare context and sources\n",
    "    context = \"\\n\\n\".join([doc['content'] for doc in results])\n",
    "    sources = [{\n",
    "        'source': doc['metadata'].get('source_file', doc['metadata'].get('source', 'unknown')),\n",
    "        'page': doc['metadata'].get('page', 'unknown'),\n",
    "        'score': doc['similarity_score'],\n",
    "        'preview': doc['content'][:300] + '...'\n",
    "    } for doc in results]\n",
    "    confidence = max([doc['similarity_score'] for doc in results])\n",
    "    \n",
    "    # Generate answer\n",
    "    prompt = f\"\"\"Use the following context to answer the question concisely.\\nContext:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\"\"\n",
    "    response = llm.invoke([prompt.format(context=context, query=query)])\n",
    "    \n",
    "    output = {\n",
    "        'answer': response.content,\n",
    "        'sources': sources,\n",
    "        'confidence': confidence\n",
    "    }\n",
    "    if return_context:\n",
    "        output['context'] = context\n",
    "    return output\n",
    "\n",
    "# Example usage:\n",
    "result = rag_advanced(\"Hard Negative Mining Technqiues\", rag_retriever, llm, top_k=3, min_score=0.1, return_context=True)\n",
    "print(\"Answer:\", result['answer'])\n",
    "print(\"Sources:\", result['sources'])\n",
    "print(\"Confidence:\", result['confidence'])\n",
    "print(\"Context Preview:\", result['context'][:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6599877e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Advanced RAG Pipeline: Streaming, Citations, History, Summarization ---\n",
    "from typing import List, Dict, Any\n",
    "import time\n",
    "\n",
    "class AdvancedRAGPipeline:\n",
    "    def __init__(self, retriever, llm):\n",
    "        self.retriever = retriever\n",
    "        self.llm = llm\n",
    "        self.history = []  # Store query history\n",
    "\n",
    "    def query(self, question: str, top_k: int = 5, min_score: float = 0.2, stream: bool = False, summarize: bool = False) -> Dict[str, Any]:\n",
    "        # Retrieve relevant documents\n",
    "        results = self.retriever.retrieve(question, top_k=top_k, score_threshold=min_score)\n",
    "        if not results:\n",
    "            answer = \"No relevant context found.\"\n",
    "            sources = []\n",
    "            context = \"\"\n",
    "        else:\n",
    "            context = \"\\n\\n\".join([doc['content'] for doc in results])\n",
    "            sources = [{\n",
    "                'source': doc['metadata'].get('source_file', doc['metadata'].get('source', 'unknown')),\n",
    "                'page': doc['metadata'].get('page', 'unknown'),\n",
    "                'score': doc['similarity_score'],\n",
    "                'preview': doc['content'][:120] + '...'\n",
    "            } for doc in results]\n",
    "            # Streaming answer simulation\n",
    "            prompt = f\"\"\"Use the following context to answer the question concisely.\\nContext:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\"\"\n",
    "            if stream:\n",
    "                print(\"Streaming answer:\")\n",
    "                for i in range(0, len(prompt), 80):\n",
    "                    print(prompt[i:i+80], end='', flush=True)\n",
    "                    time.sleep(0.05)\n",
    "                print()\n",
    "            response = self.llm.invoke([prompt.format(context=context, question=question)])\n",
    "            answer = response.content\n",
    "\n",
    "        # Add citations to answer\n",
    "        citations = [f\"[{i+1}] {src['source']} (page {src['page']})\" for i, src in enumerate(sources)]\n",
    "        answer_with_citations = answer + \"\\n\\nCitations:\\n\" + \"\\n\".join(citations) if citations else answer\n",
    "\n",
    "        # Optionally summarize answer\n",
    "        summary = None\n",
    "        if summarize and answer:\n",
    "            summary_prompt = f\"Summarize the following answer in 2 sentences:\\n{answer}\"\n",
    "            summary_resp = self.llm.invoke([summary_prompt])\n",
    "            summary = summary_resp.content\n",
    "\n",
    "        # Store query history\n",
    "        self.history.append({\n",
    "            'question': question,\n",
    "            'answer': answer,\n",
    "            'sources': sources,\n",
    "            'summary': summary\n",
    "        })\n",
    "\n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': answer_with_citations,\n",
    "            'sources': sources,\n",
    "            'summary': summary,\n",
    "            'history': self.history\n",
    "        }\n",
    "\n",
    "# Example usage:\n",
    "adv_rag = AdvancedRAGPipeline(rag_retriever, llm)\n",
    "result = adv_rag.query(\"what is attention is all you need\", top_k=3, min_score=0.1, stream=True, summarize=True)\n",
    "print(\"\\nFinal Answer:\", result['answer'])\n",
    "print(\"Summary:\", result['summary'])\n",
    "print(\"History:\", result['history'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66129720",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "New_world",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
